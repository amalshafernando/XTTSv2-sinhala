{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13465111,"sourceType":"datasetVersion","datasetId":8547335,"isSourceIdPinned":false}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# XTTS-v2 Sinhala Fine-tuning on Kaggle\n\nThis notebook fine-tunes XTTS-v2 for Sinhala language using the complete pipeline.\n\n**Steps:**\n1. Environment setup (PyTorch, TTS, dependencies)\n2. Clone repository\n3. Download dataset\n4. Download XTTS-v2 base model\n5. Prepare dataset\n6. Extend vocabulary for Sinhala\n7. Fine-tune GPT model\n8. Test inference\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# CELL 1: Install PyTorch with CUDA support\n# ============================================================================\n\n!pip install torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n\n# Verify\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"PyTorch version: {torch.__version__}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","metadata":{"execution":{"iopub.status.busy":"2025-11-06T08:37:16.033683Z","iopub.execute_input":"2025-11-06T08:37:16.033947Z","iopub.status.idle":"2025-11-06T08:39:14.018577Z","shell.execute_reply.started":"2025-11-06T08:37:16.033928Z","shell.execute_reply":"2025-11-06T08:39:14.017850Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.1.0\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m411.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.1.0\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.19.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.15.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.9.0)\nCollecting triton==2.1.0 (from torch==2.1.0)\n  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\nInstalling collected packages: triton, torch, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 triton-2.1.0\nCUDA available: True\nPyTorch version: 2.1.0+cu118\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 17.1 GB\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ============================================================================\n# CELL 2: Set environment variables and verify setup\n# ============================================================================\n\nimport os\nimport sys\n\n# âš ï¸ CRITICAL: Set these BEFORE any TTS imports\nos.environ['TRANSFORMERS_NO_TORCHAO_IMPORT'] = '1'\nos.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION'] = '1'\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\nos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n\nprint(\"âœ… Environment variables set\")\nprint(f\"TRANSFORMERS_NO_TORCHAO_IMPORT = {os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT']}\")\nprint(f\"TORCH_ALLOW_UNSAFE_DESERIALIZATION = {os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION']}\")\nprint(f\"PYTORCH_CUDA_ALLOC_CONF = {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n\n# Check Python version\nprint(f\"\\nPython version: {sys.version}\")\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Check GPU\nimport torch\nprint(f\"\\nCUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:39:14.020079Z","iopub.execute_input":"2025-11-06T08:39:14.020617Z","iopub.status.idle":"2025-11-06T08:39:14.026769Z","shell.execute_reply.started":"2025-11-06T08:39:14.020599Z","shell.execute_reply":"2025-11-06T08:39:14.025965Z"}},"outputs":[{"name":"stdout","text":"âœ… Environment variables set\nTRANSFORMERS_NO_TORCHAO_IMPORT = 1\nTORCH_ALLOW_UNSAFE_DESERIALIZATION = 1\nPYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:512\n\nPython version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nWorking directory: /kaggle/working\n\nCUDA available: True\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 17.1 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ============================================================================\n# CELL 3: Install TTS and all dependencies\n# ============================================================================\n\n# Install TTS and related packages\n!pip install -q TTS==0.22.0\n\n# âš ï¸ CRITICAL FIX: Use transformers 4.36.0 instead of 4.45.2\n!pip install -q transformers==4.36.0 tokenizers==0.15.0\n\n!pip install -q librosa==0.10.2 soundfile==0.12.1 scipy==1.11.2 pysbd==0.3.4\n!pip install -q pandas==1.5.3 scikit-learn==1.3.2 tqdm==4.66.3\n!pip install -q einops==0.7.0 unidecode==1.3.8 inflect==7.0.0\n!pip install -q coqpit==0.0.16 trainer==0.0.36 mutagen\n!pip install -q pypinyin hangul_romanize num2words kagglehub\n!pip install -q requests\n\nprint(\"âœ… All dependencies installed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:39:14.027788Z","iopub.execute_input":"2025-11-06T08:39:14.028101Z","iopub.status.idle":"2025-11-06T08:41:10.892204Z","shell.execute_reply.started":"2025-11-06T08:39:14.028077Z","shell.execute_reply":"2025-11-06T08:41:10.891339Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m99.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m92.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m115.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nvisions 0.8.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nscikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\nxarray 2025.7.1 requires pandas>=2.2, but you have pandas 1.5.3 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\nnx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m89.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.0/260.0 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m51.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\nscikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.11.2 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m96.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hâœ… All dependencies installed successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ============================================================================\n# CELL 4: Verify critical packages\n# ============================================================================\n\nimport trainer\nimport TTS\nimport transformers\nimport librosa\nimport tokenizers\n\nprint(f\"trainer version: {trainer.__version__}\")\nprint(f\"TTS installed: {TTS.__version__}\")\nprint(f\"transformers version: {transformers.__version__}\")\nprint(f\"tokenizers version: {tokenizers.__version__}\")\nprint(f\"librosa version: {librosa.__version__}\")\nprint(\"âœ… All packages verified!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:10.893251Z","iopub.execute_input":"2025-11-06T08:41:10.893512Z","iopub.status.idle":"2025-11-06T08:41:13.094405Z","shell.execute_reply.started":"2025-11-06T08:41:10.893488Z","shell.execute_reply":"2025-11-06T08:41:13.093574Z"}},"outputs":[{"name":"stdout","text":"trainer version: v0.0.36\nTTS installed: 0.22.0\ntransformers version: 4.36.0\ntokenizers version: 0.15.0\nlibrosa version: 0.10.2\nâœ… All packages verified!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ============================================================================\n# CELL 5: Clone repository\n# ============================================================================\n\nimport os\n\nrepo_url = \"https://github.com/amalshafernando/XTTSv2-sinhala.git\"\nrepo_name = \"XTTSv2-sinhala\"\n\n# Clone only if it doesn't exist\nif not os.path.exists(repo_name):\n    print(f\"ğŸ”¹ Cloning {repo_name}...\")\n    !git clone {repo_url}\n    print(\"âœ… Repository cloned\")\nelse:\n    print(f\"âœ… Repository already exists: {repo_name}\")\n\n# Change to repo directory\nos.chdir(repo_name)\nprint(f\"âœ… Current directory: {os.getcwd()}\")\n\n# List contents\nprint(\"\\nğŸ”¹ Repository contents:\")\n!ls -la | head -20\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:13.095276Z","iopub.execute_input":"2025-11-06T08:41:13.095639Z","iopub.status.idle":"2025-11-06T08:41:14.060105Z","shell.execute_reply.started":"2025-11-06T08:41:13.095614Z","shell.execute_reply":"2025-11-06T08:41:14.059383Z"}},"outputs":[{"name":"stdout","text":"ğŸ”¹ Cloning XTTSv2-sinhala...\nCloning into 'XTTSv2-sinhala'...\nremote: Enumerating objects: 516, done.\u001b[K\nremote: Counting objects: 100% (516/516), done.\u001b[K\nremote: Compressing objects: 100% (407/407), done.\u001b[K\nremote: Total 516 (delta 76), reused 502 (delta 65), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (516/516), 927.56 KiB | 16.56 MiB/s, done.\nResolving deltas: 100% (76/76), done.\nâœ… Repository cloned\nâœ… Current directory: /kaggle/working/XTTSv2-sinhala\n\nğŸ”¹ Repository contents:\ntotal 412\ndrwxr-xr-x  5 root root  4096 Nov  6 08:41 .\ndrwxr-xr-x  4 root root  4096 Nov  6 08:41 ..\n-rw-r--r--  1 root root 14831 Nov  6 08:41 config_sinhala.py\n-rw-r--r--  1 root root  2461 Nov  6 08:41 download_checkpoint.py\n-rw-r--r--  1 root root 17191 Nov  6 08:41 extend_vocab_sinhala.py\ndrwxr-xr-x  8 root root  4096 Nov  6 08:41 .git\n-rw-r--r--  1 root root  4737 Nov  6 08:41 .gitignore\n-rw-r--r--  1 root root 20115 Nov  6 08:41 inference_sinhala.py\n-rw-r--r--  1 root root 47386 Nov  6 08:41 kagglebook.ipynb\n-rw-r--r--  1 root root 25832 Nov  6 08:41 kaggle-notebook.ipynb\n-rw-r--r--  1 root root 29680 Nov  6 08:41 kaggle_sinhala_finetuning.ipynb\n-rw-r--r--  1 root root 14610 Nov  6 08:41 kaggle_train_sinhala.py\n-rw-r--r--  1 root root  3057 Nov  6 08:41 NOTEBOOK_FIXES.md\n-rw-r--r--  1 root root 26347 Nov  6 08:41 november-6.ipynb\n-rw-r--r--  1 root root 13130 Nov  6 08:41 prepare_dataset_sinhala.py\n-rw-r--r--  1 root root    16 Nov  6 08:41 README.md\ndrwxr-xr-x  9 root root  4096 Nov  6 08:41 recipes\n-rw-r--r--  1 root root  1039 Nov  6 08:41 requirements.txt\n-rw-r--r--  1 root root 29408 Nov  6 08:41 Sinhala_XTTS_Final_Corrected.ipynb\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ============================================================================\n# CELL 6: Download Sinhala TTS dataset\n# ============================================================================\n\nimport kagglehub\nimport os\n\n# Download dataset\npath = kagglehub.dataset_download(\"amalshaf/sinhala-tts-dataset\")\nprint(f\"âœ… Dataset downloaded to: {path}\")\n\n# Setup paths\nkaggle_dataset_path = f\"{path}/sinhala-tts-dataset\"\nprint(f\"ğŸ“ Kaggle dataset path: {kaggle_dataset_path}\")\n\n# Verify dataset structure\nif os.path.exists(kaggle_dataset_path):\n    print(f\"\\nğŸ“‚ Dataset contents:\")\n    !ls -lh {kaggle_dataset_path}\n    \n    # Check for metadata files\n    metadata_train = f\"{kaggle_dataset_path}/metadata_train.csv\"\n    metadata_eval = f\"{kaggle_dataset_path}/metadata_eval.csv\"\n    \n    if os.path.exists(metadata_train):\n        print(f\"\\nâœ… Found: metadata_train.csv\")\n    if os.path.exists(metadata_eval):\n        print(f\"âœ… Found: metadata_eval.csv\")\n    \n    # Check for audio directory\n    audio_dirs = [\"wav\", \"wavs\", \"audio\", \"audio_files\"]\n    for audio_dir in audio_dirs:\n        audio_path = os.path.join(kaggle_dataset_path, audio_dir)\n        if os.path.exists(audio_path):\n            print(f\"âœ… Found audio directory: {audio_dir}\")\n            break\nelse:\n    print(f\"âŒ Dataset path not found: {kaggle_dataset_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:14.061361Z","iopub.execute_input":"2025-11-06T08:41:14.061655Z","iopub.status.idle":"2025-11-06T08:41:14.437374Z","shell.execute_reply.started":"2025-11-06T08:41:14.061623Z","shell.execute_reply":"2025-11-06T08:41:14.436402Z"}},"outputs":[{"name":"stdout","text":"âœ… Dataset downloaded to: /kaggle/input/sinhala-tts-dataset\nğŸ“ Kaggle dataset path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n\nğŸ“‚ Dataset contents:\ntotal 220K\n-rw-r--r-- 1 nobody nogroup  44K Nov  6 07:15 metadata_eval.csv\n-rw-r--r-- 1 nobody nogroup 175K Nov  6 07:15 metadata_train.csv\ndrwxr-xr-x 2 nobody nogroup    0 Nov  6 07:16 wavs\n\nâœ… Found: metadata_train.csv\nâœ… Found: metadata_eval.csv\nâœ… Found audio directory: wavs\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ============================================================================\n# CELL 7: Prepare dataset using prepare_dataset_sinhala.py\n# ============================================================================\n\nimport os\nimport sys\nimport shutil\n\nprint(\"=\" * 80)\nprint(\"PREPARING DATASET FOR XTTS-v2\")\nprint(\"=\" * 80)\n\n# Get dataset path from previous cell\nkaggle_dataset_path = \"/kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\"\noutput_dataset_path = \"/kaggle/working/datasets\"\n\n# Create output directory\nos.makedirs(output_dataset_path, exist_ok=True)\n\n# IMPORTANT: Copy audio files first if they exist\nsource_wavs = os.path.join(kaggle_dataset_path, \"wavs\")\ntarget_wavs = os.path.join(output_dataset_path, \"wavs\")\n\nif os.path.exists(source_wavs):\n    print(f\"\\nğŸ”¹ Copying audio files from {source_wavs} to {target_wavs}...\")\n    os.makedirs(target_wavs, exist_ok=True)\n    \n    # Count files before copying\n    source_files = [f for f in os.listdir(source_wavs) if f.endswith('.wav')]\n    print(f\"   Found {len(source_files)} audio files in source\")\n    \n    # Copy files\n    copied = 0\n    for filename in source_files:\n        src = os.path.join(source_wavs, filename)\n        dst = os.path.join(target_wavs, filename)\n        if not os.path.exists(dst):\n            shutil.copy2(src, dst)\n            copied += 1\n    \n    print(f\"   âœ… Copied {copied} new audio files\")\n    print(f\"   âœ… Total audio files in target: {len([f for f in os.listdir(target_wavs) if f.endswith('.wav')])}\")\nelse:\n    print(f\"\\nâš ï¸ Warning: Audio directory not found at {source_wavs}\")\n\n# Run dataset preparation script\n!python prepare_dataset_sinhala.py \\\n    --kaggle_path {kaggle_dataset_path} \\\n    --output_path {output_dataset_path}\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"âœ… DATASET PREPARATION COMPLETED\")\nprint(\"=\" * 80)\n\n# Verify output files\ntrain_metadata = f\"{output_dataset_path}/metadata_train.csv\"\neval_metadata = f\"{output_dataset_path}/metadata_eval.csv\"\n\nif os.path.exists(train_metadata):\n    import pandas as pd\n    df_train = pd.read_csv(train_metadata, sep='|', header=None)\n    print(f\"\\nâœ… Training samples: {len(df_train)}\")\n    \nif os.path.exists(eval_metadata):\n    df_eval = pd.read_csv(eval_metadata, sep='|', header=None)\n    print(f\"âœ… Evaluation samples: {len(df_eval)}\")\n\n# Verify audio files are accessible\nif os.path.exists(target_wavs):\n    audio_count = len([f for f in os.listdir(target_wavs) if f.endswith('.wav')])\n    print(f\"âœ… Audio files in working directory: {audio_count}\")\n    \n    # Check if metadata references can be found\n    if os.path.exists(train_metadata):\n        sample_row = df_train.iloc[0]\n        sample_audio = sample_row[0]  # First column is audio_file\n        sample_audio_path = os.path.join(output_dataset_path, sample_audio)\n        if os.path.exists(sample_audio_path):\n            print(f\"âœ… Sample audio file accessible: {sample_audio}\")\n        else:\n            print(f\"âš ï¸ Warning: Sample audio file not found: {sample_audio_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:14.440047Z","iopub.execute_input":"2025-11-06T08:41:14.440323Z","iopub.status.idle":"2025-11-06T08:41:28.338649Z","shell.execute_reply.started":"2025-11-06T08:41:14.440298Z","shell.execute_reply":"2025-11-06T08:41:28.337892Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nPREPARING DATASET FOR XTTS-v2\n================================================================================\n\nğŸ”¹ Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs...\n   Found 1251 audio files in source\n   âœ… Copied 1251 new audio files\n   âœ… Total audio files in target: 1251\n\n============================================================\nCONVERTING DATASET TO XTTS-v2 FORMAT\n============================================================\n\n[2/3] Reading metadata files\n    Train: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_train.csv\n    Eval: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_eval.csv\n    Train samples: 1000\n    Eval samples: 251\n    Detected pipe-separated format in train CSV\n    Detected pipe-separated format in eval CSV\n\n[3/3] Converting train metadata\n\n[1/3] Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs\nCopying audio files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 1054.22it/s]\n    âœ“ Copied 1000 files\n    âœ“ Saved 1000 samples to /kaggle/working/datasets/metadata_train.csv\n    Format: audio_file|text|speaker_name\n\n[3/3] Converting eval metadata\n\n[1/3] Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs\nCopying audio files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [00:00<00:00, 1111.67it/s]\n    âœ“ Copied 251 files\n    âœ“ Saved 251 samples to /kaggle/working/datasets/metadata_eval.csv\n    Format: audio_file|text|speaker_name\n\n[Verification] Checking output files\n    âœ“ metadata_train.csv: Format OK\n      Sample: wavs/sin_2282_sin_2282_8427486285.wav|à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Š...\n    âœ“ metadata_eval.csv: Format OK\n      Sample: wavs/sin_9228_sin_9228_8967768400.wav|à¶¸à·™à¶¸ à¶šà·’à¶ºà¶¸à¶±à·Š à¶­à·”à·… à¶‡à¶­à·’ à¶ºà¶®à·à¶»à·Šà¶®à¶º à·€à·’à¶¸à·ƒà· à¶¶à·à¶½à·”à·€ à·„à·œà¶­...\n\n============================================================\nDATASET PREPARATION COMPLETED SUCCESSFULLY\n============================================================\n\nOutput directory: /kaggle/working/datasets\n  - metadata_train.csv (1000 samples)\n  - metadata_eval.csv (251 samples)\n  - wavs/ (audio files)\n\n================================================================================\nâœ… DATASET PREPARATION COMPLETED\n================================================================================\n\nâœ… Training samples: 1000\nâœ… Evaluation samples: 251\nâœ… Audio files in working directory: 1251\nâœ… Sample audio file accessible: wavs/sin_2282_sin_2282_8427486285.wav\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# CELL 8: Download XTTS-v2 base model files\n# ============================================================================\n\nimport os\nimport requests\nfrom tqdm import tqdm\n\n# Create output directory\noutput_dir = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\nos.makedirs(output_dir, exist_ok=True)\n\nprint(\"=\" * 80)\nprint(\"DOWNLOADING XTTS-v2 MODEL FILES\")\nprint(\"=\" * 80)\n\n# Define all required files from Hugging Face\nbase_url = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/\"\n\nfiles_to_download = {\n    \"config.json\": f\"{base_url}config.json\",\n    \"vocab.json\": f\"{base_url}vocab.json\",\n    \"model.pth\": f\"{base_url}model.pth\",\n    \"dvae.pth\": f\"{base_url}dvae.pth\",\n    \"mel_stats.pth\": f\"{base_url}mel_stats.pth\",\n}\n\ndef download_file(url, output_path):\n    '''Download file with progress bar'''\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get('content-length', 0))\n    \n    with open(output_path, 'wb') as f:\n        with tqdm(total=total_size, unit='B', unit_scale=True, desc=os.path.basename(output_path)) as pbar:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n                    pbar.update(len(chunk))\n\n# Download each file\nfor filename, url in files_to_download.items():\n    output_path = os.path.join(output_dir, filename)\n    \n    if os.path.exists(output_path):\n        size_mb = os.path.getsize(output_path) / (1024 * 1024)\n        print(f\"âœ… {filename} already exists ({size_mb:.1f} MB), skipping...\")\n    else:\n        print(f\"\\nğŸ”¹ Downloading {filename}...\")\n        try:\n            download_file(url, output_path)\n            print(f\"âœ… {filename} downloaded successfully\")\n        except Exception as e:\n            print(f\"âŒ Failed to download {filename}: {e}\")\n\n# Verify all files downloaded\nprint(f\"\\n{'=' * 80}\")\nprint(\"VERIFICATION\")\nprint(f\"{'=' * 80}\")\n\nall_downloaded = True\nfor filename in files_to_download.keys():\n    filepath = os.path.join(output_dir, filename)\n    if os.path.exists(filepath):\n        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n        print(f\"âœ… {filename}: {size_mb:.1f} MB\")\n    else:\n        print(f\"âŒ {filename}: MISSING!\")\n        all_downloaded = False\n\nif all_downloaded:\n    print(f\"\\n{'=' * 80}\")\n    print(\"âœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\")\n    print(f\"{'=' * 80}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:28.339597Z","iopub.execute_input":"2025-11-06T08:41:28.339899Z","iopub.status.idle":"2025-11-06T08:41:38.509389Z","shell.execute_reply.started":"2025-11-06T08:41:28.339855Z","shell.execute_reply":"2025-11-06T08:41:38.508703Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nDOWNLOADING XTTS-v2 MODEL FILES\n================================================================================\n\nğŸ”¹ Downloading config.json...\n","output_type":"stream"},{"name":"stderr","text":"config.json: 4.37kB [00:00, 6.38MB/s]","output_type":"stream"},{"name":"stdout","text":"âœ… config.json downloaded successfully\n\nğŸ”¹ Downloading vocab.json...\n","output_type":"stream"},{"name":"stderr","text":"\nvocab.json: 361kB [00:00, 74.3MB/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… vocab.json downloaded successfully\n\nğŸ”¹ Downloading model.pth...\n","output_type":"stream"},{"name":"stderr","text":"model.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87G/1.87G [00:08<00:00, 225MB/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… model.pth downloaded successfully\n\nğŸ”¹ Downloading dvae.pth...\n","output_type":"stream"},{"name":"stderr","text":"dvae.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211M/211M [00:00<00:00, 280MB/s] \n","output_type":"stream"},{"name":"stdout","text":"âœ… dvae.pth downloaded successfully\n\nğŸ”¹ Downloading mel_stats.pth...\n","output_type":"stream"},{"name":"stderr","text":"mel_stats.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.07k/1.07k [00:00<00:00, 1.71MB/s]","output_type":"stream"},{"name":"stdout","text":"âœ… mel_stats.pth downloaded successfully\n\n================================================================================\nVERIFICATION\n================================================================================\nâœ… config.json: 0.0 MB\nâœ… vocab.json: 0.3 MB\nâœ… model.pth: 1781.4 MB\nâœ… dvae.pth: 200.8 MB\nâœ… mel_stats.pth: 0.0 MB\n\n================================================================================\nâœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ============================================================================\n# CELL 9: Extend vocabulary for Sinhala language\n# ============================================================================\n\nimport os\nimport json\n\nprint(\"=\" * 80)\nprint(\"EXTENDING VOCABULARY FOR SINHALA\")\nprint(\"=\" * 80)\n\n# Paths\nmetadata_path = \"/kaggle/working/datasets/metadata_train.csv\"\noutput_path = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n\n# Verify paths exist\nif not os.path.exists(metadata_path):\n    print(f\"âŒ Error: Metadata file not found: {metadata_path}\")\nelse:\n    print(f\"âœ… Metadata file found: {metadata_path}\")\n\nif not os.path.exists(output_path):\n    print(f\"âŒ Error: Output path not found: {output_path}\")\nelse:\n    print(f\"âœ… Output path exists: {output_path}\")\n\n# Run extend_vocab_sinhala.py\nprint(f\"\\nğŸ”¹ Running extend_vocab_sinhala.py...\")\n!python extend_vocab_sinhala.py \\\n    --metadata_path {metadata_path} \\\n    --output_path {output_path} \\\n    --language si \\\n    --vocab_size 15000\n\nprint(\"\\nâœ… Vocabulary extension completed!\")\n\n# Verify the extended vocab\nvocab_path = os.path.join(output_path, \"vocab.json\")\nif os.path.exists(vocab_path):\n    with open(vocab_path, 'r', encoding='utf-8') as f:\n        vocab = json.load(f)\n    print(f\"\\nâœ… Extended vocabulary size: {len(vocab):,} tokens\")\n    \n    # Check for Sinhala characters in vocab\n    # Note: ByteLevel BPE works at byte level, so explicit Sinhala tokens may not appear\n    # This is OK - what matters is that there are no UNK tokens\n    sinhala_tokens = [token for token in vocab.keys() if any('\\u0D80' <= char <= '\\u0DFF' for char in token)]\n    print(f\"â„¹ï¸ Sinhala-specific tokens: {len(sinhala_tokens)}\")\n    print(f\"   (Note: ByteLevel BPE works at byte level, so explicit Sinhala tokens may not appear)\")\n    print(f\"   âœ… What matters: No UNK tokens = tokenizer can handle Sinhala text correctly\")\n    \n    # Verify config.json was updated\n    config_path = os.path.join(output_path, \"config.json\")\n    if os.path.exists(config_path):\n        with open(config_path, 'r', encoding='utf-8') as f:\n            config = json.load(f)\n        if 'language_ids' in config and 'si' in config['language_ids']:\n            print(f\"âœ… Sinhala language (si) added to config.json\")\n            print(f\"   Language ID: {config['language_ids']['si']}\")\nelse:\n    print(f\"\\nâŒ Vocabulary file not found at: {vocab_path}\")\n    \nprint(\"\\n\" + \"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:38.510136Z","iopub.execute_input":"2025-11-06T08:41:38.510430Z","iopub.status.idle":"2025-11-06T08:41:39.326921Z","shell.execute_reply.started":"2025-11-06T08:41:38.510403Z","shell.execute_reply":"2025-11-06T08:41:39.325964Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nEXTENDING VOCABULARY FOR SINHALA\n================================================================================\nâœ… Metadata file found: /kaggle/working/datasets/metadata_train.csv\nâœ… Output path exists: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n\nğŸ”¹ Running extend_vocab_sinhala.py...\n\n================================================================================\nSINHALA VOCABULARY EXTENSION FOR XTTS-v2\n================================================================================\n\nğŸ“‹ Configuration:\n   - Language: si (Sinhala)\n   - Vocabulary size: 15,000 tokens\n   - Metadata: /kaggle/working/datasets/metadata_train.csv\n   - Output: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSTEP 1: LOADING SINHALA TEXTS\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“– Loading Sinhala texts from: /kaggle/working/datasets/metadata_train.csv\nâœ… CSV loaded successfully\nâœ… Loaded 1000 Sinhala text samples\n\nğŸ“ Sample Sinhala texts:\n  1. à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.\n  2. à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.\n  3. à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š à¶»à¶§à¶šà·’.\n\nğŸ“Š Text Statistics:\n   - Total characters: 48,540\n   - Sinhala characters: 40,716 (83.9%)\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSTEP 2: TRAINING BPE TOKENIZER\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n================================================================================\nTRAINING SINHALA BPE TOKENIZER\nTarget Vocabulary Size: 15000\n================================================================================\n\nâœ… Tokenizer initialized with ByteLevel pre-tokenizer\nâœ… BpeTrainer configured\n   - Min frequency: 1\n   - Special tokens: 5\n\nğŸ”§ Training BPE tokenizer on 1000 Sinhala texts...\n\u001b[2K[00:00:00] Tokenize words                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1508     /     1508[00:00:00] Tokenize words                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0        /        0\n\u001b[2K[00:00:00] Count pairs                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1508     /     1508\n\u001b[2K[00:00:00] Compute merges                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1599     /     1599\n\nâœ… Training completed!\n   - Actual vocabulary size: 1,660 tokens\n\nâœ… Testing tokenization on samples:\n   Text: à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.     â†’  35 tokens [âœ… No UNK]\n   Text: à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.      â†’  29 tokens [âœ… No UNK]\n   Text: à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š â†’  42 tokens [âœ… No UNK]\n\n   âœ… EXCELLENT: No UNK tokens in test samples!\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSTEP 3: SAVING VOCABULARY\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n================================================================================\nSAVING VOCABULARY\n================================================================================\n\nâœ… Output directory created: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\nâœ… Vocabulary saved: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\n   - File size: 45.6 KB\n   - Tokens: 1,660\nâœ… Tokenizer saved: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/tokenizer.json\n   - File size: 98.0 KB\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSTEP 4: UPDATING CONFIG\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ” Searching for config.json...\nâœ… Found: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n\n================================================================================\nUPDATING CONFIG.JSON FOR SINHALA\n================================================================================\n\nğŸ“– Loading config from: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\nâœ… Config loaded successfully\n\nâœ… Added language_ids:\n   - Language: si\n   - ID: 21\nâœ… Added language settings:\n   - Phoneme language: None (grapheme-based)\n   - Use phonemes: False\n   - Language name: Sinhala\n\nâœ… Config saved successfully: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n\nğŸ“Š Languages in config: ['si']\n\n================================================================================\nâœ… VOCABULARY EXTENSION COMPLETE!\n================================================================================\n\nğŸ“ Output files created:\n   âœ… /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\n   âœ… /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/tokenizer.json\n   âœ… /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json (updated)\n\nğŸš€ Ready for GPT fine-tuning!\n   Use command:\n   CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n     --output_path /kaggle/working/checkpoints/XTTS_v2.0_original_model_files \\\n     --metadatas <path_to_train>,<path_to_eval>,si \\\n     --num_epochs 5 \\\n     --batch_size 8\n\n================================================================================\n\n\nâœ… Vocabulary extension completed!\n\nâœ… Extended vocabulary size: 1,660 tokens\nâ„¹ï¸ Sinhala-specific tokens: 0\n   (Note: ByteLevel BPE works at byte level, so explicit Sinhala tokens may not appear)\n   âœ… What matters: No UNK tokens = tokenizer can handle Sinhala text correctly\nâœ… Sinhala language (si) added to config.json\n   Language ID: 21\n\n================================================================================\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## ğŸ“ Understanding Vocabulary Results\n\n**About \"Sinhala-specific tokens: 0\":**\n- âœ… **This is OK!** ByteLevel BPE tokenizer works at the **byte level**, not character level\n- It doesn't need explicit Sinhala character tokens because it encodes Unicode bytes\n- The important thing is: **No UNK tokens** = tokenizer can handle all Sinhala text\n\n**About vocabulary size (921 vs 15000):**\n- The actual vocabulary size depends on the dataset size and diversity\n- With 1000 samples, 921 tokens is reasonable\n- The tokenizer will still work correctly as long as there are no UNK tokens\n- For larger datasets, you'll get closer to the target size\n\n**What matters:**\n- âœ… No UNK tokens in test samples = tokenizer is working correctly\n- âœ… All Sinhala text can be tokenized properly\n- âœ… Ready for training!\n","metadata":{}},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: Run complete training pipeline using kaggle_train_sinhala.py\n# ============================================================================\n\nimport os\nimport sys\n\nprint(\"=\" * 80)\nprint(\"STARTING COMPLETE TRAINING PIPELINE\")\nprint(\"=\" * 80)\n\n# Verify we're in the repo directory\nif not os.path.exists(\"kaggle_train_sinhala.py\"):\n    print(\"âŒ Error: kaggle_train_sinhala.py not found in current directory\")\n    print(f\"Current directory: {os.getcwd()}\")\n    print(\"\\nTrying to find it...\")\n    !find . -name \"kaggle_train_sinhala.py\" -type f\nelse:\n    print(\"âœ… Found kaggle_train_sinhala.py\")\n    \n    # Run the complete training pipeline\n    print(\"\\nğŸš€ Starting training pipeline...\")\n    print(\"This will run all phases:\")\n    print(\"  1. Setup verification\")\n    print(\"  2. Dataset preparation\")\n    print(\"  3. Model download\")\n    print(\"  4. Vocabulary extension\")\n    print(\"  5. GPT fine-tuning\")\n    print(\"\\nâš ï¸ This may take several hours...\")\n    \n    !python kaggle_train_sinhala.py\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"âœ… TRAINING PIPELINE COMPLETED!\")\n    print(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:39.328129Z","iopub.execute_input":"2025-11-06T08:41:39.328498Z","iopub.status.idle":"2025-11-06T08:41:48.713922Z","shell.execute_reply.started":"2025-11-06T08:41:39.328475Z","shell.execute_reply":"2025-11-06T08:41:48.713072Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nSTARTING COMPLETE TRAINING PIPELINE\n================================================================================\nâœ… Found kaggle_train_sinhala.py\n\nğŸš€ Starting training pipeline...\nThis will run all phases:\n  1. Setup verification\n  2. Dataset preparation\n  3. Model download\n  4. Vocabulary extension\n  5. GPT fine-tuning\n\nâš ï¸ This may take several hours...\n\n================================================================================\nXTTS-v2 SINHALA FINE-TUNING PIPELINE\n================================================================================\n\n============================================================\nPHASE 1: SETUP VERIFICATION\n============================================================\n\n================================================================================\nSINHALA XTTS-v2 KAGGLE TRAINING CONFIGURATION\n================================================================================\n\n[Language Settings]\n  Language Code: si\n  Language Name: Sinhala\n  Script: Sinhala script (abugida)\n\n[Path Settings]\n  Dataset: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n  Working: /kaggle/working\n  Output: /kaggle/working/datasets\n  Checkpoints: /kaggle/working/checkpoints\n\n[Vocabulary]\n  Type: ByteLevel BPE\n  Size: 15,000 tokens\n  Min Frequency: 2\n  Special Tokens: 5\n\n[Training Parameters]\n  Epochs: 5\n  Batch Size: 8\n  Gradient Accumulation: 4\n  Effective Batch: 32\n  Learning Rate: 5e-06\n  Weight Decay: 1e-02\n  Save Step: 50,000\n\n[Text & Audio]\n  Max Text Length: 400 tokens\n  Max Audio Length: 330750 samples (~13.8s)\n  Sample Rate: 22050 Hz (training)\n  Output Sample Rate: 24000 Hz\n\n[Computation]\n  Optimizer: AdamW\n  Use DeepSpeed: False\n  Loader Workers: 8\n\n[Logging & Monitoring]\n  Dashboard: tensorboard\n  Run Name: GPT_XTTS_FT_Sinhala\n  Project: XTTS_Sinhala_Trainer\n\n================================================================================\nâœ… Configuration is VALID\n\n\n[1/4] PyTorch Version: 2.1.0+cu118\n[2/4] CUDA Available: True\n    CUDA Version: 11.8\n    GPU Count: 1\n    GPU 0: Tesla P100-PCIE-16GB\n      Memory: 15.89 GB\n\n[3/4] Checking paths\n    dataset_path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n    output_path: /kaggle/working/datasets\n      âœ“ Directory exists/created\n    checkpoint_path: /kaggle/working/checkpoints\n      âœ“ Directory exists/created\n    model_files_path: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n      âœ“ Directory exists/created\n    training_output: /kaggle/working/checkpoints/run/training\n      âœ“ Directory exists/created\n\n[4/4] Python Version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n\n============================================================\nPHASE 1 COMPLETED\n============================================================\n\n============================================================\nPHASE 2: DATASET PREPARATION\n============================================================\n\n[1/2] Preparing dataset from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n[2/2] Running prepare_dataset_sinhala.py\n    Command: /usr/bin/python3 prepare_dataset_sinhala.py --kaggle_path /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset --output_path /kaggle/working/datasets\n\n============================================================\nCONVERTING DATASET TO XTTS-v2 FORMAT\n============================================================\n\n[2/3] Reading metadata files\n    Train: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_train.csv\n    Eval: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_eval.csv\n    Train samples: 1000\n    Eval samples: 251\n    Detected pipe-separated format in train CSV\n    Detected pipe-separated format in eval CSV\n\n[3/3] Converting train metadata\n\n[1/3] Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs\n    âœ“ Copied 1000 files\n    âœ“ Saved 1000 samples to /kaggle/working/datasets/metadata_train.csv\n    Format: audio_file|text|speaker_name\n\n[3/3] Converting eval metadata\n\n[1/3] Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs\n    âœ“ Copied 251 files\n    âœ“ Saved 251 samples to /kaggle/working/datasets/metadata_eval.csv\n    Format: audio_file|text|speaker_name\n\n[Verification] Checking output files\n    âœ“ metadata_train.csv: Format OK\n      Sample: wavs/sin_2282_sin_2282_8427486285.wav|à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Š...\n    âœ“ metadata_eval.csv: Format OK\n      Sample: wavs/sin_9228_sin_9228_8967768400.wav|à¶¸à·™à¶¸ à¶šà·’à¶ºà¶¸à¶±à·Š à¶­à·”à·… à¶‡à¶­à·’ à¶ºà¶®à·à¶»à·Šà¶®à¶º à·€à·’à¶¸à·ƒà· à¶¶à·à¶½à·”à·€ à·„à·œà¶­...\n\n============================================================\nDATASET PREPARATION COMPLETED SUCCESSFULLY\n============================================================\n\nOutput directory: /kaggle/working/datasets\n  - metadata_train.csv (1000 samples)\n  - metadata_eval.csv (251 samples)\n  - wavs/ (audio files)\n\n\n    âœ“ Training metadata: /kaggle/working/datasets/metadata_train.csv\n    âœ“ Evaluation metadata: /kaggle/working/datasets/metadata_eval.csv\n\n============================================================\nPHASE 2 COMPLETED\n============================================================\n\n============================================================\nPHASE 3: DOWNLOAD XTTS-v2 MODEL\n============================================================\n\n[1/2] Downloading XTTS-v2 model files to /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n[2/2] Using download_checkpoint.py\n\n    âœ“ Download completed\n\n    âœ“ All model files downloaded successfully\n      - dvae.pth (200.8 MB)\n      - mel_stats.pth (0.0 MB)\n      - vocab.json (0.0 MB)\n      - model.pth (1781.4 MB)\n      - config.json (0.0 MB)\n\n============================================================\nPHASE 3 COMPLETED\n============================================================\n\n============================================================\nPHASE 4: VOCABULARY EXTENSION\n============================================================\n\n[1/2] Extending vocabulary for Sinhala language\n[2/2] Running extend_vocab_sinhala.py\n    Command: /usr/bin/python3 extend_vocab_sinhala.py --metadata_path /kaggle/working/datasets/metadata_train.csv --output_path /kaggle/working/checkpoints --language si --vocab_size 15000\n\n\n\n\n================================================================================\nSINHALA VOCABULARY EXTENSION FOR XTTS-v2\n================================================================================\n\nğŸ“‹ Configuration:\n   - Language: si (Sinhala)\n   - Vocabulary size: 15,000 tokens\n   - Metadata: /kaggle/working/datasets/metadata_train.csv\n   - Output: /kaggle/working/checkpoints\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSTEP 1: LOADING SINHALA TEXTS\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ“– Loading Sinhala texts from: /kaggle/working/datasets/metadata_train.csv\nâœ… CSV loaded successfully\nâœ… Loaded 1000 Sinhala text samples\n\nğŸ“ Sample Sinhala texts:\n  1. à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.\n  2. à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.\n  3. à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š à¶»à¶§à¶šà·’.\n\nğŸ“Š Text Statistics:\n   - Total characters: 48,540\n   - Sinhala characters: 40,716 (83.9%)\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSTEP 2: TRAINING BPE TOKENIZER\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n================================================================================\nTRAINING SINHALA BPE TOKENIZER\nTarget Vocabulary Size: 15000\n================================================================================\n\nâœ… Tokenizer initialized with ByteLevel pre-tokenizer\nâœ… BpeTrainer configured\n   - Min frequency: 1\n   - Special tokens: 5\n\nğŸ”§ Training BPE tokenizer on 1000 Sinhala texts...\n\nâœ… Training completed!\n   - Actual vocabulary size: 1,660 tokens\n\nâœ… Testing tokenization on samples:\n   Text: à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.     â†’  35 tokens [âœ… No UNK]\n   Text: à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.      â†’  29 tokens [âœ… No UNK]\n   Text: à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š â†’  42 tokens [âœ… No UNK]\n\n   âœ… EXCELLENT: No UNK tokens in test samples!\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSTEP 3: SAVING VOCABULARY\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\n================================================================================\nSAVING VOCABULARY\n================================================================================\n\nâœ… Output directory created: /kaggle/working/checkpoints\nâœ… Vocabulary saved: /kaggle/working/checkpoints/vocab.json\n   - File size: 45.6 KB\n   - Tokens: 1,660\nâœ… Tokenizer saved: /kaggle/working/checkpoints/tokenizer.json\n   - File size: 98.0 KB\n\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSTEP 4: UPDATING CONFIG\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nğŸ” Searching for config.json...\n   â„¹ï¸ Not found: /kaggle/working/checkpoints/config.json\n   â„¹ï¸ Not found: /kaggle/working/config.json\nâœ… Found: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n\n================================================================================\nUPDATING CONFIG.JSON FOR SINHALA\n================================================================================\n\nğŸ“– Loading config from: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\nâœ… Config loaded successfully\n\nâœ… Added language_ids:\n   - Language: si\n   - ID: 22\nâœ… Added language settings:\n   - Phoneme language: None (grapheme-based)\n   - Use phonemes: False\n   - Language name: Sinhala\n\nâœ… Config saved successfully: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n\nğŸ“Š Languages in config: ['si']\n\n================================================================================\nâœ… VOCABULARY EXTENSION COMPLETE!\n================================================================================\n\nğŸ“ Output files created:\n   âœ… /kaggle/working/checkpoints/vocab.json\n   âœ… /kaggle/working/checkpoints/tokenizer.json\n   âœ… /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json (updated)\n\nğŸš€ Ready for GPT fine-tuning!\n   Use command:\n   CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n     --output_path /kaggle/working/checkpoints \\\n     --metadatas <path_to_train>,<path_to_eval>,si \\\n     --num_epochs 5 \\\n     --batch_size 8\n\n================================================================================\n\n\n\n    âœ“ Vocabulary extended successfully\n    âœ“ Language 'si' added to config.json\n\n============================================================\nPHASE 4 COMPLETED\n============================================================\n\n============================================================\nPHASE 5: DVAE FINE-TUNING (OPTIONAL)\n============================================================\n\n[INFO] DVAE fine-tuning is skipped by default.\n       Uncomment this phase if you have >20 hours of training data.\n       To enable, uncomment the code in phase_5_dvae_finetuning() function.\n\n============================================================\nPHASE 5 COMPLETED (SKIPPED)\n============================================================\n\n============================================================\nPHASE 6: GPT FINE-TUNING\n============================================================\n\n[1/3] Preparing GPT fine-tuning\n    Training metadata: /kaggle/working/datasets/metadata_train.csv\n    Evaluation metadata: /kaggle/working/datasets/metadata_eval.csv\n    Language: si\n\n[2/3] Running GPT fine-tuning\n    Batch size: 8\n    Gradient accumulation: 4\n    Learning rate: 5e-06\n    Epochs: 5\n\n[3/3] Command: /usr/bin/python3 train_gpt_xtts.py --output_path /kaggle/working/datasets --metadatas /kaggle/working/datasets/metadata_train.csv,/kaggle/working/datasets/metadata_eval.csv,si --num_epochs 5 --batch_size 8 --grad_acumm 4 --max_audio_length 330750 --max_text_length 400 --lr 5e-06 --weight_decay 0.01 --save_step 50000\n    Starting training...\n    (This may take several hours)\nTraceback (most recent call last):\n  File \"/kaggle/working/XTTSv2-sinhala/train_gpt_xtts.py\", line 36, in <module>\n    from TTS.trainer import Trainer, TrainerArgs\nModuleNotFoundError: No module named 'TTS.trainer'\n\n    âŒ Training failed with return code 1\n\n================================================================================\nâŒ PIPELINE FAILED: GPT fine-tuning failed\n================================================================================\nTraceback (most recent call last):\n  File \"/kaggle/working/XTTSv2-sinhala/kaggle_train_sinhala.py\", line 377, in main\n    phase_6_gpt_finetuning(\n  File \"/kaggle/working/XTTSv2-sinhala/kaggle_train_sinhala.py\", line 342, in phase_6_gpt_finetuning\n    raise RuntimeError(\"GPT fine-tuning failed\")\nRuntimeError: GPT fine-tuning failed\n\n================================================================================\nâœ… TRAINING PIPELINE COMPLETED!\n================================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# CELL 11: Alternative - Run GPT training directly (if pipeline fails)\n# ============================================================================\n\n# Uncomment this cell only if kaggle_train_sinhala.py fails\n# This runs GPT training directly with all required parameters\n\nimport os\n\nprint(\"=\" * 80)\nprint(\"ALTERNATIVE: DIRECT GPT TRAINING\")\nprint(\"=\" * 80)\nprint(\"âš ï¸ Only use this if the pipeline in Cell 10 failed\")\nprint(\"=\" * 80)\n\n# Uncomment below to run directly:\n\"\"\"\n!CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n    --output_path /kaggle/working/checkpoints/ \\\n    --metadatas /kaggle/working/datasets/metadata_train.csv,/kaggle/working/datasets/metadata_eval.csv,si \\\n    --num_epochs 5 \\\n    --batch_size 8 \\\n    --grad_acumm 4 \\\n    --max_text_length 400 \\\n    --max_audio_length 330750 \\\n    --weight_decay 1e-2 \\\n    --lr 5e-6 \\\n    --save_step 50000\n\nprint(\"\\nâœ… GPT training completed!\")\n\"\"\"\n\nprint(\"â„¹ï¸ This cell is commented out. Uncomment to use if needed.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:48.714869Z","iopub.execute_input":"2025-11-06T08:41:48.715151Z","iopub.status.idle":"2025-11-06T08:41:48.720901Z","shell.execute_reply.started":"2025-11-06T08:41:48.715126Z","shell.execute_reply":"2025-11-06T08:41:48.720187Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nALTERNATIVE: DIRECT GPT TRAINING\n================================================================================\nâš ï¸ Only use this if the pipeline in Cell 10 failed\n================================================================================\nâ„¹ï¸ This cell is commented out. Uncomment to use if needed.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# CELL 12: Verify training output and find best model\n# ============================================================================\n\nimport os\nimport glob\n\nprint(\"=\" * 80)\nprint(\"VERIFYING TRAINING OUTPUT\")\nprint(\"=\" * 80)\n\n# Search for trained models\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\n\n# Look for GPT_XTTS_FT directories\nmodel_dirs = glob.glob(f\"{checkpoint_dir}/GPT_XTTS_FT*\")\n\nif model_dirs:\n    print(f\"\\nâœ… Found {len(model_dirs)} training output(s):\")\n    for model_dir in model_dirs:\n        print(f\"\\nğŸ“ {model_dir}\")\n        \n        # Look for best_model.pth\n        best_model = os.path.join(model_dir, \"best_model.pth\")\n        if os.path.exists(best_model):\n            size_mb = os.path.getsize(best_model) / (1024 * 1024)\n            print(f\"   âœ… best_model.pth ({size_mb:.1f} MB)\")\n        \n        # Look for config.json\n        config_file = os.path.join(model_dir, \"config.json\")\n        if os.path.exists(config_file):\n            print(f\"   âœ… config.json\")\n        \n        # List all files\n        files = os.listdir(model_dir)\n        print(f\"   ğŸ“„ Total files: {len(files)}\")\n        if len(files) <= 10:\n            for f in files:\n                print(f\"      - {f}\")\nelse:\n    print(f\"\\nâš ï¸ No training output found in {checkpoint_dir}\")\n    print(\"   Training may still be in progress or may have failed\")\n\nprint(\"\\n\" + \"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:48.721954Z","iopub.execute_input":"2025-11-06T08:41:48.722228Z","iopub.status.idle":"2025-11-06T08:41:48.748262Z","shell.execute_reply.started":"2025-11-06T08:41:48.722210Z","shell.execute_reply":"2025-11-06T08:41:48.747639Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nVERIFYING TRAINING OUTPUT\n================================================================================\n\nâš ï¸ No training output found in /kaggle/working/checkpoints\n   Training may still be in progress or may have failed\n\n================================================================================\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ============================================================================\n# CELL 13: Test inference with trained model\n# ============================================================================\n\nimport os\nimport glob\n\nprint(\"=\" * 80)\nprint(\"TESTING INFERENCE\")\nprint(\"=\" * 80)\n\n# Find the best model\ncheckpoint_dir = \"/kaggle/working/checkpoints\"\nmodel_dirs = glob.glob(f\"{checkpoint_dir}/GPT_XTTS_FT*\")\n\nif not model_dirs:\n    print(\"âŒ No trained model found. Please complete training first.\")\nelse:\n    model_dir = model_dirs[0]  # Use the first one\n    best_model = os.path.join(model_dir, \"best_model.pth\")\n    config_file = os.path.join(model_dir, \"config.json\")\n    vocab_file = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\"\n    \n    if not os.path.exists(best_model):\n        print(f\"âŒ Best model not found: {best_model}\")\n    elif not os.path.exists(config_file):\n        print(f\"âŒ Config file not found: {config_file}\")\n    elif not os.path.exists(vocab_file):\n        print(f\"âŒ Vocab file not found: {vocab_file}\")\n    else:\n        print(f\"âœ… Found trained model: {model_dir}\")\n        print(f\"\\nğŸ“ Test Sinhala texts:\")\n        test_texts = [\n            \"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\",  # \"Always very important\"\n            \"à·à·Šâ€à¶»à·“ à¶½à¶‚à¶šà· à¶”à¶¶à·š à¶‹à¶­à·”à¶»à·”à¶¯à·™à·ƒà·’à¶±à·Š\",  # \"Sri Lanka from your north\"\n            \"à·ƒà·’à¶‚à·„à¶½ à¶·à·à·‚à·à·€ à¶…à¶´à¶œà·š à¶¢à·à¶­à·’à¶š à¶·à·à·‚à·à·€à¶ºà·’\",  # \"Sinhala is our national language\"\n        ]\n        \n        for i, text in enumerate(test_texts, 1):\n            print(f\"   {i}. {text}\")\n        \n        print(f\"\\nğŸ”¹ To test inference, use:\")\n        print(f\"   python inference_sinhala.py \\\\\")\n        print(f\"     --checkpoint_path {best_model} \\\\\")\n        print(f\"     --config_path {config_file} \\\\\")\n        print(f\"     --vocab_path {vocab_file} \\\\\")\n        print(f\"     --text \\\"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\\\" \\\\\")\n        print(f\"     --reference_audio <path_to_reference_audio.wav> \\\\\")\n        print(f\"     --output_path output.wav\")\n        \n        # Try to find a reference audio file\n        reference_audio = \"/kaggle/working/datasets/wavs\"\n        if os.path.exists(reference_audio):\n            audio_files = [f for f in os.listdir(reference_audio) if f.endswith('.wav')]\n            if audio_files:\n                ref_audio_path = os.path.join(reference_audio, audio_files[0])\n                print(f\"\\nâœ… Found reference audio: {ref_audio_path}\")\n                print(f\"\\nğŸ”¹ Running inference test...\")\n                \n                # Run inference\n                output_audio = \"/kaggle/working/test_output.wav\"\n                !python inference_sinhala.py \\\n                    --checkpoint_path {best_model} \\\n                    --config_path {config_file} \\\n                    --vocab_path {vocab_file} \\\n                    --text \"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\" \\\n                    --reference_audio {ref_audio_path} \\\n                    --output_path {output_audio}\n                \n                if os.path.exists(output_audio):\n                    size_mb = os.path.getsize(output_audio) / (1024 * 1024)\n                    print(f\"\\nâœ… Inference successful!\")\n                    print(f\"   Output: {output_audio} ({size_mb:.2f} MB)\")\n                else:\n                    print(f\"\\nâš ï¸ Inference may have failed - output file not found\")\n\nprint(\"\\n\" + \"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:48.748998Z","iopub.execute_input":"2025-11-06T08:41:48.749229Z","iopub.status.idle":"2025-11-06T08:41:48.767314Z","shell.execute_reply.started":"2025-11-06T08:41:48.749203Z","shell.execute_reply":"2025-11-06T08:41:48.766550Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nTESTING INFERENCE\n================================================================================\nâŒ No trained model found. Please complete training first.\n\n================================================================================\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# ============================================================================\n# CELL 14: Summary and next steps\n# ============================================================================\n\nprint(\"=\" * 80)\nprint(\"TRAINING SUMMARY\")\nprint(\"=\" * 80)\n\nprint(\"\\nâœ… Training pipeline completed!\")\nprint(\"\\nğŸ“ Output locations:\")\nprint(\"   - Trained model: /kaggle/working/checkpoints/GPT_XTTS_FT-*/\")\nprint(\"   - Base model files: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/\")\nprint(\"   - Dataset: /kaggle/working/datasets/\")\n\nprint(\"\\nğŸš€ To use the trained model:\")\nprint(\"   1. Download the checkpoint directory from Kaggle\")\nprint(\"   2. Use inference_sinhala.py to generate Sinhala speech\")\nprint(\"   3. Provide any Sinhala text and a reference audio file\")\n\nprint(\"\\nğŸ“ Example inference command:\")\nprint(\"   python inference_sinhala.py \\\\\")\nprint(\"     --checkpoint_path checkpoints/GPT_XTTS_FT-*/best_model.pth \\\\\")\nprint(\"     --config_path checkpoints/GPT_XTTS_FT-*/config.json \\\\\")\nprint(\"     --vocab_path checkpoints/XTTS_v2.0_original_model_files/vocab.json \\\\\")\nprint(\"     --text \\\"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\\\" \\\\\")\nprint(\"     --reference_audio reference.wav \\\\\")\nprint(\"     --output_path output.wav\")\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"âœ… ALL DONE!\")\nprint(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T08:41:48.768424Z","iopub.execute_input":"2025-11-06T08:41:48.768623Z","iopub.status.idle":"2025-11-06T08:41:48.784460Z","shell.execute_reply.started":"2025-11-06T08:41:48.768608Z","shell.execute_reply":"2025-11-06T08:41:48.783607Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nTRAINING SUMMARY\n================================================================================\n\nâœ… Training pipeline completed!\n\nğŸ“ Output locations:\n   - Trained model: /kaggle/working/checkpoints/GPT_XTTS_FT-*/\n   - Base model files: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/\n   - Dataset: /kaggle/working/datasets/\n\nğŸš€ To use the trained model:\n   1. Download the checkpoint directory from Kaggle\n   2. Use inference_sinhala.py to generate Sinhala speech\n   3. Provide any Sinhala text and a reference audio file\n\nğŸ“ Example inference command:\n   python inference_sinhala.py \\\n     --checkpoint_path checkpoints/GPT_XTTS_FT-*/best_model.pth \\\n     --config_path checkpoints/GPT_XTTS_FT-*/config.json \\\n     --vocab_path checkpoints/XTTS_v2.0_original_model_files/vocab.json \\\n     --text \"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\" \\\n     --reference_audio reference.wav \\\n     --output_path output.wav\n\n================================================================================\nâœ… ALL DONE!\n================================================================================\n","output_type":"stream"}],"execution_count":14}]}