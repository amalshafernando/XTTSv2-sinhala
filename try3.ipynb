{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XTTS-v2 Sinhala Fine-tuning on Kaggle\n",
    "\n",
    "This notebook fine-tunes XTTS-v2 for Sinhala language using the complete pipeline.\n",
    "\n",
    "**Steps:**\n",
    "1. Environment setup (PyTorch, TTS, dependencies)\n",
    "2. Clone repository\n",
    "3. Download dataset\n",
    "4. Download XTTS-v2 base model\n",
    "5. Prepare dataset\n",
    "6. Extend vocabulary for Sinhala\n",
    "7. Fine-tune GPT model\n",
    "8. Test inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:35:11.179163Z",
     "iopub.status.busy": "2025-11-06T09:35:11.178926Z",
     "iopub.status.idle": "2025-11-06T09:37:11.616528Z",
     "shell.execute_reply": "2025-11-06T09:37:11.615777Z",
     "shell.execute_reply.started": "2025-11-06T09:35:11.179140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.1.0\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m376.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.1.0\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.9.0)\n",
      "Collecting triton==2.1.0 (from torch==2.1.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Installing collected packages: triton, torch, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0+cu124\n",
      "    Uninstalling torchaudio-2.6.0+cu124:\n",
      "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 triton-2.1.0\n",
      "CUDA available: True\n",
      "PyTorch version: 2.1.0+cu118\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "GPU Memory: 17.1 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Install PyTorch with CUDA support\n",
    "# ============================================================================\n",
    "\n",
    "!pip install torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Verify\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:37:11.618720Z",
     "iopub.status.busy": "2025-11-06T09:37:11.618415Z",
     "iopub.status.idle": "2025-11-06T09:37:11.625577Z",
     "shell.execute_reply": "2025-11-06T09:37:11.624813Z",
     "shell.execute_reply.started": "2025-11-06T09:37:11.618701Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables set\n",
      "TRANSFORMERS_NO_TORCHAO_IMPORT = 1\n",
      "TORCH_ALLOW_UNSAFE_DESERIALIZATION = 1\n",
      "PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:512\n",
      "\n",
      "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "Working directory: /kaggle/working\n",
      "\n",
      "CUDA available: True\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "GPU Memory: 17.1 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Set environment variables and verify setup\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# âš ï¸ CRITICAL: Set these BEFORE any TTS imports\n",
    "os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT'] = '1'\n",
    "os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION'] = '1'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print(\"âœ… Environment variables set\")\n",
    "print(f\"TRANSFORMERS_NO_TORCHAO_IMPORT = {os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT']}\")\n",
    "print(f\"TORCH_ALLOW_UNSAFE_DESERIALIZATION = {os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION']}\")\n",
    "print(f\"PYTORCH_CUDA_ALLOC_CONF = {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n",
    "\n",
    "# Check Python version\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:37:11.626720Z",
     "iopub.status.busy": "2025-11-06T09:37:11.626361Z",
     "iopub.status.idle": "2025-11-06T09:39:09.344950Z",
     "shell.execute_reply": "2025-11-06T09:39:09.344116Z",
     "shell.execute_reply.started": "2025-11-06T09:37:11.626696Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m51.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m95.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m110.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m91.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "visions 0.8.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\n",
      "xarray 2025.7.1 requires pandas>=2.2, but you have pandas 1.5.3 which is incompatible.\n",
      "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "nx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m87.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.0/260.0 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
      "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.11.2 which is incompatible.\n",
      "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m104.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
      "dataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hâœ… All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Install TTS and all dependencies\n",
    "# ============================================================================\n",
    "\n",
    "# Install TTS and related packages\n",
    "!pip install -q TTS==0.22.0\n",
    "\n",
    "# âš ï¸ CRITICAL FIX: Use transformers 4.36.0 instead of 4.45.2\n",
    "!pip install -q transformers==4.36.0 tokenizers==0.15.0\n",
    "\n",
    "!pip install -q librosa==0.10.2 soundfile==0.12.1 scipy==1.11.2 pysbd==0.3.4\n",
    "!pip install -q pandas==1.5.3 scikit-learn==1.3.2 tqdm==4.66.3\n",
    "!pip install -q einops==0.7.0 unidecode==1.3.8 inflect==7.0.0\n",
    "!pip install -q coqpit==0.0.16 trainer==0.0.36 mutagen\n",
    "!pip install -q pypinyin hangul_romanize num2words kagglehub\n",
    "!pip install -q requests\n",
    "\n",
    "print(\"âœ… All dependencies installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:39:09.346310Z",
     "iopub.status.busy": "2025-11-06T09:39:09.346062Z",
     "iopub.status.idle": "2025-11-06T09:39:11.507616Z",
     "shell.execute_reply": "2025-11-06T09:39:11.506883Z",
     "shell.execute_reply.started": "2025-11-06T09:39:09.346286Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer version: v0.0.36\n",
      "TTS installed: 0.22.0\n",
      "transformers version: 4.36.0\n",
      "tokenizers version: 0.15.0\n",
      "librosa version: 0.10.2\n",
      "âœ… All packages verified!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Verify critical packages\n",
    "# ============================================================================\n",
    "\n",
    "import trainer\n",
    "import TTS\n",
    "import transformers\n",
    "import librosa\n",
    "import tokenizers\n",
    "\n",
    "print(f\"trainer version: {trainer.__version__}\")\n",
    "print(f\"TTS installed: {TTS.__version__}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"tokenizers version: {tokenizers.__version__}\")\n",
    "print(f\"librosa version: {librosa.__version__}\")\n",
    "print(\"âœ… All packages verified!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:39:11.508931Z",
     "iopub.status.busy": "2025-11-06T09:39:11.508473Z",
     "iopub.status.idle": "2025-11-06T09:39:12.474249Z",
     "shell.execute_reply": "2025-11-06T09:39:12.473440Z",
     "shell.execute_reply.started": "2025-11-06T09:39:11.508902Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Cloning XTTSv2-sinhala...\n",
      "Cloning into 'XTTSv2-sinhala'...\n",
      "remote: Enumerating objects: 523, done.\u001b[K\n",
      "remote: Counting objects: 100% (523/523), done.\u001b[K\n",
      "remote: Compressing objects: 100% (411/411), done.\u001b[K\n",
      "remote: Total 523 (delta 81), reused 507 (delta 68), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (523/523), 942.87 KiB | 16.54 MiB/s, done.\n",
      "Resolving deltas: 100% (81/81), done.\n",
      "âœ… Repository cloned\n",
      "âœ… Current directory: /kaggle/working/XTTSv2-sinhala\n",
      "\n",
      "ğŸ”¹ Repository contents:\n",
      "total 424\n",
      "drwxr-xr-x  5 root root  4096 Nov  6 09:39 .\n",
      "drwxr-xr-x  4 root root  4096 Nov  6 09:39 ..\n",
      "-rw-r--r--  1 root root 14831 Nov  6 09:39 config_sinhala.py\n",
      "-rw-r--r--  1 root root  2461 Nov  6 09:39 download_checkpoint.py\n",
      "-rw-r--r--  1 root root 17191 Nov  6 09:39 extend_vocab_sinhala.py\n",
      "drwxr-xr-x  8 root root  4096 Nov  6 09:39 .git\n",
      "-rw-r--r--  1 root root  4738 Nov  6 09:39 .gitignore\n",
      "-rw-r--r--  1 root root 20115 Nov  6 09:39 inference_sinhala.py\n",
      "-rw-r--r--  1 root root 47386 Nov  6 09:39 kagglebook.ipynb\n",
      "-rw-r--r--  1 root root 25832 Nov  6 09:39 kaggle-notebook.ipynb\n",
      "-rw-r--r--  1 root root 29680 Nov  6 09:39 kaggle_sinhala_finetuning.ipynb\n",
      "-rw-r--r--  1 root root 15618 Nov  6 09:39 kaggle_train_sinhala.py\n",
      "-rw-r--r--  1 root root  3057 Nov  6 09:39 NOTEBOOK_FIXES.md\n",
      "-rw-r--r--  1 root root 26347 Nov  6 09:39 november-6.ipynb\n",
      "-rw-r--r--  1 root root 13130 Nov  6 09:39 prepare_dataset_sinhala.py\n",
      "-rw-r--r--  1 root root  5728 Nov  6 09:39 README.md\n",
      "drwxr-xr-x  9 root root  4096 Nov  6 09:39 recipes\n",
      "-rw-r--r--  1 root root  1039 Nov  6 09:39 requirements.txt\n",
      "-rw-r--r--  1 root root 29408 Nov  6 09:39 Sinhala_XTTS_Final_Corrected.ipynb\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Clone repository\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "repo_url = \"https://github.com/amalshafernando/XTTSv2-sinhala.git\"\n",
    "repo_name = \"XTTSv2-sinhala\"\n",
    "\n",
    "# Clone only if it doesn't exist\n",
    "if not os.path.exists(repo_name):\n",
    "    print(f\"ğŸ”¹ Cloning {repo_name}...\")\n",
    "    !git clone {repo_url}\n",
    "    print(\"âœ… Repository cloned\")\n",
    "else:\n",
    "    print(f\"âœ… Repository already exists: {repo_name}\")\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(repo_name)\n",
    "print(f\"âœ… Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List contents\n",
    "print(\"\\nğŸ”¹ Repository contents:\")\n",
    "!ls -la | head -20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:39:12.475605Z",
     "iopub.status.busy": "2025-11-06T09:39:12.475316Z",
     "iopub.status.idle": "2025-11-06T09:39:13.021075Z",
     "shell.execute_reply": "2025-11-06T09:39:13.020226Z",
     "shell.execute_reply.started": "2025-11-06T09:39:12.475571Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset downloaded to: /kaggle/input/sinhala-tts-dataset\n",
      "ğŸ“ Kaggle dataset path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n",
      "\n",
      "ğŸ“‚ Dataset contents:\n",
      "total 220K\n",
      "-rw-r--r-- 1 nobody nogroup  44K Nov  6 07:15 metadata_eval.csv\n",
      "-rw-r--r-- 1 nobody nogroup 175K Nov  6 07:15 metadata_train.csv\n",
      "drwxr-xr-x 2 nobody nogroup    0 Nov  6 07:16 wavs\n",
      "\n",
      "âœ… Found: metadata_train.csv\n",
      "âœ… Found: metadata_eval.csv\n",
      "âœ… Found audio directory: wavs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Download Sinhala TTS dataset\n",
    "# ============================================================================\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"amalshaf/sinhala-tts-dataset\")\n",
    "print(f\"âœ… Dataset downloaded to: {path}\")\n",
    "\n",
    "# Setup paths\n",
    "kaggle_dataset_path = f\"{path}/sinhala-tts-dataset\"\n",
    "print(f\"ğŸ“ Kaggle dataset path: {kaggle_dataset_path}\")\n",
    "\n",
    "# Verify dataset structure\n",
    "if os.path.exists(kaggle_dataset_path):\n",
    "    print(f\"\\nğŸ“‚ Dataset contents:\")\n",
    "    !ls -lh {kaggle_dataset_path}\n",
    "    \n",
    "    # Check for metadata files\n",
    "    metadata_train = f\"{kaggle_dataset_path}/metadata_train.csv\"\n",
    "    metadata_eval = f\"{kaggle_dataset_path}/metadata_eval.csv\"\n",
    "    \n",
    "    if os.path.exists(metadata_train):\n",
    "        print(f\"\\nâœ… Found: metadata_train.csv\")\n",
    "    if os.path.exists(metadata_eval):\n",
    "        print(f\"âœ… Found: metadata_eval.csv\")\n",
    "    \n",
    "    # Check for audio directory\n",
    "    audio_dirs = [\"wav\", \"wavs\", \"audio\", \"audio_files\"]\n",
    "    for audio_dir in audio_dirs:\n",
    "        audio_path = os.path.join(kaggle_dataset_path, audio_dir)\n",
    "        if os.path.exists(audio_path):\n",
    "            print(f\"âœ… Found audio directory: {audio_dir}\")\n",
    "            break\n",
    "else:\n",
    "    print(f\"âŒ Dataset path not found: {kaggle_dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:39:13.024217Z",
     "iopub.status.busy": "2025-11-06T09:39:13.023968Z",
     "iopub.status.idle": "2025-11-06T09:39:26.064239Z",
     "shell.execute_reply": "2025-11-06T09:39:26.063589Z",
     "shell.execute_reply.started": "2025-11-06T09:39:13.024193Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING DATASET FOR XTTS-v2\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¹ Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs...\n",
      "   Found 1251 audio files in source\n",
      "   âœ… Copied 1251 new audio files\n",
      "   âœ… Total audio files in target: 1251\n",
      "\n",
      "============================================================\n",
      "CONVERTING DATASET TO XTTS-v2 FORMAT\n",
      "============================================================\n",
      "\n",
      "[2/3] Reading metadata files\n",
      "    Train: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_train.csv\n",
      "    Eval: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_eval.csv\n",
      "    Train samples: 1000\n",
      "    Eval samples: 251\n",
      "    Detected pipe-separated format in train CSV\n",
      "    Detected pipe-separated format in eval CSV\n",
      "\n",
      "[3/3] Converting train metadata\n",
      "\n",
      "[1/3] Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs\n",
      "Copying audio files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:01<00:00, 911.28it/s]\n",
      "    âœ“ Copied 1000 files\n",
      "    âœ“ Saved 1000 samples to /kaggle/working/datasets/metadata_train.csv\n",
      "    Format: audio_file|text|speaker_name\n",
      "\n",
      "[3/3] Converting eval metadata\n",
      "\n",
      "[1/3] Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs\n",
      "Copying audio files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 251/251 [00:00<00:00, 1042.62it/s]\n",
      "    âœ“ Copied 251 files\n",
      "    âœ“ Saved 251 samples to /kaggle/working/datasets/metadata_eval.csv\n",
      "    Format: audio_file|text|speaker_name\n",
      "\n",
      "[Verification] Checking output files\n",
      "    âœ“ metadata_train.csv: Format OK\n",
      "      Sample: wavs/sin_2282_sin_2282_8427486285.wav|à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Š...\n",
      "    âœ“ metadata_eval.csv: Format OK\n",
      "      Sample: wavs/sin_9228_sin_9228_8967768400.wav|à¶¸à·™à¶¸ à¶šà·’à¶ºà¶¸à¶±à·Š à¶­à·”à·… à¶‡à¶­à·’ à¶ºà¶®à·à¶»à·Šà¶®à¶º à·€à·’à¶¸à·ƒà· à¶¶à·à¶½à·”à·€ à·„à·œà¶­...\n",
      "\n",
      "============================================================\n",
      "DATASET PREPARATION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "Output directory: /kaggle/working/datasets\n",
      "  - metadata_train.csv (1000 samples)\n",
      "  - metadata_eval.csv (251 samples)\n",
      "  - wavs/ (audio files)\n",
      "\n",
      "================================================================================\n",
      "âœ… DATASET PREPARATION COMPLETED\n",
      "================================================================================\n",
      "\n",
      "âœ… Training samples: 1000\n",
      "âœ… Evaluation samples: 251\n",
      "âœ… Audio files in working directory: 1251\n",
      "âœ… Sample audio file accessible: wavs/sin_2282_sin_2282_8427486285.wav\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Prepare dataset using prepare_dataset_sinhala.py\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPARING DATASET FOR XTTS-v2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get dataset path from previous cell\n",
    "kaggle_dataset_path = \"/kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\"\n",
    "output_dataset_path = \"/kaggle/working/datasets\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dataset_path, exist_ok=True)\n",
    "\n",
    "# IMPORTANT: Copy audio files first if they exist\n",
    "source_wavs = os.path.join(kaggle_dataset_path, \"wavs\")\n",
    "target_wavs = os.path.join(output_dataset_path, \"wavs\")\n",
    "\n",
    "if os.path.exists(source_wavs):\n",
    "    print(f\"\\nğŸ”¹ Copying audio files from {source_wavs} to {target_wavs}...\")\n",
    "    os.makedirs(target_wavs, exist_ok=True)\n",
    "    \n",
    "    # Count files before copying\n",
    "    source_files = [f for f in os.listdir(source_wavs) if f.endswith('.wav')]\n",
    "    print(f\"   Found {len(source_files)} audio files in source\")\n",
    "    \n",
    "    # Copy files\n",
    "    copied = 0\n",
    "    for filename in source_files:\n",
    "        src = os.path.join(source_wavs, filename)\n",
    "        dst = os.path.join(target_wavs, filename)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            copied += 1\n",
    "    \n",
    "    print(f\"   âœ… Copied {copied} new audio files\")\n",
    "    print(f\"   âœ… Total audio files in target: {len([f for f in os.listdir(target_wavs) if f.endswith('.wav')])}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Warning: Audio directory not found at {source_wavs}\")\n",
    "\n",
    "# Run dataset preparation script\n",
    "!python prepare_dataset_sinhala.py \\\n",
    "    --kaggle_path {kaggle_dataset_path} \\\n",
    "    --output_path {output_dataset_path}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… DATASET PREPARATION COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify output files\n",
    "train_metadata = f\"{output_dataset_path}/metadata_train.csv\"\n",
    "eval_metadata = f\"{output_dataset_path}/metadata_eval.csv\"\n",
    "\n",
    "if os.path.exists(train_metadata):\n",
    "    import pandas as pd\n",
    "    df_train = pd.read_csv(train_metadata, sep='|', header=None)\n",
    "    print(f\"\\nâœ… Training samples: {len(df_train)}\")\n",
    "    \n",
    "if os.path.exists(eval_metadata):\n",
    "    df_eval = pd.read_csv(eval_metadata, sep='|', header=None)\n",
    "    print(f\"âœ… Evaluation samples: {len(df_eval)}\")\n",
    "\n",
    "# Verify audio files are accessible\n",
    "if os.path.exists(target_wavs):\n",
    "    audio_count = len([f for f in os.listdir(target_wavs) if f.endswith('.wav')])\n",
    "    print(f\"âœ… Audio files in working directory: {audio_count}\")\n",
    "    \n",
    "    # Check if metadata references can be found\n",
    "    if os.path.exists(train_metadata):\n",
    "        sample_row = df_train.iloc[0]\n",
    "        sample_audio = sample_row[0]  # First column is audio_file\n",
    "        sample_audio_path = os.path.join(output_dataset_path, sample_audio)\n",
    "        if os.path.exists(sample_audio_path):\n",
    "            print(f\"âœ… Sample audio file accessible: {sample_audio}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Warning: Sample audio file not found: {sample_audio_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add headers to derived XTTS CSVs (required by the GPT loader)\n",
    "import pandas as pd\n",
    "\n",
    "for split in [\"train\", \"eval\"]:\n",
    "    p = f\"/kaggle/working/datasets/metadata_{split}.csv\"\n",
    "    df = pd.read_csv(p, sep=\"|\", header=None, names=[\"audio_file\",\"text\",\"speaker_name\"])\n",
    "    df.to_csv(p, sep=\"|\", index=False)\n",
    "print(\"âœ… Added headers to metadata CSVs (audio_file|text|speaker_name)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:39:26.065422Z",
     "iopub.status.busy": "2025-11-06T09:39:26.065226Z",
     "iopub.status.idle": "2025-11-06T09:39:35.095358Z",
     "shell.execute_reply": "2025-11-06T09:39:35.094668Z",
     "shell.execute_reply.started": "2025-11-06T09:39:26.065404Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOWNLOADING XTTS-v2 MODEL FILES\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¹ Downloading config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 4.37kB [00:00, 8.38MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… config.json downloaded successfully\n",
      "\n",
      "ğŸ”¹ Downloading vocab.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.json: 361kB [00:00, 45.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… vocab.json downloaded successfully\n",
      "\n",
      "ğŸ”¹ Downloading model.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87G/1.87G [00:06<00:00, 274MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… model.pth downloaded successfully\n",
      "\n",
      "ğŸ”¹ Downloading dvae.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dvae.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211M/211M [00:00<00:00, 253MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dvae.pth downloaded successfully\n",
      "\n",
      "ğŸ”¹ Downloading mel_stats.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mel_stats.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.07k/1.07k [00:00<00:00, 7.23MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… mel_stats.pth downloaded successfully\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "âœ… config.json: 0.0 MB\n",
      "âœ… vocab.json: 0.3 MB\n",
      "âœ… model.pth: 1781.4 MB\n",
      "âœ… dvae.pth: 200.8 MB\n",
      "âœ… mel_stats.pth: 0.0 MB\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Download XTTS-v2 base model files\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DOWNLOADING XTTS-v2 MODEL FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define all required files from Hugging Face\n",
    "base_url = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/\"\n",
    "\n",
    "files_to_download = {\n",
    "    \"config.json\": f\"{base_url}config.json\",\n",
    "    \"vocab.json\": f\"{base_url}vocab.json\",\n",
    "    \"model.pth\": f\"{base_url}model.pth\",\n",
    "    \"dvae.pth\": f\"{base_url}dvae.pth\",\n",
    "    \"mel_stats.pth\": f\"{base_url}mel_stats.pth\",\n",
    "}\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    '''Download file with progress bar'''\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=os.path.basename(output_path)) as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "# Download each file\n",
    "for filename, url in files_to_download.items():\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "        print(f\"âœ… {filename} already exists ({size_mb:.1f} MB), skipping...\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ”¹ Downloading {filename}...\")\n",
    "        try:\n",
    "            download_file(url, output_path)\n",
    "            print(f\"âœ… {filename} downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to download {filename}: {e}\")\n",
    "\n",
    "# Verify all files downloaded\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"VERIFICATION\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "all_downloaded = True\n",
    "for filename in files_to_download.keys():\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"âœ… {filename}: {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"âŒ {filename}: MISSING!\")\n",
    "        all_downloaded = False\n",
    "\n",
    "if all_downloaded:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"âœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\")\n",
    "    print(f\"{'=' * 80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:39:35.096347Z",
     "iopub.status.busy": "2025-11-06T09:39:35.096104Z",
     "iopub.status.idle": "2025-11-06T09:39:35.884510Z",
     "shell.execute_reply": "2025-11-06T09:39:35.883495Z",
     "shell.execute_reply.started": "2025-11-06T09:39:35.096322Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTENDING VOCABULARY FOR SINHALA\n",
      "================================================================================\n",
      "âœ… Metadata file found: /kaggle/working/datasets/metadata_train.csv\n",
      "âœ… Output path exists: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n",
      "\n",
      "ğŸ”¹ Running extend_vocab_sinhala.py...\n",
      "\n",
      "================================================================================\n",
      "SINHALA VOCABULARY EXTENSION FOR XTTS-v2\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   - Language: si (Sinhala)\n",
      "   - Vocabulary size: 15,000 tokens\n",
      "   - Metadata: /kaggle/working/datasets/metadata_train.csv\n",
      "   - Output: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 1: LOADING SINHALA TEXTS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“– Loading Sinhala texts from: /kaggle/working/datasets/metadata_train.csv\n",
      "âœ… CSV loaded successfully\n",
      "âœ… Loaded 1000 Sinhala text samples\n",
      "\n",
      "ğŸ“ Sample Sinhala texts:\n",
      "  1. à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.\n",
      "  2. à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.\n",
      "  3. à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š à¶»à¶§à¶šà·’.\n",
      "\n",
      "ğŸ“Š Text Statistics:\n",
      "   - Total characters: 48,540\n",
      "   - Sinhala characters: 40,716 (83.9%)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 2: TRAINING BPE TOKENIZER\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "================================================================================\n",
      "TRAINING SINHALA BPE TOKENIZER\n",
      "Target Vocabulary Size: 15000\n",
      "================================================================================\n",
      "\n",
      "âœ… Tokenizer initialized with ByteLevel pre-tokenizer\n",
      "âœ… BpeTrainer configured\n",
      "   - Min frequency: 1\n",
      "   - Special tokens: 5\n",
      "\n",
      "ğŸ”§ Training BPE tokenizer on 1000 Sinhala texts...\n",
      "\u001b[2K[00:00:00] Tokenize words                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1508     /     1508[00:00:00] Tokenize words                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0        /        0\n",
      "\u001b[2K[00:00:00] Count pairs                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1508     /     1508\n",
      "\u001b[2K[00:00:00] Compute merges                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1599     /     1599\n",
      "\n",
      "âœ… Training completed!\n",
      "   - Actual vocabulary size: 1,660 tokens\n",
      "\n",
      "âœ… Testing tokenization on samples:\n",
      "   Text: à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.     â†’  35 tokens [âœ… No UNK]\n",
      "   Text: à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.      â†’  29 tokens [âœ… No UNK]\n",
      "   Text: à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š â†’  42 tokens [âœ… No UNK]\n",
      "\n",
      "   âœ… EXCELLENT: No UNK tokens in test samples!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 3: SAVING VOCABULARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "================================================================================\n",
      "SAVING VOCABULARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Output directory created: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n",
      "âœ… Vocabulary saved: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\n",
      "   - File size: 45.6 KB\n",
      "   - Tokens: 1,660\n",
      "âœ… Tokenizer saved: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/tokenizer.json\n",
      "   - File size: 98.0 KB\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 4: UPDATING CONFIG\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Searching for config.json...\n",
      "âœ… Found: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n",
      "\n",
      "================================================================================\n",
      "UPDATING CONFIG.JSON FOR SINHALA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading config from: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n",
      "âœ… Config loaded successfully\n",
      "\n",
      "âœ… Added language_ids:\n",
      "   - Language: si\n",
      "   - ID: 21\n",
      "âœ… Added language settings:\n",
      "   - Phoneme language: None (grapheme-based)\n",
      "   - Use phonemes: False\n",
      "   - Language name: Sinhala\n",
      "\n",
      "âœ… Config saved successfully: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n",
      "\n",
      "ğŸ“Š Languages in config: ['si']\n",
      "\n",
      "================================================================================\n",
      "âœ… VOCABULARY EXTENSION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Output files created:\n",
      "   âœ… /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\n",
      "   âœ… /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/tokenizer.json\n",
      "   âœ… /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json (updated)\n",
      "\n",
      "ğŸš€ Ready for GPT fine-tuning!\n",
      "   Use command:\n",
      "   CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n",
      "     --output_path /kaggle/working/checkpoints/XTTS_v2.0_original_model_files \\\n",
      "     --metadatas <path_to_train>,<path_to_eval>,si \\\n",
      "     --num_epochs 5 \\\n",
      "     --batch_size 8\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "âœ… Vocabulary extension completed!\n",
      "\n",
      "âœ… Extended vocabulary size: 1,660 tokens\n",
      "â„¹ï¸ Sinhala-specific tokens: 0\n",
      "   (Note: ByteLevel BPE works at byte level, so explicit Sinhala tokens may not appear)\n",
      "   âœ… What matters: No UNK tokens = tokenizer can handle Sinhala text correctly\n",
      "âœ… Sinhala language (si) added to config.json\n",
      "   Language ID: 21\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Extend vocabulary for Sinhala language\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTENDING VOCABULARY FOR SINHALA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paths\n",
    "metadata_path = \"/kaggle/working/datasets/metadata_train.csv\"\n",
    "output_path = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(metadata_path):\n",
    "    print(f\"âŒ Error: Metadata file not found: {metadata_path}\")\n",
    "else:\n",
    "    print(f\"âœ… Metadata file found: {metadata_path}\")\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    print(f\"âŒ Error: Output path not found: {output_path}\")\n",
    "else:\n",
    "    print(f\"âœ… Output path exists: {output_path}\")\n",
    "\n",
    "# Run extend_vocab_sinhala.py\n",
    "print(f\"\\nğŸ”¹ Running extend_vocab_sinhala.py...\")\n",
    "!python extend_vocab_sinhala.py \\\n",
    "    --metadata_path {metadata_path} \\\n",
    "    --output_path {output_path} \\\n",
    "    --language si \\\n",
    "    --vocab_size 15000\n",
    "\n",
    "print(\"\\nâœ… Vocabulary extension completed!\")\n",
    "\n",
    "# Verify the extended vocab\n",
    "vocab_path = os.path.join(output_path, \"vocab.json\")\n",
    "if os.path.exists(vocab_path):\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    print(f\"\\nâœ… Extended vocabulary size: {len(vocab):,} tokens\")\n",
    "    \n",
    "    # Check for Sinhala characters in vocab\n",
    "    # Note: ByteLevel BPE works at byte level, so explicit Sinhala tokens may not appear\n",
    "    # This is OK - what matters is that there are no UNK tokens\n",
    "    sinhala_tokens = [token for token in vocab.keys() if any('\\u0D80' <= char <= '\\u0DFF' for char in token)]\n",
    "    print(f\"â„¹ï¸ Sinhala-specific tokens: {len(sinhala_tokens)}\")\n",
    "    print(f\"   (Note: ByteLevel BPE works at byte level, so explicit Sinhala tokens may not appear)\")\n",
    "    print(f\"   âœ… What matters: No UNK tokens = tokenizer can handle Sinhala text correctly\")\n",
    "    \n",
    "    # Verify config.json was updated\n",
    "    config_path = os.path.join(output_path, \"config.json\")\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        if 'language_ids' in config and 'si' in config['language_ids']:\n",
    "            print(f\"âœ… Sinhala language (si) added to config.json\")\n",
    "            print(f\"   Language ID: {config['language_ids']['si']}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Vocabulary file not found at: {vocab_path}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Understanding Vocabulary Results\n",
    "\n",
    "**About \"Sinhala-specific tokens: 0\":**\n",
    "- âœ… **This is OK!** ByteLevel BPE tokenizer works at the **byte level**, not character level\n",
    "- It doesn't need explicit Sinhala character tokens because it encodes Unicode bytes\n",
    "- The important thing is: **No UNK tokens** = tokenizer can handle all Sinhala text\n",
    "\n",
    "**About vocabulary size (921 vs 15000):**\n",
    "- The actual vocabulary size depends on the dataset size and diversity\n",
    "- With 1000 samples, 921 tokens is reasonable\n",
    "- The tokenizer will still work correctly as long as there are no UNK tokens\n",
    "- For larger datasets, you'll get closer to the target size\n",
    "\n",
    "**What matters:**\n",
    "- âœ… No UNK tokens in test samples = tokenizer is working correctly\n",
    "- âœ… All Sinhala text can be tokenized properly\n",
    "- âœ… Ready for training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:39:35.886356Z",
     "iopub.status.busy": "2025-11-06T09:39:35.885731Z",
     "iopub.status.idle": "2025-11-06T09:40:41.792516Z",
     "shell.execute_reply": "2025-11-06T09:40:41.791514Z",
     "shell.execute_reply.started": "2025-11-06T09:39:35.886330Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "STARTING COMPLETE TRAINING PIPELINE\n",
      "================================================================================\n",
      "âœ… Found kaggle_train_sinhala.py\n",
      "\n",
      "ğŸš€ Starting training pipeline...\n",
      "This will run all phases:\n",
      "  1. Setup verification\n",
      "  2. Dataset preparation\n",
      "  3. Model download\n",
      "  4. Vocabulary extension\n",
      "  5. GPT fine-tuning\n",
      "\n",
      "âš ï¸ This may take several hours...\n",
      "\n",
      "================================================================================\n",
      "XTTS-v2 SINHALA FINE-TUNING PIPELINE\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "PHASE 1: SETUP VERIFICATION\n",
      "============================================================\n",
      "\n",
      "================================================================================\n",
      "SINHALA XTTS-v2 KAGGLE TRAINING CONFIGURATION\n",
      "================================================================================\n",
      "\n",
      "[Language Settings]\n",
      "  Language Code: si\n",
      "  Language Name: Sinhala\n",
      "  Script: Sinhala script (abugida)\n",
      "\n",
      "[Path Settings]\n",
      "  Dataset: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n",
      "  Working: /kaggle/working\n",
      "  Output: /kaggle/working/datasets\n",
      "  Checkpoints: /kaggle/working/checkpoints\n",
      "\n",
      "[Vocabulary]\n",
      "  Type: ByteLevel BPE\n",
      "  Size: 15,000 tokens\n",
      "  Min Frequency: 2\n",
      "  Special Tokens: 5\n",
      "\n",
      "[Training Parameters]\n",
      "  Epochs: 5\n",
      "  Batch Size: 8\n",
      "  Gradient Accumulation: 4\n",
      "  Effective Batch: 32\n",
      "  Learning Rate: 5e-06\n",
      "  Weight Decay: 1e-02\n",
      "  Save Step: 50,000\n",
      "\n",
      "[Text & Audio]\n",
      "  Max Text Length: 400 tokens\n",
      "  Max Audio Length: 330750 samples (~13.8s)\n",
      "  Sample Rate: 22050 Hz (training)\n",
      "  Output Sample Rate: 24000 Hz\n",
      "\n",
      "[Computation]\n",
      "  Optimizer: AdamW\n",
      "  Use DeepSpeed: False\n",
      "  Loader Workers: 8\n",
      "\n",
      "[Logging & Monitoring]\n",
      "  Dashboard: tensorboard\n",
      "  Run Name: GPT_XTTS_FT_Sinhala\n",
      "  Project: XTTS_Sinhala_Trainer\n",
      "\n",
      "================================================================================\n",
      "âœ… Configuration is VALID\n",
      "\n",
      "\n",
      "[1/4] PyTorch Version: 2.1.0+cu118\n",
      "[2/4] CUDA Available: True\n",
      "    CUDA Version: 11.8\n",
      "    GPU Count: 1\n",
      "    GPU 0: Tesla P100-PCIE-16GB\n",
      "      Memory: 15.89 GB\n",
      "\n",
      "[3/4] Checking paths\n",
      "    dataset_path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n",
      "    output_path: /kaggle/working/datasets\n",
      "      âœ“ Directory exists/created\n",
      "    checkpoint_path: /kaggle/working/checkpoints\n",
      "      âœ“ Directory exists/created\n",
      "    model_files_path: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n",
      "      âœ“ Directory exists/created\n",
      "    training_output: /kaggle/working/checkpoints/run/training\n",
      "      âœ“ Directory exists/created\n",
      "\n",
      "[4/4] Python Version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "\n",
      "============================================================\n",
      "PHASE 1 COMPLETED\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PHASE 2: DATASET PREPARATION\n",
      "============================================================\n",
      "\n",
      "[1/2] Preparing dataset from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n",
      "[2/2] Running prepare_dataset_sinhala.py\n",
      "    Command: /usr/bin/python3 prepare_dataset_sinhala.py --kaggle_path /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset --output_path /kaggle/working/datasets\n",
      "\n",
      "============================================================\n",
      "CONVERTING DATASET TO XTTS-v2 FORMAT\n",
      "============================================================\n",
      "\n",
      "[2/3] Reading metadata files\n",
      "    Train: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_train.csv\n",
      "    Eval: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_eval.csv\n",
      "    Train samples: 1000\n",
      "    Eval samples: 251\n",
      "    Detected pipe-separated format in train CSV\n",
      "    Detected pipe-separated format in eval CSV\n",
      "\n",
      "[3/3] Converting train metadata\n",
      "\n",
      "[1/3] Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs\n",
      "    âœ“ Copied 1000 files\n",
      "    âœ“ Saved 1000 samples to /kaggle/working/datasets/metadata_train.csv\n",
      "    Format: audio_file|text|speaker_name\n",
      "\n",
      "[3/3] Converting eval metadata\n",
      "\n",
      "[1/3] Copying audio files from /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wavs to /kaggle/working/datasets/wavs\n",
      "    âœ“ Copied 251 files\n",
      "    âœ“ Saved 251 samples to /kaggle/working/datasets/metadata_eval.csv\n",
      "    Format: audio_file|text|speaker_name\n",
      "\n",
      "[Verification] Checking output files\n",
      "    âœ“ metadata_train.csv: Format OK\n",
      "      Sample: wavs/sin_2282_sin_2282_8427486285.wav|à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Š...\n",
      "    âœ“ metadata_eval.csv: Format OK\n",
      "      Sample: wavs/sin_9228_sin_9228_8967768400.wav|à¶¸à·™à¶¸ à¶šà·’à¶ºà¶¸à¶±à·Š à¶­à·”à·… à¶‡à¶­à·’ à¶ºà¶®à·à¶»à·Šà¶®à¶º à·€à·’à¶¸à·ƒà· à¶¶à·à¶½à·”à·€ à·„à·œà¶­...\n",
      "\n",
      "============================================================\n",
      "DATASET PREPARATION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "Output directory: /kaggle/working/datasets\n",
      "  - metadata_train.csv (1000 samples)\n",
      "  - metadata_eval.csv (251 samples)\n",
      "  - wavs/ (audio files)\n",
      "\n",
      "\n",
      "    âœ“ Training metadata: /kaggle/working/datasets/metadata_train.csv\n",
      "    âœ“ Evaluation metadata: /kaggle/working/datasets/metadata_eval.csv\n",
      "\n",
      "============================================================\n",
      "PHASE 2 COMPLETED\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PHASE 3: DOWNLOAD XTTS-v2 MODEL\n",
      "============================================================\n",
      "\n",
      "[1/2] Downloading XTTS-v2 model files to /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n",
      "[2/2] Using download_checkpoint.py\n",
      "\n",
      "    âœ“ Download completed\n",
      "\n",
      "    âœ“ All model files downloaded successfully\n",
      "      - dvae.pth (200.8 MB)\n",
      "      - mel_stats.pth (0.0 MB)\n",
      "      - vocab.json (0.0 MB)\n",
      "      - model.pth (1781.4 MB)\n",
      "      - config.json (0.0 MB)\n",
      "\n",
      "============================================================\n",
      "PHASE 3 COMPLETED\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PHASE 4: VOCABULARY EXTENSION\n",
      "============================================================\n",
      "\n",
      "[1/2] Extending vocabulary for Sinhala language\n",
      "[2/2] Running extend_vocab_sinhala.py\n",
      "    Command: /usr/bin/python3 extend_vocab_sinhala.py --metadata_path /kaggle/working/datasets/metadata_train.csv --output_path /kaggle/working/checkpoints --language si --vocab_size 15000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SINHALA VOCABULARY EXTENSION FOR XTTS-v2\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   - Language: si (Sinhala)\n",
      "   - Vocabulary size: 15,000 tokens\n",
      "   - Metadata: /kaggle/working/datasets/metadata_train.csv\n",
      "   - Output: /kaggle/working/checkpoints\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 1: LOADING SINHALA TEXTS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“– Loading Sinhala texts from: /kaggle/working/datasets/metadata_train.csv\n",
      "âœ… CSV loaded successfully\n",
      "âœ… Loaded 1000 Sinhala text samples\n",
      "\n",
      "ğŸ“ Sample Sinhala texts:\n",
      "  1. à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.\n",
      "  2. à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.\n",
      "  3. à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š à¶»à¶§à¶šà·’.\n",
      "\n",
      "ğŸ“Š Text Statistics:\n",
      "   - Total characters: 48,540\n",
      "   - Sinhala characters: 40,716 (83.9%)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 2: TRAINING BPE TOKENIZER\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "================================================================================\n",
      "TRAINING SINHALA BPE TOKENIZER\n",
      "Target Vocabulary Size: 15000\n",
      "================================================================================\n",
      "\n",
      "âœ… Tokenizer initialized with ByteLevel pre-tokenizer\n",
      "âœ… BpeTrainer configured\n",
      "   - Min frequency: 1\n",
      "   - Special tokens: 5\n",
      "\n",
      "ğŸ”§ Training BPE tokenizer on 1000 Sinhala texts...\n",
      "\n",
      "âœ… Training completed!\n",
      "   - Actual vocabulary size: 1,660 tokens\n",
      "\n",
      "âœ… Testing tokenization on samples:\n",
      "   Text: à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.     â†’  35 tokens [âœ… No UNK]\n",
      "   Text: à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.      â†’  29 tokens [âœ… No UNK]\n",
      "   Text: à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š â†’  42 tokens [âœ… No UNK]\n",
      "\n",
      "   âœ… EXCELLENT: No UNK tokens in test samples!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 3: SAVING VOCABULARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "================================================================================\n",
      "SAVING VOCABULARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Output directory created: /kaggle/working/checkpoints\n",
      "âœ… Vocabulary saved: /kaggle/working/checkpoints/vocab.json\n",
      "   - File size: 45.6 KB\n",
      "   - Tokens: 1,660\n",
      "âœ… Tokenizer saved: /kaggle/working/checkpoints/tokenizer.json\n",
      "   - File size: 98.0 KB\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 4: UPDATING CONFIG\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Searching for config.json...\n",
      "   â„¹ï¸ Not found: /kaggle/working/checkpoints/config.json\n",
      "   â„¹ï¸ Not found: /kaggle/working/config.json\n",
      "âœ… Found: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n",
      "\n",
      "================================================================================\n",
      "UPDATING CONFIG.JSON FOR SINHALA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading config from: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n",
      "âœ… Config loaded successfully\n",
      "\n",
      "âœ… Added language_ids:\n",
      "   - Language: si\n",
      "   - ID: 22\n",
      "âœ… Added language settings:\n",
      "   - Phoneme language: None (grapheme-based)\n",
      "   - Use phonemes: False\n",
      "   - Language name: Sinhala\n",
      "\n",
      "âœ… Config saved successfully: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n",
      "\n",
      "ğŸ“Š Languages in config: ['si']\n",
      "\n",
      "================================================================================\n",
      "âœ… VOCABULARY EXTENSION COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "ğŸ“ Output files created:\n",
      "   âœ… /kaggle/working/checkpoints/vocab.json\n",
      "   âœ… /kaggle/working/checkpoints/tokenizer.json\n",
      "   âœ… /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json (updated)\n",
      "\n",
      "ğŸš€ Ready for GPT fine-tuning!\n",
      "   Use command:\n",
      "   CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n",
      "     --output_path /kaggle/working/checkpoints \\\n",
      "     --metadatas <path_to_train>,<path_to_eval>,si \\\n",
      "     --num_epochs 5 \\\n",
      "     --batch_size 8\n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "\n",
      "    âœ“ Vocabulary extended successfully\n",
      "    âœ“ Language 'si' added to config.json\n",
      "\n",
      "============================================================\n",
      "PHASE 4 COMPLETED\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "PHASE 5: DVAE FINE-TUNING (SKIPPED)\n",
      "============================================================\n",
      "Info: Use --enable_dvae to run DVAE fine-tuning (recommended only for large datasets >20h)\n",
      "\n",
      "============================================================\n",
      "PHASE 6: GPT FINE-TUNING\n",
      "============================================================\n",
      "\n",
      "[1/3] Preparing GPT fine-tuning\n",
      "    Training metadata: /kaggle/working/datasets/metadata_train.csv\n",
      "    Evaluation metadata: /kaggle/working/datasets/metadata_eval.csv\n",
      "    Language: si\n",
      "\n",
      "[2/3] Running GPT fine-tuning\n",
      "    Batch size: 8\n",
      "    Gradient accumulation: 4\n",
      "    Learning rate: 5e-06\n",
      "    Epochs: 5\n",
      "\n",
      "[3/3] Command: /usr/bin/python3 train_gpt_xtts.py --output_path /kaggle/working/checkpoints/run/training --metadatas /kaggle/working/datasets/metadata_train.csv,/kaggle/working/datasets/metadata_eval.csv,si --num_epochs 5 --batch_size 8 --grad_acumm 4 --max_audio_length 330750 --max_text_length 400 --lr 5e-06 --weight_decay 0.01 --save_step 50000\n",
      "    Starting training...\n",
      "    (This may take several hours)\n",
      "\n",
      "================================================================================\n",
      "XTTS-v2 GPT FINE-TUNING FOR SINHALA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   - Output path: /kaggle/working/checkpoints/run/training\n",
      "   - Number of epochs: 5\n",
      "   - Batch size: 8\n",
      "   - Gradient accumulation: 4\n",
      "   - Learning rate: 5e-06\n",
      "   - Weight decay: 0.01\n",
      "   - Max audio length: 330750\n",
      "   - Max text length: 400\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "PROCESSING DATASETS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“‚ Processing metadata spec: /kaggle/working/datasets/metadata_train.csv,/kaggle/working/datasets/metadata_eval.csv,si\n",
      "   âœ… Train CSV: metadata_train.csv\n",
      "   âœ… Eval CSV: metadata_eval.csv\n",
      "   âœ… Language: si\n",
      "   âœ… Dataset config created\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "DOWNLOADING XTTS-v2 MODEL FILES\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“ Checkpoint directory: /kaggle/working/checkpoints/run/training/XTTS_v2.0_original_model_files/\n",
      "\n",
      "ğŸ“¥ Downloading DVAE files...\n",
      "  0%|                                              | 0.00/1.07k [00:00<?, ?iB/s]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.07k/1.07k [00:00<00:00, 3.04kiB/s]\u001b[A\n",
      "\n",
      "  4%|â–ˆâ–Œ                                    | 8.63M/211M [00:00<00:02, 86.3MiB/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–‹                                   | 19.9M/211M [00:00<00:01, 102MiB/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 31.2M/211M [00:00<00:01, 107MiB/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 42.5M/211M [00:00<00:01, 109MiB/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                             | 53.7M/211M [00:00<00:01, 110MiB/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                           | 64.9M/211M [00:00<00:01, 111MiB/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 76.2M/211M [00:00<00:01, 112MiB/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 87.3M/211M [00:00<00:01, 111MiB/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 98.5M/211M [00:00<00:01, 111MiB/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 110M/211M [00:01<00:00, 109MiB/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                 | 120M/211M [00:01<00:00, 107MiB/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 131M/211M [00:01<00:00, 106MiB/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰             | 142M/211M [00:01<00:00, 106MiB/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           | 153M/211M [00:01<00:00, 108MiB/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 164M/211M [00:01<00:00, 110MiB/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 176M/211M [00:01<00:00, 110MiB/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 187M/211M [00:01<00:00, 112MiB/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 198M/211M [00:01<00:00, 112MiB/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 210M/211M [00:01<00:00, 113MiB/s]\u001b[Aâœ… DVAE files downloaded\n",
      "\n",
      "ğŸ“¥ Downloading XTTS-v2 tokenizer...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211M/211M [00:02<00:00, 96.1MiB/s]\n",
      "âœ… Tokenizer downloaded\n",
      "\n",
      "ğŸ“¥ Downloading XTTS-v2 checkpoint...\n",
      "\n",
      "361kiB [00:00, 1.07MiB/s]                          | 0.00/1.87G [00:00<?, ?iB/s]\u001b[A\n",
      "\n",
      "  0%|â–                                    | 8.11M/1.87G [00:00<00:22, 81.1MiB/s]\u001b[A\n",
      "  1%|â–                                     | 19.7M/1.87G [00:00<00:18, 102MiB/s]\u001b[A\n",
      "  2%|â–Œ                                     | 30.4M/1.87G [00:00<00:17, 104MiB/s]\u001b[A\n",
      "  2%|â–Š                                     | 41.5M/1.87G [00:00<00:17, 107MiB/s]\u001b[A\n",
      "  3%|â–ˆ                                     | 52.8M/1.87G [00:00<00:16, 109MiB/s]\u001b[A\n",
      "  3%|â–ˆâ–                                    | 64.0M/1.87G [00:00<00:16, 110MiB/s]\u001b[A\n",
      "  4%|â–ˆâ–Œ                                    | 75.2M/1.87G [00:00<00:16, 111MiB/s]\u001b[A\n",
      "  5%|â–ˆâ–Š                                    | 86.8M/1.87G [00:00<00:15, 112MiB/s]\u001b[A\n",
      "  5%|â–ˆâ–ˆ                                    | 98.3M/1.87G [00:00<00:15, 113MiB/s]\u001b[A\n",
      "  6%|â–ˆâ–ˆâ–                                    | 110M/1.87G [00:01<00:15, 116MiB/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–Œ                                    | 123M/1.87G [00:01<00:14, 117MiB/s]\u001b[A\n",
      "  7%|â–ˆâ–ˆâ–Š                                    | 135M/1.87G [00:01<00:14, 119MiB/s]\u001b[A\n",
      "  8%|â–ˆâ–ˆâ–ˆ                                    | 147M/1.87G [00:01<00:14, 120MiB/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–                                   | 159M/1.87G [00:01<00:14, 121MiB/s]\u001b[A\n",
      "  9%|â–ˆâ–ˆâ–ˆâ–Œ                                   | 171M/1.87G [00:01<00:14, 120MiB/s]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–Š                                   | 183M/1.87G [00:01<00:13, 120MiB/s]\u001b[A\n",
      " 10%|â–ˆâ–ˆâ–ˆâ–ˆ                                   | 195M/1.87G [00:01<00:13, 120MiB/s]\u001b[A\n",
      " 11%|â–ˆâ–ˆâ–ˆâ–ˆâ–                                  | 207M/1.87G [00:01<00:13, 120MiB/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                  | 219M/1.87G [00:01<00:13, 120MiB/s]\u001b[A\n",
      " 12%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š                                  | 231M/1.87G [00:02<00:13, 120MiB/s]\u001b[A\n",
      " 13%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                  | 244M/1.87G [00:02<00:13, 120MiB/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                 | 256M/1.87G [00:02<00:13, 115MiB/s]\u001b[A\n",
      " 14%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                 | 267M/1.87G [00:02<00:14, 114MiB/s]\u001b[A\n",
      " 15%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                 | 279M/1.87G [00:02<00:14, 113MiB/s]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                 | 290M/1.87G [00:02<00:14, 113MiB/s]\u001b[A\n",
      " 16%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                                | 301M/1.87G [00:02<00:13, 112MiB/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                | 312M/1.87G [00:02<00:13, 112MiB/s]\u001b[A\n",
      " 17%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                                | 324M/1.87G [00:02<00:13, 112MiB/s]\u001b[A\n",
      " 18%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                                | 335M/1.87G [00:02<00:13, 112MiB/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 346M/1.87G [00:03<00:13, 111MiB/s]\u001b[A\n",
      " 19%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                               | 357M/1.87G [00:03<00:13, 112MiB/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                               | 369M/1.87G [00:03<00:13, 112MiB/s]\u001b[A\n",
      " 20%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                               | 380M/1.87G [00:03<00:13, 112MiB/s]\u001b[A\n",
      " 21%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 391M/1.87G [00:03<00:13, 111MiB/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                              | 402M/1.87G [00:03<00:13, 110MiB/s]\u001b[A\n",
      " 22%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                              | 413M/1.87G [00:03<00:13, 111MiB/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                              | 424M/1.87G [00:03<00:14, 102MiB/s]\u001b[A\n",
      " 23%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              | 436M/1.87G [00:03<00:13, 105MiB/s]\u001b[A\n",
      " 24%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                             | 447M/1.87G [00:03<00:13, 107MiB/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                             | 458M/1.87G [00:04<00:13, 108MiB/s]\u001b[A\n",
      " 25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                             | 469M/1.87G [00:04<00:12, 110MiB/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                             | 481M/1.87G [00:04<00:12, 111MiB/s]\u001b[A\n",
      " 26%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 492M/1.87G [00:04<00:12, 111MiB/s]\u001b[A\n",
      " 27%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                            | 503M/1.87G [00:04<00:12, 111MiB/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                            | 514M/1.87G [00:04<00:12, 111MiB/s]\u001b[A\n",
      " 28%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                            | 525M/1.87G [00:04<00:12, 112MiB/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 537M/1.87G [00:04<00:11, 111MiB/s]\u001b[A\n",
      " 29%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                           | 548M/1.87G [00:04<00:11, 111MiB/s]\u001b[A\n",
      " 30%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                           | 559M/1.87G [00:04<00:11, 111MiB/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                           | 570M/1.87G [00:05<00:11, 111MiB/s]\u001b[A\n",
      " 31%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 581M/1.87G [00:05<00:11, 112MiB/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                          | 592M/1.87G [00:05<00:11, 111MiB/s]\u001b[A\n",
      " 32%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                          | 604M/1.87G [00:05<00:11, 111MiB/s]\u001b[A\n",
      " 33%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                          | 615M/1.87G [00:05<00:11, 111MiB/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                          | 626M/1.87G [00:05<00:11, 111MiB/s]\u001b[A\n",
      " 34%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                         | 637M/1.87G [00:05<00:11, 111MiB/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                         | 648M/1.87G [00:05<00:10, 111MiB/s]\u001b[A\n",
      " 35%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                         | 659M/1.87G [00:05<00:10, 111MiB/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                         | 671M/1.87G [00:05<00:11, 108MiB/s]\u001b[A\n",
      " 36%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 681M/1.87G [00:06<00:11, 104MiB/s]\u001b[A\n",
      " 37%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                        | 693M/1.87G [00:06<00:11, 107MiB/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                        | 703M/1.87G [00:06<00:10, 107MiB/s]\u001b[A\n",
      " 38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                        | 715M/1.87G [00:06<00:10, 108MiB/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 726M/1.87G [00:06<00:10, 109MiB/s]\u001b[A\n",
      " 39%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                       | 737M/1.87G [00:06<00:10, 110MiB/s]\u001b[A\n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                       | 748M/1.87G [00:06<00:10, 111MiB/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                       | 759M/1.87G [00:06<00:10, 111MiB/s]\u001b[A\n",
      " 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                       | 770M/1.87G [00:06<00:09, 111MiB/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                      | 781M/1.87G [00:06<00:09, 111MiB/s]\u001b[A\n",
      " 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                      | 793M/1.87G [00:07<00:09, 111MiB/s]\u001b[A\n",
      " 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                      | 804M/1.87G [00:07<00:09, 111MiB/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                      | 815M/1.87G [00:07<00:09, 111MiB/s]\u001b[A\n",
      " 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 826M/1.87G [00:07<00:09, 111MiB/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                     | 837M/1.87G [00:07<00:09, 110MiB/s]\u001b[A\n",
      " 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                     | 848M/1.87G [00:07<00:09, 111MiB/s]\u001b[A\n",
      " 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                     | 860M/1.87G [00:07<00:09, 111MiB/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 871M/1.87G [00:07<00:08, 111MiB/s]\u001b[A\n",
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                    | 882M/1.87G [00:07<00:08, 112MiB/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                    | 893M/1.87G [00:08<00:08, 112MiB/s]\u001b[A\n",
      " 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰                    | 905M/1.87G [00:08<00:08, 112MiB/s]\u001b[A\n",
      " 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                    | 916M/1.87G [00:08<00:08, 112MiB/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                   | 927M/1.87G [00:08<00:08, 112MiB/s]\u001b[A\n",
      " 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                   | 938M/1.87G [00:08<00:08, 109MiB/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                   | 949M/1.87G [00:08<00:08, 107MiB/s]\u001b[A\n",
      " 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                   | 960M/1.87G [00:08<00:08, 105MiB/s]\u001b[A\n",
      " 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 970M/1.87G [00:08<00:08, 105MiB/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                  | 981M/1.87G [00:08<00:08, 104MiB/s]\u001b[A\n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                  | 991M/1.87G [00:08<00:08, 103MiB/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 1.00G/1.87G [00:09<00:08, 103MiB/s]\u001b[A\n",
      " 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                 | 1.01G/1.87G [00:09<00:08, 103MiB/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                 | 1.02G/1.87G [00:09<00:08, 103MiB/s]\u001b[A\n",
      " 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                 | 1.03G/1.87G [00:09<00:08, 102MiB/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 1.04G/1.87G [00:09<00:08, 102MiB/s]\u001b[A\n",
      " 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                | 1.05G/1.87G [00:09<00:07, 102MiB/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                | 1.06G/1.87G [00:09<00:07, 103MiB/s]\u001b[A\n",
      " 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                | 1.07G/1.87G [00:09<00:07, 103MiB/s]\u001b[A\n",
      " 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                | 1.08G/1.87G [00:09<00:07, 102MiB/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 1.09G/1.87G [00:09<00:07, 102MiB/s]\u001b[A\n",
      " 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–               | 1.10G/1.87G [00:10<00:07, 102MiB/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹               | 1.11G/1.87G [00:10<00:07, 102MiB/s]\u001b[A\n",
      " 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰               | 1.12G/1.87G [00:10<00:07, 101MiB/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ               | 1.13G/1.87G [00:10<00:07, 101MiB/s]\u001b[A\n",
      " 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–              | 1.15G/1.87G [00:10<00:07, 102MiB/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ              | 1.16G/1.87G [00:10<00:07, 102MiB/s]\u001b[A\n",
      " 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹              | 1.17G/1.87G [00:10<00:06, 101MiB/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰              | 1.18G/1.87G [00:10<00:06, 102MiB/s]\u001b[A\n",
      " 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ              | 1.19G/1.87G [00:10<00:06, 101MiB/s]\u001b[A\n",
      " 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–             | 1.20G/1.87G [00:10<00:06, 102MiB/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ             | 1.21G/1.87G [00:11<00:06, 102MiB/s]\u001b[A\n",
      " 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ             | 1.22G/1.87G [00:11<00:06, 94.4MiB/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 1.23G/1.87G [00:11<00:07, 89.5MiB/s]\u001b[A\n",
      " 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–            | 1.24G/1.87G [00:11<00:07, 87.6MiB/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹            | 1.24G/1.87G [00:11<00:07, 86.0MiB/s]\u001b[A\n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š            | 1.25G/1.87G [00:11<00:07, 84.5MiB/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰            | 1.26G/1.87G [00:11<00:07, 85.6MiB/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 1.27G/1.87G [00:11<00:07, 83.9MiB/s]\u001b[A\n",
      " 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 1.28G/1.87G [00:11<00:07, 83.0MiB/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 1.29G/1.87G [00:12<00:07, 81.4MiB/s]\u001b[A\n",
      " 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹           | 1.30G/1.87G [00:12<00:06, 82.2MiB/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š           | 1.30G/1.87G [00:12<00:07, 79.7MiB/s]\u001b[A\n",
      " 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰           | 1.31G/1.87G [00:12<00:06, 81.6MiB/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 1.32G/1.87G [00:12<00:06, 86.2MiB/s]\u001b[A\n",
      " 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–          | 1.33G/1.87G [00:12<00:05, 90.5MiB/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 1.34G/1.87G [00:12<00:05, 93.1MiB/s]\u001b[A\n",
      " 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š          | 1.35G/1.87G [00:12<00:05, 95.7MiB/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰          | 1.36G/1.87G [00:12<00:05, 97.5MiB/s]\u001b[A\n",
      " 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 1.37G/1.87G [00:12<00:05, 98.8MiB/s]\u001b[A\n",
      " 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–         | 1.38G/1.87G [00:13<00:04, 98.2MiB/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 1.39G/1.87G [00:13<00:04, 99.5MiB/s]\u001b[A\n",
      " 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ         | 1.40G/1.87G [00:13<00:04, 100MiB/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹         | 1.41G/1.87G [00:13<00:04, 100MiB/s]\u001b[A\n",
      " 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 1.42G/1.87G [00:13<00:04, 99.0MiB/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–        | 1.43G/1.87G [00:13<00:04, 98.9MiB/s]\u001b[A\n",
      " 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ        | 1.44G/1.87G [00:13<00:04, 99.9MiB/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š        | 1.45G/1.87G [00:13<00:04, 93.3MiB/s]\u001b[A\n",
      " 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰        | 1.46G/1.87G [00:13<00:04, 95.6MiB/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 1.47G/1.87G [00:13<00:04, 97.7MiB/s]\u001b[A\n",
      " 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 1.48G/1.87G [00:14<00:03, 99.1MiB/s]\u001b[A\n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–       | 1.49G/1.87G [00:14<00:03, 101MiB/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š       | 1.50G/1.87G [00:14<00:05, 66.4MiB/s]\u001b[A\n",
      " 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰       | 1.51G/1.87G [00:14<00:04, 74.4MiB/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 1.52G/1.87G [00:14<00:04, 81.0MiB/s]\u001b[A\n",
      " 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 1.53G/1.87G [00:14<00:03, 86.0MiB/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 1.54G/1.87G [00:14<00:03, 90.3MiB/s]\u001b[A\n",
      " 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š      | 1.56G/1.87G [00:14<00:03, 93.3MiB/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ      | 1.57G/1.87G [00:15<00:03, 96.0MiB/s]\u001b[A\n",
      " 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.58G/1.87G [00:15<00:02, 97.7MiB/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 1.59G/1.87G [00:15<00:02, 96.6MiB/s]\u001b[A\n",
      " 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 1.60G/1.87G [00:15<00:02, 97.0MiB/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 1.61G/1.87G [00:15<00:02, 97.6MiB/s]\u001b[A\n",
      " 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1.62G/1.87G [00:15<00:02, 97.5MiB/s]\u001b[A\n",
      " 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.63G/1.87G [00:15<00:02, 97.0MiB/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 1.63G/1.87G [00:15<00:02, 97.6MiB/s]\u001b[A\n",
      " 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 1.64G/1.87G [00:15<00:02, 98.3MiB/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 1.66G/1.87G [00:15<00:02, 98.9MiB/s]\u001b[A\n",
      " 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 1.66G/1.87G [00:16<00:02, 98.5MiB/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1.67G/1.87G [00:16<00:01, 98.4MiB/s]\u001b[A\n",
      " 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 1.68G/1.87G [00:16<00:01, 98.7MiB/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 1.69G/1.87G [00:16<00:01, 98.8MiB/s]\u001b[A\n",
      " 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 1.70G/1.87G [00:16<00:01, 98.3MiB/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 1.71G/1.87G [00:16<00:01, 97.8MiB/s]\u001b[A\n",
      " 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.72G/1.87G [00:16<00:01, 98.2MiB/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.73G/1.87G [00:16<00:01, 99.4MiB/s]\u001b[A\n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 1.74G/1.87G [00:16<00:01, 100MiB/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 1.75G/1.87G [00:16<00:01, 101MiB/s]\u001b[A\n",
      " 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 1.77G/1.87G [00:17<00:01, 101MiB/s]\u001b[A\n",
      " 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 1.78G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 1.79G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      " 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 1.80G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 1.81G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      " 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 1.82G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.83G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      " 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 1.84G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 1.85G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      " 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 1.86G/1.87G [00:17<00:00, 101MiB/s]\u001b[A\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 1.87G/1.87G [00:18<00:00, 103MiB/s]\u001b[Aâœ… Model checkpoint downloaded\n",
      "\n",
      "ğŸ“¥ Downloading XTTS-v2 config...\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87G/1.87G [00:18<00:00, 102MiB/s]\n",
      "âœ… Config file downloaded\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "CONFIGURING MODEL\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "âœ… GPT model arguments configured\n",
      "âœ… Audio configuration configured\n",
      "\n",
      "âœ… Base configuration loaded from checkpoint\n",
      "\n",
      "ğŸ“ Setting Sinhala-specific configuration:\n",
      "   âœ… use_phonemes = False (grapheme-based)\n",
      "   âœ… phoneme_language = None\n",
      "   âœ… text_cleaner = english_cleaners\n",
      "\n",
      "ğŸ“Š Training configuration updated:\n",
      "   - Optimizer: AdamW\n",
      "   - Learning rate: 5e-06\n",
      "   - Weight decay: 0.01\n",
      "   - Save step: 50000\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "INITIALIZING MODEL\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ”§ Initializing GPT model from config...\n",
      ">> DVAE weights restored from: /kaggle/working/checkpoints/run/training/XTTS_v2.0_original_model_files/dvae.pth\n",
      "âœ… Model initialized\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "LOADING TRAINING SAMPLES\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“¦ Loading training and evaluation samples...\n",
      "\n",
      "\n",
      "âŒ Error: \n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/XTTSv2-sinhala/train_gpt_xtts.py\", line 540, in main\n",
      "    trainer_out_path = train_gpt(\n",
      "                       ^^^^^^^^^^\n",
      "  File \"/kaggle/working/XTTSv2-sinhala/train_gpt_xtts.py\", line 454, in train_gpt\n",
      "    train_samples, eval_samples = load_tts_samples(\n",
      "                                  ^^^^^^^^^^^^^^^^^\n",
      "  File \"/kaggle/working/XTTSv2-sinhala/TTS/tts/datasets/__init__.py\", line 120, in load_tts_samples\n",
      "    meta_data_train = formatter(root_path, meta_file_train, ignored_speakers=ignored_speakers)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/kaggle/working/XTTSv2-sinhala/TTS/tts/datasets/formatters.py\", line 67, in coqui\n",
      "    assert all(x in metadata.columns for x in [\"audio_file\", \"text\"])\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError\n",
      "\u001b[0m\n",
      "    âŒ Training failed with return code 1\n",
      "\n",
      "================================================================================\n",
      "âŒ PIPELINE FAILED: GPT fine-tuning failed\n",
      "================================================================================\n",
      "Traceback (most recent call last):\n",
      "  File \"/kaggle/working/XTTSv2-sinhala/kaggle_train_sinhala.py\", line 403, in main\n",
      "    phase_6_gpt_finetuning(\n",
      "  File \"/kaggle/working/XTTSv2-sinhala/kaggle_train_sinhala.py\", line 352, in phase_6_gpt_finetuning\n",
      "    raise RuntimeError(\"GPT fine-tuning failed\")\n",
      "RuntimeError: GPT fine-tuning failed\n",
      "\n",
      "================================================================================\n",
      "âœ… TRAINING PIPELINE COMPLETED!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Run complete training pipeline using kaggle_train_sinhala.py\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STARTING COMPLETE TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify we're in the repo directory\n",
    "if not os.path.exists(\"kaggle_train_sinhala.py\"):\n",
    "    print(\"âŒ Error: kaggle_train_sinhala.py not found in current directory\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(\"\\nTrying to find it...\")\n",
    "    !find . -name \"kaggle_train_sinhala.py\" -type f\n",
    "else:\n",
    "    print(\"âœ… Found kaggle_train_sinhala.py\")\n",
    "    \n",
    "    # Run the complete training pipeline\n",
    "    print(\"\\nğŸš€ Starting training pipeline...\")\n",
    "    print(\"This will run all phases:\")\n",
    "    print(\"  1. Setup verification\")\n",
    "    print(\"  2. Dataset preparation\")\n",
    "    print(\"  3. Model download\")\n",
    "    print(\"  4. Vocabulary extension\")\n",
    "    print(\"  5. GPT fine-tuning\")\n",
    "    print(\"\\nâš ï¸ This may take several hours...\")\n",
    "    \n",
    "    !python kaggle_train_sinhala.py\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… TRAINING PIPELINE COMPLETED!\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:40:41.794157Z",
     "iopub.status.busy": "2025-11-06T09:40:41.793804Z",
     "iopub.status.idle": "2025-11-06T09:40:41.800048Z",
     "shell.execute_reply": "2025-11-06T09:40:41.799251Z",
     "shell.execute_reply.started": "2025-11-06T09:40:41.794122Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ALTERNATIVE: DIRECT GPT TRAINING\n",
      "================================================================================\n",
      "âš ï¸ Only use this if the pipeline in Cell 10 failed\n",
      "================================================================================\n",
      "â„¹ï¸ This cell is commented out. Uncomment to use if needed.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Alternative - Run GPT training directly (if pipeline fails)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment this cell only if kaggle_train_sinhala.py fails\n",
    "# This runs GPT training directly with all required parameters\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALTERNATIVE: DIRECT GPT TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âš ï¸ Only use this if the pipeline in Cell 10 failed\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Uncomment below to run directly:\n",
    "\"\"\"\n",
    "!CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n",
    "    --output_path /kaggle/working/checkpoints/ \\\n",
    "    --metadatas /kaggle/working/datasets/metadata_train.csv,/kaggle/working/datasets/metadata_eval.csv,si \\\n",
    "    --num_epochs 5 \\\n",
    "    --batch_size 8 \\\n",
    "    --grad_acumm 4 \\\n",
    "    --max_text_length 400 \\\n",
    "    --max_audio_length 330750 \\\n",
    "    --weight_decay 1e-2 \\\n",
    "    --lr 5e-6 \\\n",
    "    --save_step 50000\n",
    "\n",
    "print(\"\\nâœ… GPT training completed!\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"â„¹ï¸ This cell is commented out. Uncomment to use if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:40:41.801046Z",
     "iopub.status.busy": "2025-11-06T09:40:41.800861Z",
     "iopub.status.idle": "2025-11-06T09:40:41.821651Z",
     "shell.execute_reply": "2025-11-06T09:40:41.821053Z",
     "shell.execute_reply.started": "2025-11-06T09:40:41.801031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "VERIFYING TRAINING OUTPUT\n",
      "================================================================================\n",
      "\n",
      "âš ï¸ No training output found in /kaggle/working/checkpoints\n",
      "   Training may still be in progress or may have failed\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: Verify training output and find best model\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERIFYING TRAINING OUTPUT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Search for trained models\n",
    "checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
    "\n",
    "# Look for GPT_XTTS_FT directories\n",
    "model_dirs = glob.glob(f\"{checkpoint_dir}/GPT_XTTS_FT*\")\n",
    "\n",
    "if model_dirs:\n",
    "    print(f\"\\nâœ… Found {len(model_dirs)} training output(s):\")\n",
    "    for model_dir in model_dirs:\n",
    "        print(f\"\\nğŸ“ {model_dir}\")\n",
    "        \n",
    "        # Look for best_model.pth\n",
    "        best_model = os.path.join(model_dir, \"best_model.pth\")\n",
    "        if os.path.exists(best_model):\n",
    "            size_mb = os.path.getsize(best_model) / (1024 * 1024)\n",
    "            print(f\"   âœ… best_model.pth ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Look for config.json\n",
    "        config_file = os.path.join(model_dir, \"config.json\")\n",
    "        if os.path.exists(config_file):\n",
    "            print(f\"   âœ… config.json\")\n",
    "        \n",
    "        # List all files\n",
    "        files = os.listdir(model_dir)\n",
    "        print(f\"   ğŸ“„ Total files: {len(files)}\")\n",
    "        if len(files) <= 10:\n",
    "            for f in files:\n",
    "                print(f\"      - {f}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ No training output found in {checkpoint_dir}\")\n",
    "    print(\"   Training may still be in progress or may have failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:40:41.822607Z",
     "iopub.status.busy": "2025-11-06T09:40:41.822363Z",
     "iopub.status.idle": "2025-11-06T09:40:41.843439Z",
     "shell.execute_reply": "2025-11-06T09:40:41.842889Z",
     "shell.execute_reply.started": "2025-11-06T09:40:41.822584Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING INFERENCE\n",
      "================================================================================\n",
      "âŒ No trained model found. Please complete training first.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: Test inference with trained model\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING INFERENCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find the best model\n",
    "checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
    "model_dirs = glob.glob(f\"{checkpoint_dir}/GPT_XTTS_FT*\")\n",
    "\n",
    "if not model_dirs:\n",
    "    print(\"âŒ No trained model found. Please complete training first.\")\n",
    "else:\n",
    "    model_dir = model_dirs[0]  # Use the first one\n",
    "    best_model = os.path.join(model_dir, \"best_model.pth\")\n",
    "    config_file = os.path.join(model_dir, \"config.json\")\n",
    "    vocab_file = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\"\n",
    "    \n",
    "    if not os.path.exists(best_model):\n",
    "        print(f\"âŒ Best model not found: {best_model}\")\n",
    "    elif not os.path.exists(config_file):\n",
    "        print(f\"âŒ Config file not found: {config_file}\")\n",
    "    elif not os.path.exists(vocab_file):\n",
    "        print(f\"âŒ Vocab file not found: {vocab_file}\")\n",
    "    else:\n",
    "        print(f\"âœ… Found trained model: {model_dir}\")\n",
    "        print(f\"\\nğŸ“ Test Sinhala texts:\")\n",
    "        test_texts = [\n",
    "            \"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\",  # \"Always very important\"\n",
    "            \"à·à·Šâ€à¶»à·“ à¶½à¶‚à¶šà· à¶”à¶¶à·š à¶‹à¶­à·”à¶»à·”à¶¯à·™à·ƒà·’à¶±à·Š\",  # \"Sri Lanka from your north\"\n",
    "            \"à·ƒà·’à¶‚à·„à¶½ à¶·à·à·‚à·à·€ à¶…à¶´à¶œà·š à¶¢à·à¶­à·’à¶š à¶·à·à·‚à·à·€à¶ºà·’\",  # \"Sinhala is our national language\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(test_texts, 1):\n",
    "            print(f\"   {i}. {text}\")\n",
    "        \n",
    "        print(f\"\\nğŸ”¹ To test inference, use:\")\n",
    "        print(f\"   python inference_sinhala.py \\\\\")\n",
    "        print(f\"     --checkpoint_path {best_model} \\\\\")\n",
    "        print(f\"     --config_path {config_file} \\\\\")\n",
    "        print(f\"     --vocab_path {vocab_file} \\\\\")\n",
    "        print(f\"     --text \\\"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\\\" \\\\\")\n",
    "        print(f\"     --reference_audio <path_to_reference_audio.wav> \\\\\")\n",
    "        print(f\"     --output_path output.wav\")\n",
    "        \n",
    "        # Try to find a reference audio file\n",
    "        reference_audio = \"/kaggle/working/datasets/wavs\"\n",
    "        if os.path.exists(reference_audio):\n",
    "            audio_files = [f for f in os.listdir(reference_audio) if f.endswith('.wav')]\n",
    "            if audio_files:\n",
    "                ref_audio_path = os.path.join(reference_audio, audio_files[0])\n",
    "                print(f\"\\nâœ… Found reference audio: {ref_audio_path}\")\n",
    "                print(f\"\\nğŸ”¹ Running inference test...\")\n",
    "                \n",
    "                # Run inference\n",
    "                output_audio = \"/kaggle/working/test_output.wav\"\n",
    "                !python inference_sinhala.py \\\n",
    "                    --checkpoint_path {best_model} \\\n",
    "                    --config_path {config_file} \\\n",
    "                    --vocab_path {vocab_file} \\\n",
    "                    --text \"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\" \\\n",
    "                    --reference_audio {ref_audio_path} \\\n",
    "                    --output_path {output_audio}\n",
    "                \n",
    "                if os.path.exists(output_audio):\n",
    "                    size_mb = os.path.getsize(output_audio) / (1024 * 1024)\n",
    "                    print(f\"\\nâœ… Inference successful!\")\n",
    "                    print(f\"   Output: {output_audio} ({size_mb:.2f} MB)\")\n",
    "                else:\n",
    "                    print(f\"\\nâš ï¸ Inference may have failed - output file not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T09:40:41.844402Z",
     "iopub.status.busy": "2025-11-06T09:40:41.844171Z",
     "iopub.status.idle": "2025-11-06T09:40:41.862076Z",
     "shell.execute_reply": "2025-11-06T09:40:41.861272Z",
     "shell.execute_reply.started": "2025-11-06T09:40:41.844380Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING SUMMARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Training pipeline completed!\n",
      "\n",
      "ğŸ“ Output locations:\n",
      "   - Trained model: /kaggle/working/checkpoints/GPT_XTTS_FT-*/\n",
      "   - Base model files: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/\n",
      "   - Dataset: /kaggle/working/datasets/\n",
      "\n",
      "ğŸš€ To use the trained model:\n",
      "   1. Download the checkpoint directory from Kaggle\n",
      "   2. Use inference_sinhala.py to generate Sinhala speech\n",
      "   3. Provide any Sinhala text and a reference audio file\n",
      "\n",
      "ğŸ“ Example inference command:\n",
      "   python inference_sinhala.py \\\n",
      "     --checkpoint_path checkpoints/GPT_XTTS_FT-*/best_model.pth \\\n",
      "     --config_path checkpoints/GPT_XTTS_FT-*/config.json \\\n",
      "     --vocab_path checkpoints/XTTS_v2.0_original_model_files/vocab.json \\\n",
      "     --text \"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\" \\\n",
      "     --reference_audio reference.wav \\\n",
      "     --output_path output.wav\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL DONE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: Summary and next steps\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Training pipeline completed!\")\n",
    "print(\"\\nğŸ“ Output locations:\")\n",
    "print(\"   - Trained model: /kaggle/working/checkpoints/GPT_XTTS_FT-*/\")\n",
    "print(\"   - Base model files: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/\")\n",
    "print(\"   - Dataset: /kaggle/working/datasets/\")\n",
    "\n",
    "print(\"\\nğŸš€ To use the trained model:\")\n",
    "print(\"   1. Download the checkpoint directory from Kaggle\")\n",
    "print(\"   2. Use inference_sinhala.py to generate Sinhala speech\")\n",
    "print(\"   3. Provide any Sinhala text and a reference audio file\")\n",
    "\n",
    "print(\"\\nğŸ“ Example inference command:\")\n",
    "print(\"   python inference_sinhala.py \\\\\")\n",
    "print(\"     --checkpoint_path checkpoints/GPT_XTTS_FT-*/best_model.pth \\\\\")\n",
    "print(\"     --config_path checkpoints/GPT_XTTS_FT-*/config.json \\\\\")\n",
    "print(\"     --vocab_path checkpoints/XTTS_v2.0_original_model_files/vocab.json \\\\\")\n",
    "print(\"     --text \\\"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\\\" \\\\\")\n",
    "print(\"     --reference_audio reference.wav \\\\\")\n",
    "print(\"     --output_path output.wav\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ALL DONE!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8547335,
     "isSourceIdPinned": false,
     "sourceId": 13465111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
