{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XTTS-v2 Sinhala Fine-tuning on Kaggle\n",
    "\n",
    "This notebook fine-tunes XTTS-v2 for Sinhala language using the complete pipeline.\n",
    "\n",
    "**Steps:**\n",
    "1. Environment setup (PyTorch, TTS, dependencies)\n",
    "2. Clone repository\n",
    "3. Download dataset\n",
    "4. Download XTTS-v2 base model\n",
    "5. Prepare dataset\n",
    "6. Extend vocabulary for Sinhala\n",
    "7. Fine-tune GPT model\n",
    "8. Test inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:20:36.578795Z",
     "iopub.status.busy": "2025-11-06T07:20:36.578570Z",
     "iopub.status.idle": "2025-11-06T07:22:36.863904Z",
     "shell.execute_reply": "2025-11-06T07:22:36.863171Z",
     "shell.execute_reply.started": "2025-11-06T07:20:36.578772Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Collecting torch==2.1.0\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m415.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchaudio==2.1.0\n",
      "  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m72.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.15.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.9.0)\n",
      "Collecting triton==2.1.0 (from torch==2.1.0)\n",
      "  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\n",
      "Installing collected packages: triton, torch, torchaudio\n",
      "  Attempting uninstall: triton\n",
      "    Found existing installation: triton 3.2.0\n",
      "    Uninstalling triton-3.2.0:\n",
      "      Successfully uninstalled triton-3.2.0\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 2.6.0+cu124\n",
      "    Uninstalling torch-2.6.0+cu124:\n",
      "      Successfully uninstalled torch-2.6.0+cu124\n",
      "  Attempting uninstall: torchaudio\n",
      "    Found existing installation: torchaudio 2.6.0+cu124\n",
      "    Uninstalling torchaudio-2.6.0+cu124:\n",
      "      Successfully uninstalled torchaudio-2.6.0+cu124\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 triton-2.1.0\n",
      "CUDA available: True\n",
      "PyTorch version: 2.1.0+cu118\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "GPU Memory: 17.1 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 1: Install PyTorch with CUDA support\n",
    "# ============================================================================\n",
    "\n",
    "!pip install torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
    "\n",
    "# Verify\n",
    "import torch\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:22:48.953663Z",
     "iopub.status.busy": "2025-11-06T07:22:48.953101Z",
     "iopub.status.idle": "2025-11-06T07:22:48.959598Z",
     "shell.execute_reply": "2025-11-06T07:22:48.958732Z",
     "shell.execute_reply.started": "2025-11-06T07:22:48.953638Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Environment variables set\n",
      "TRANSFORMERS_NO_TORCHAO_IMPORT = 1\n",
      "TORCH_ALLOW_UNSAFE_DESERIALIZATION = 1\n",
      "PYTORCH_CUDA_ALLOC_CONF = max_split_size_mb:512\n",
      "\n",
      "Python version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
      "Working directory: /kaggle/working\n",
      "\n",
      "CUDA available: True\n",
      "GPU: Tesla P100-PCIE-16GB\n",
      "GPU Memory: 17.1 GB\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 2: Set environment variables and verify setup\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# âš ï¸ CRITICAL: Set these BEFORE any TTS imports\n",
    "os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT'] = '1'\n",
    "os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION'] = '1'\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\n",
    "print(\"âœ… Environment variables set\")\n",
    "print(f\"TRANSFORMERS_NO_TORCHAO_IMPORT = {os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT']}\")\n",
    "print(f\"TORCH_ALLOW_UNSAFE_DESERIALIZATION = {os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION']}\")\n",
    "print(f\"PYTORCH_CUDA_ALLOC_CONF = {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n",
    "\n",
    "# Check Python version\n",
    "print(f\"\\nPython version: {sys.version}\")\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "\n",
    "# Check GPU\n",
    "import torch\n",
    "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:22:53.773276Z",
     "iopub.status.busy": "2025-11-06T07:22:53.772999Z",
     "iopub.status.idle": "2025-11-06T07:24:51.285337Z",
     "shell.execute_reply": "2025-11-06T07:24:51.284259Z",
     "shell.execute_reply.started": "2025-11-06T07:22:53.773254Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m123.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "visions 0.8.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "google-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\n",
      "google-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
      "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
      "google-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\n",
      "xarray 2025.7.1 requires pandas>=2.2, but you have pandas 1.5.3 which is incompatible.\n",
      "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "nx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m106.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.0/260.0 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "category-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
      "scikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\n",
      "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.11.2 which is incompatible.\n",
      "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
      "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
      "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m32.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\n",
      "woodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "featuretools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "pyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\n",
      "preprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\n",
      "dopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\n",
      "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
      "gradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\n",
      "umap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\n",
      "dataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hâœ… All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 3: Install TTS and all dependencies\n",
    "# ============================================================================\n",
    "\n",
    "# Install TTS and related packages\n",
    "!pip install -q TTS==0.22.0\n",
    "\n",
    "# âš ï¸ CRITICAL FIX: Use transformers 4.36.0 instead of 4.45.2\n",
    "!pip install -q transformers==4.36.0 tokenizers==0.15.0\n",
    "\n",
    "!pip install -q librosa==0.10.2 soundfile==0.12.1 scipy==1.11.2 pysbd==0.3.4\n",
    "!pip install -q pandas==1.5.3 scikit-learn==1.3.2 tqdm==4.66.3\n",
    "!pip install -q einops==0.7.0 unidecode==1.3.8 inflect==7.0.0\n",
    "!pip install -q coqpit==0.0.16 trainer==0.0.36 mutagen\n",
    "!pip install -q pypinyin hangul_romanize num2words kagglehub\n",
    "!pip install -q requests\n",
    "\n",
    "print(\"âœ… All dependencies installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:25:44.915237Z",
     "iopub.status.busy": "2025-11-06T07:25:44.914554Z",
     "iopub.status.idle": "2025-11-06T07:25:47.087637Z",
     "shell.execute_reply": "2025-11-06T07:25:47.087025Z",
     "shell.execute_reply.started": "2025-11-06T07:25:44.915198Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainer version: v0.0.36\n",
      "TTS installed: 0.22.0\n",
      "transformers version: 4.36.0\n",
      "tokenizers version: 0.15.0\n",
      "librosa version: 0.10.2\n",
      "âœ… All packages verified!\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 4: Verify critical packages\n",
    "# ============================================================================\n",
    "\n",
    "import trainer\n",
    "import TTS\n",
    "import transformers\n",
    "import librosa\n",
    "import tokenizers\n",
    "\n",
    "print(f\"trainer version: {trainer.__version__}\")\n",
    "print(f\"TTS installed: {TTS.__version__}\")\n",
    "print(f\"transformers version: {transformers.__version__}\")\n",
    "print(f\"tokenizers version: {tokenizers.__version__}\")\n",
    "print(f\"librosa version: {librosa.__version__}\")\n",
    "print(\"âœ… All packages verified!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:25:48.763162Z",
     "iopub.status.busy": "2025-11-06T07:25:48.762522Z",
     "iopub.status.idle": "2025-11-06T07:25:50.461520Z",
     "shell.execute_reply": "2025-11-06T07:25:50.460562Z",
     "shell.execute_reply.started": "2025-11-06T07:25:48.763128Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Cloning XTTSv2-sinhala...\n",
      "Cloning into 'XTTSv2-sinhala'...\n",
      "remote: Enumerating objects: 508, done.\u001b[K\n",
      "remote: Counting objects: 100% (508/508), done.\u001b[K\n",
      "remote: Compressing objects: 100% (400/400), done.\u001b[K\n",
      "remote: Total 508 (delta 71), reused 498 (delta 64), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (508/508), 907.82 KiB | 2.10 MiB/s, done.\n",
      "Resolving deltas: 100% (71/71), done.\n",
      "âœ… Repository cloned\n",
      "âœ… Current directory: /kaggle/working/XTTSv2-sinhala\n",
      "\n",
      "ğŸ”¹ Repository contents:\n",
      "total 300\n",
      "drwxr-xr-x  5 root root  4096 Nov  6 07:25 .\n",
      "drwxr-xr-x  4 root root  4096 Nov  6 07:25 ..\n",
      "-rw-r--r--  1 root root 14831 Nov  6 07:25 config_sinhala.py\n",
      "-rw-r--r--  1 root root  2461 Nov  6 07:25 download_checkpoint.py\n",
      "-rw-r--r--  1 root root 16785 Nov  6 07:25 extend_vocab_sinhala.py\n",
      "drwxr-xr-x  8 root root  4096 Nov  6 07:25 .git\n",
      "-rw-r--r--  1 root root  4730 Nov  6 07:25 .gitignore\n",
      "-rw-r--r--  1 root root 20115 Nov  6 07:25 inference_sinhala.py\n",
      "-rw-r--r--  1 root root 47386 Nov  6 07:25 kagglebook.ipynb\n",
      "-rw-r--r--  1 root root 25832 Nov  6 07:25 kaggle-notebook.ipynb\n",
      "-rw-r--r--  1 root root 14610 Nov  6 07:25 kaggle_train_sinhala.py\n",
      "-rw-r--r--  1 root root 26347 Nov  6 07:25 november-6.ipynb\n",
      "-rw-r--r--  1 root root 13049 Nov  6 07:25 prepare_dataset_sinhala.py\n",
      "-rw-r--r--  1 root root    16 Nov  6 07:25 README.md\n",
      "drwxr-xr-x  9 root root  4096 Nov  6 07:25 recipes\n",
      "-rw-r--r--  1 root root  1039 Nov  6 07:25 requirements.txt\n",
      "-rw-r--r--  1 root root 29408 Nov  6 07:25 Sinhala_XTTS_Final_Corrected.ipynb\n",
      "-rw-r--r--  1 root root  7502 Nov  6 07:25 train_dvae_xtts.py\n",
      "-rw-r--r--  1 root root   235 Nov  6 07:25 train_dvae_xtts.sh\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 5: Clone repository\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "\n",
    "repo_url = \"https://github.com/amalshafernando/XTTSv2-sinhala.git\"\n",
    "repo_name = \"XTTSv2-sinhala\"\n",
    "\n",
    "# Clone only if it doesn't exist\n",
    "if not os.path.exists(repo_name):\n",
    "    print(f\"ğŸ”¹ Cloning {repo_name}...\")\n",
    "    !git clone {repo_url}\n",
    "    print(\"âœ… Repository cloned\")\n",
    "else:\n",
    "    print(f\"âœ… Repository already exists: {repo_name}\")\n",
    "\n",
    "# Change to repo directory\n",
    "os.chdir(repo_name)\n",
    "print(f\"âœ… Current directory: {os.getcwd()}\")\n",
    "\n",
    "# List contents\n",
    "print(\"\\nğŸ”¹ Repository contents:\")\n",
    "!ls -la | head -20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:25:57.946735Z",
     "iopub.status.busy": "2025-11-06T07:25:57.946461Z",
     "iopub.status.idle": "2025-11-06T07:25:58.531461Z",
     "shell.execute_reply": "2025-11-06T07:25:58.530723Z",
     "shell.execute_reply.started": "2025-11-06T07:25:57.946714Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Dataset downloaded to: /kaggle/input/sinhala-tts-dataset\n",
      "ğŸ“ Kaggle dataset path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n",
      "\n",
      "ğŸ“‚ Dataset contents:\n",
      "total 220K\n",
      "-rw-r--r-- 1 nobody nogroup  44K Nov  5 17:37 metadata_eval.csv\n",
      "-rw-r--r-- 1 nobody nogroup 175K Nov  5 17:37 metadata_train.csv\n",
      "drwxr-xr-x 2 nobody nogroup    0 Nov  5 17:37 wavs\n",
      "\n",
      "âœ… Found: metadata_train.csv\n",
      "âœ… Found: metadata_eval.csv\n",
      "âœ… Found audio directory: wavs\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 6: Download Sinhala TTS dataset\n",
    "# ============================================================================\n",
    "\n",
    "import kagglehub\n",
    "import os\n",
    "\n",
    "# Download dataset\n",
    "path = kagglehub.dataset_download(\"amalshaf/sinhala-tts-dataset\")\n",
    "print(f\"âœ… Dataset downloaded to: {path}\")\n",
    "\n",
    "# Setup paths\n",
    "kaggle_dataset_path = f\"{path}/sinhala-tts-dataset\"\n",
    "print(f\"ğŸ“ Kaggle dataset path: {kaggle_dataset_path}\")\n",
    "\n",
    "# Verify dataset structure\n",
    "if os.path.exists(kaggle_dataset_path):\n",
    "    print(f\"\\nğŸ“‚ Dataset contents:\")\n",
    "    !ls -lh {kaggle_dataset_path}\n",
    "    \n",
    "    # Check for metadata files\n",
    "    metadata_train = f\"{kaggle_dataset_path}/metadata_train.csv\"\n",
    "    metadata_eval = f\"{kaggle_dataset_path}/metadata_eval.csv\"\n",
    "    \n",
    "    if os.path.exists(metadata_train):\n",
    "        print(f\"\\nâœ… Found: metadata_train.csv\")\n",
    "    if os.path.exists(metadata_eval):\n",
    "        print(f\"âœ… Found: metadata_eval.csv\")\n",
    "    \n",
    "    # Check for audio directory\n",
    "    audio_dirs = [\"wav\", \"wavs\", \"audio\", \"audio_files\"]\n",
    "    for audio_dir in audio_dirs:\n",
    "        audio_path = os.path.join(kaggle_dataset_path, audio_dir)\n",
    "        if os.path.exists(audio_path):\n",
    "            print(f\"âœ… Found audio directory: {audio_dir}\")\n",
    "            break\n",
    "else:\n",
    "    print(f\"âŒ Dataset path not found: {kaggle_dataset_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:26:13.799279Z",
     "iopub.status.busy": "2025-11-06T07:26:13.798493Z",
     "iopub.status.idle": "2025-11-06T07:26:15.073383Z",
     "shell.execute_reply": "2025-11-06T07:26:15.072759Z",
     "shell.execute_reply.started": "2025-11-06T07:26:13.799248Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREPARING DATASET FOR XTTS-v2\n",
      "================================================================================\n",
      "\n",
      "============================================================\n",
      "CONVERTING DATASET TO XTTS-v2 FORMAT\n",
      "============================================================\n",
      "\n",
      "[2/3] Reading metadata files\n",
      "    Train: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_train.csv\n",
      "    Eval: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_eval.csv\n",
      "    Train samples: 1000\n",
      "    Eval samples: 251\n",
      "    Detected pipe-separated format in train CSV\n",
      "    Detected pipe-separated format in eval CSV\n",
      "\n",
      "[3/3] Converting train metadata\n",
      "    âš  Warning: Could not find audio directory. Expected: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wav\n",
      "    Will proceed with metadata conversion only.\n",
      "    âœ“ Saved 1000 samples to /kaggle/working/datasets/metadata_train.csv\n",
      "    Format: audio_file|text|speaker_name\n",
      "\n",
      "[3/3] Converting eval metadata\n",
      "    âš  Warning: Could not find audio directory. Expected: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/wav\n",
      "    Will proceed with metadata conversion only.\n",
      "    âœ“ Saved 251 samples to /kaggle/working/datasets/metadata_eval.csv\n",
      "    Format: audio_file|text|speaker_name\n",
      "\n",
      "[Verification] Checking output files\n",
      "    âœ“ metadata_train.csv: Format OK\n",
      "      Sample: wavs/sin_2282_sin_2282_8427486285.wav|à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Š...\n",
      "    âœ“ metadata_eval.csv: Format OK\n",
      "      Sample: wavs/sin_9228_sin_9228_8967768400.wav|à¶¸à·™à¶¸ à¶šà·’à¶ºà¶¸à¶±à·Š à¶­à·”à·… à¶‡à¶­à·’ à¶ºà¶®à·à¶»à·Šà¶®à¶º à·€à·’à¶¸à·ƒà· à¶¶à·à¶½à·”à·€ à·„à·œà¶­...\n",
      "\n",
      "============================================================\n",
      "DATASET PREPARATION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "Output directory: /kaggle/working/datasets\n",
      "  - metadata_train.csv (1000 samples)\n",
      "  - metadata_eval.csv (251 samples)\n",
      "  - wavs/ (audio files)\n",
      "\n",
      "================================================================================\n",
      "âœ… DATASET PREPARATION COMPLETED\n",
      "================================================================================\n",
      "\n",
      "âœ… Training samples: 1000\n",
      "âœ… Evaluation samples: 251\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 7: Prepare dataset using prepare_dataset_sinhala.py\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PREPARING DATASET FOR XTTS-v2\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get dataset path from previous cell\n",
    "kaggle_dataset_path = \"/kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\"\n",
    "output_dataset_path = \"/kaggle/working/datasets\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(output_dataset_path, exist_ok=True)\n",
    "\n",
    "# IMPORTANT: Copy audio files first if they exist\n",
    "source_wavs = os.path.join(kaggle_dataset_path, \"wavs\")\n",
    "target_wavs = os.path.join(output_dataset_path, \"wavs\")\n",
    "\n",
    "if os.path.exists(source_wavs):\n",
    "    print(f\"\\nğŸ”¹ Copying audio files from {source_wavs} to {target_wavs}...\")\n",
    "    os.makedirs(target_wavs, exist_ok=True)\n",
    "    \n",
    "    # Count files before copying\n",
    "    source_files = [f for f in os.listdir(source_wavs) if f.endswith('.wav')]\n",
    "    print(f\"   Found {len(source_files)} audio files in source\")\n",
    "    \n",
    "    # Copy files\n",
    "    copied = 0\n",
    "    for filename in source_files:\n",
    "        src = os.path.join(source_wavs, filename)\n",
    "        dst = os.path.join(target_wavs, filename)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copy2(src, dst)\n",
    "            copied += 1\n",
    "    \n",
    "    print(f\"   âœ… Copied {copied} new audio files\")\n",
    "    print(f\"   âœ… Total audio files in target: {len([f for f in os.listdir(target_wavs) if f.endswith('.wav')])}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Warning: Audio directory not found at {source_wavs}\")\n",
    "\n",
    "# Run dataset preparation script\n",
    "!python prepare_dataset_sinhala.py \\\n",
    "    --kaggle_path {kaggle_dataset_path} \\\n",
    "    --output_path {output_dataset_path}\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… DATASET PREPARATION COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify output files\n",
    "train_metadata = f\"{output_dataset_path}/metadata_train.csv\"\n",
    "eval_metadata = f\"{output_dataset_path}/metadata_eval.csv\"\n",
    "\n",
    "if os.path.exists(train_metadata):\n",
    "    import pandas as pd\n",
    "    df_train = pd.read_csv(train_metadata, sep='|', header=None)\n",
    "    print(f\"\\nâœ… Training samples: {len(df_train)}\")\n",
    "    \n",
    "if os.path.exists(eval_metadata):\n",
    "    df_eval = pd.read_csv(eval_metadata, sep='|', header=None)\n",
    "    print(f\"âœ… Evaluation samples: {len(df_eval)}\")\n",
    "\n",
    "# Verify audio files are accessible\n",
    "if os.path.exists(target_wavs):\n",
    "    audio_count = len([f for f in os.listdir(target_wavs) if f.endswith('.wav')])\n",
    "    print(f\"âœ… Audio files in working directory: {audio_count}\")\n",
    "    \n",
    "    # Check if metadata references can be found\n",
    "    if os.path.exists(train_metadata):\n",
    "        sample_row = df_train.iloc[0]\n",
    "        sample_audio = sample_row[0]  # First column is audio_file\n",
    "        sample_audio_path = os.path.join(output_dataset_path, sample_audio)\n",
    "        if os.path.exists(sample_audio_path):\n",
    "            print(f\"âœ… Sample audio file accessible: {sample_audio}\")\n",
    "        else:\n",
    "            print(f\"âš ï¸ Warning: Sample audio file not found: {sample_audio_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:26:47.821984Z",
     "iopub.status.busy": "2025-11-06T07:26:47.821331Z",
     "iopub.status.idle": "2025-11-06T07:26:57.060524Z",
     "shell.execute_reply": "2025-11-06T07:26:57.059750Z",
     "shell.execute_reply.started": "2025-11-06T07:26:47.821957Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DOWNLOADING XTTS-v2 MODEL FILES\n",
      "================================================================================\n",
      "\n",
      "ğŸ”¹ Downloading config.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 4.37kB [00:00, 8.87MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… config.json downloaded successfully\n",
      "\n",
      "ğŸ”¹ Downloading vocab.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "vocab.json: 361kB [00:00, 146MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… vocab.json downloaded successfully\n",
      "\n",
      "ğŸ”¹ Downloading model.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87G/1.87G [00:06<00:00, 282MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… model.pth downloaded successfully\n",
      "\n",
      "ğŸ”¹ Downloading dvae.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dvae.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211M/211M [00:00<00:00, 281MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… dvae.pth downloaded successfully\n",
      "\n",
      "ğŸ”¹ Downloading mel_stats.pth...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "mel_stats.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.07k/1.07k [00:00<00:00, 7.60MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… mel_stats.pth downloaded successfully\n",
      "\n",
      "================================================================================\n",
      "VERIFICATION\n",
      "================================================================================\n",
      "âœ… config.json: 0.0 MB\n",
      "âœ… vocab.json: 0.3 MB\n",
      "âœ… model.pth: 1781.4 MB\n",
      "âœ… dvae.pth: 200.8 MB\n",
      "âœ… mel_stats.pth: 0.0 MB\n",
      "\n",
      "================================================================================\n",
      "âœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 8: Download XTTS-v2 base model files\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DOWNLOADING XTTS-v2 MODEL FILES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Define all required files from Hugging Face\n",
    "base_url = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/\"\n",
    "\n",
    "files_to_download = {\n",
    "    \"config.json\": f\"{base_url}config.json\",\n",
    "    \"vocab.json\": f\"{base_url}vocab.json\",\n",
    "    \"model.pth\": f\"{base_url}model.pth\",\n",
    "    \"dvae.pth\": f\"{base_url}dvae.pth\",\n",
    "    \"mel_stats.pth\": f\"{base_url}mel_stats.pth\",\n",
    "}\n",
    "\n",
    "def download_file(url, output_path):\n",
    "    '''Download file with progress bar'''\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(output_path, 'wb') as f:\n",
    "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=os.path.basename(output_path)) as pbar:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "                    pbar.update(len(chunk))\n",
    "\n",
    "# Download each file\n",
    "for filename, url in files_to_download.items():\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    \n",
    "    if os.path.exists(output_path):\n",
    "        size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "        print(f\"âœ… {filename} already exists ({size_mb:.1f} MB), skipping...\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ”¹ Downloading {filename}...\")\n",
    "        try:\n",
    "            download_file(url, output_path)\n",
    "            print(f\"âœ… {filename} downloaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed to download {filename}: {e}\")\n",
    "\n",
    "# Verify all files downloaded\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"VERIFICATION\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "all_downloaded = True\n",
    "for filename in files_to_download.keys():\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    if os.path.exists(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"âœ… {filename}: {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"âŒ {filename}: MISSING!\")\n",
    "        all_downloaded = False\n",
    "\n",
    "if all_downloaded:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"âœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\")\n",
    "    print(f\"{'=' * 80}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-06T07:27:07.936731Z",
     "iopub.status.busy": "2025-11-06T07:27:07.936153Z",
     "iopub.status.idle": "2025-11-06T07:27:08.679450Z",
     "shell.execute_reply": "2025-11-06T07:27:08.678711Z",
     "shell.execute_reply.started": "2025-11-06T07:27:07.936711Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTENDING VOCABULARY FOR SINHALA\n",
      "================================================================================\n",
      "âœ… Metadata file found: /kaggle/working/datasets/metadata_train.csv\n",
      "âœ… Output path exists: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n",
      "\n",
      "ğŸ”¹ Running extend_vocab_sinhala.py...\n",
      "\n",
      "================================================================================\n",
      "SINHALA VOCABULARY EXTENSION FOR XTTS-v2\n",
      "================================================================================\n",
      "\n",
      "ğŸ“‹ Configuration:\n",
      "   - Language: si (Sinhala)\n",
      "   - Vocabulary size: 15,000 tokens\n",
      "   - Metadata: /kaggle/working/datasets/metadata_train.csv\n",
      "   - Output: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 1: LOADING SINHALA TEXTS\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ“– Loading Sinhala texts from: /kaggle/working/datasets/metadata_train.csv\n",
      "âœ… CSV loaded successfully\n",
      "âœ… Loaded 1000 Sinhala text samples\n",
      "\n",
      "ğŸ“ Sample Sinhala texts:\n",
      "  1. à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.\n",
      "  2. à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.\n",
      "  3. à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š à¶»à¶§à¶šà·’.\n",
      "\n",
      "ğŸ“Š Text Statistics:\n",
      "   - Total characters: 48,540\n",
      "   - Sinhala characters: 40,716 (83.9%)\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 2: TRAINING BPE TOKENIZER\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "================================================================================\n",
      "TRAINING SINHALA BPE TOKENIZER\n",
      "Target Vocabulary Size: 15000\n",
      "================================================================================\n",
      "\n",
      "âœ… Tokenizer initialized with ByteLevel pre-tokenizer\n",
      "âœ… BpeTrainer configured\n",
      "   - Min frequency: 2\n",
      "   - Special tokens: 5\n",
      "\n",
      "ğŸ”§ Training BPE tokenizer on 1000 Sinhala texts...\n",
      "\u001b[2K[00:00:00] Tokenize words                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1508     /     1508[00:00:00] Tokenize words                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0        /        0\n",
      "\u001b[2K[00:00:00] Count pairs                    â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 1508     /     1508\n",
      "\u001b[2K[00:00:00] Compute merges                 â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 860      /      860\n",
      "\n",
      "âœ… Training completed!\n",
      "   - Actual vocabulary size: 921 tokens\n",
      "\n",
      "âœ… Testing tokenization on samples:\n",
      "   Text: à¶´à·œà·…à·œà¶±à·Šà¶±à¶»à·”à·€à·š à¶œà¶½à·Š à·€à·’à·„à·à¶»à¶ºà·š à·„à·’à¶³à·’ à¶´à·’à·…à·’à¶¸à¶º à¶¸à·™à·€à·à¶±à·Šà¶±à¶šà·’.     â†’  35 tokens [âœ… No UNK]\n",
      "   Text: à¶»à¶¢à¶­à·”à¶¸à· à¶’à¶šà·š à¶±à·’à¶»à·Šà¶¸à·à¶« à·€à¶§à·’à¶±à·à¶šà¶¸ à¶¯à·à¶šà·Šà¶š à¶‘à¶šà¶­à·Š à¶±à·’à¶ºà¶¸à¶ºà·’.      â†’  29 tokens [âœ… No UNK]\n",
      "   Text: à·à·à¶±à·Šà¶­ à¶½à·”à·ƒà·’à¶ºà· à¶±à·à¶œà·™à¶±à·„à·’à¶» à¶šà·à¶»à¶¶à·’à¶ºà¶±à·Š à¶¸à·”à·„à·”à¶¯à·š à¶´à·’à·„à·’à¶§à·’ à¶¯à·–à¶´à¶­à·Š â†’  42 tokens [âœ… No UNK]\n",
      "\n",
      "   âœ… EXCELLENT: No UNK tokens in test samples!\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 3: SAVING VOCABULARY\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "================================================================================\n",
      "SAVING VOCABULARY\n",
      "================================================================================\n",
      "\n",
      "âœ… Output directory created: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files\n",
      "âœ… Vocabulary saved: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\n",
      "   - File size: 22.6 KB\n",
      "   - Tokens: 921\n",
      "âœ… Tokenizer saved: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/tokenizer.json\n",
      "   - File size: 49.7 KB\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "STEP 4: UPDATING CONFIG\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "ğŸ” Searching for config.json...\n",
      "âœ… Found: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n",
      "\n",
      "================================================================================\n",
      "UPDATING CONFIG.JSON FOR SINHALA\n",
      "================================================================================\n",
      "\n",
      "ğŸ“– Loading config from: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/config.json\n",
      "âœ… Config loaded successfully\n",
      "\n",
      "âœ… Added language_ids:\n",
      "   - Language: si\n",
      "   - ID: 21\n",
      "\n",
      "âŒ ERROR: list indices must be integers or slices, not str\n",
      "\n",
      "================================================================================\n",
      "\n",
      "âœ… Vocabulary extension completed!\n",
      "\n",
      "âœ… Extended vocabulary size: 921 tokens\n",
      "âœ… Sinhala-specific tokens: 0\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CELL 9: Extend vocabulary for Sinhala language\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"EXTENDING VOCABULARY FOR SINHALA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Paths\n",
    "metadata_path = \"/kaggle/working/datasets/metadata_train.csv\"\n",
    "output_path = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
    "\n",
    "# Verify paths exist\n",
    "if not os.path.exists(metadata_path):\n",
    "    print(f\"âŒ Error: Metadata file not found: {metadata_path}\")\n",
    "else:\n",
    "    print(f\"âœ… Metadata file found: {metadata_path}\")\n",
    "\n",
    "if not os.path.exists(output_path):\n",
    "    print(f\"âŒ Error: Output path not found: {output_path}\")\n",
    "else:\n",
    "    print(f\"âœ… Output path exists: {output_path}\")\n",
    "\n",
    "# Run extend_vocab_sinhala.py\n",
    "print(f\"\\nğŸ”¹ Running extend_vocab_sinhala.py...\")\n",
    "!python extend_vocab_sinhala.py \\\n",
    "    --metadata_path {metadata_path} \\\n",
    "    --output_path {output_path} \\\n",
    "    --language si \\\n",
    "    --vocab_size 15000\n",
    "\n",
    "print(\"\\nâœ… Vocabulary extension completed!\")\n",
    "\n",
    "# Verify the extended vocab\n",
    "vocab_path = os.path.join(output_path, \"vocab.json\")\n",
    "if os.path.exists(vocab_path):\n",
    "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
    "        vocab = json.load(f)\n",
    "    print(f\"\\nâœ… Extended vocabulary size: {len(vocab):,} tokens\")\n",
    "    \n",
    "    # Check for Sinhala characters in vocab\n",
    "    # Note: ByteLevel BPE works at byte level, so explicit Sinhala tokens may not appear\n",
    "    # This is OK - what matters is that there are no UNK tokens\n",
    "    sinhala_tokens = [token for token in vocab.keys() if any('\\u0D80' <= char <= '\\u0DFF' for char in token)]\n",
    "    print(f\"â„¹ï¸ Sinhala-specific tokens: {len(sinhala_tokens)}\")\n",
    "    print(f\"   (Note: ByteLevel BPE works at byte level, so explicit Sinhala tokens may not appear)\")\n",
    "    print(f\"   âœ… What matters: No UNK tokens = tokenizer can handle Sinhala text correctly\")\n",
    "    \n",
    "    # Verify config.json was updated\n",
    "    config_path = os.path.join(output_path, \"config.json\")\n",
    "    if os.path.exists(config_path):\n",
    "        with open(config_path, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        if 'language_ids' in config and 'si' in config['language_ids']:\n",
    "            print(f\"âœ… Sinhala language (si) added to config.json\")\n",
    "            print(f\"   Language ID: {config['language_ids']['si']}\")\n",
    "else:\n",
    "    print(f\"\\nâŒ Vocabulary file not found at: {vocab_path}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“ Understanding Vocabulary Results\n",
    "\n",
    "**About \"Sinhala-specific tokens: 0\":**\n",
    "- âœ… **This is OK!** ByteLevel BPE tokenizer works at the **byte level**, not character level\n",
    "- It doesn't need explicit Sinhala character tokens because it encodes Unicode bytes\n",
    "- The important thing is: **No UNK tokens** = tokenizer can handle all Sinhala text\n",
    "\n",
    "**About vocabulary size (921 vs 15000):**\n",
    "- The actual vocabulary size depends on the dataset size and diversity\n",
    "- With 1000 samples, 921 tokens is reasonable\n",
    "- The tokenizer will still work correctly as long as there are no UNK tokens\n",
    "- For larger datasets, you'll get closer to the target size\n",
    "\n",
    "**What matters:**\n",
    "- âœ… No UNK tokens in test samples = tokenizer is working correctly\n",
    "- âœ… All Sinhala text can be tokenized properly\n",
    "- âœ… Ready for training!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-06T07:20:09.232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 10: Run complete training pipeline using kaggle_train_sinhala.py\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"STARTING COMPLETE TRAINING PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Verify we're in the repo directory\n",
    "if not os.path.exists(\"kaggle_train_sinhala.py\"):\n",
    "    print(\"âŒ Error: kaggle_train_sinhala.py not found in current directory\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")\n",
    "    print(\"\\nTrying to find it...\")\n",
    "    !find . -name \"kaggle_train_sinhala.py\" -type f\n",
    "else:\n",
    "    print(\"âœ… Found kaggle_train_sinhala.py\")\n",
    "    \n",
    "    # Run the complete training pipeline\n",
    "    print(\"\\nğŸš€ Starting training pipeline...\")\n",
    "    print(\"This will run all phases:\")\n",
    "    print(\"  1. Setup verification\")\n",
    "    print(\"  2. Dataset preparation\")\n",
    "    print(\"  3. Model download\")\n",
    "    print(\"  4. Vocabulary extension\")\n",
    "    print(\"  5. GPT fine-tuning\")\n",
    "    print(\"\\nâš ï¸ This may take several hours...\")\n",
    "    \n",
    "    !python kaggle_train_sinhala.py\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âœ… TRAINING PIPELINE COMPLETED!\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-06T07:20:09.233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 11: Alternative - Run GPT training directly (if pipeline fails)\n",
    "# ============================================================================\n",
    "\n",
    "# Uncomment this cell only if kaggle_train_sinhala.py fails\n",
    "# This runs GPT training directly with all required parameters\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ALTERNATIVE: DIRECT GPT TRAINING\")\n",
    "print(\"=\" * 80)\n",
    "print(\"âš ï¸ Only use this if the pipeline in Cell 10 failed\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Uncomment below to run directly:\n",
    "\"\"\"\n",
    "!CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n",
    "    --output_path /kaggle/working/checkpoints/ \\\n",
    "    --metadatas /kaggle/working/datasets/metadata_train.csv,/kaggle/working/datasets/metadata_eval.csv,si \\\n",
    "    --num_epochs 5 \\\n",
    "    --batch_size 8 \\\n",
    "    --grad_acumm 4 \\\n",
    "    --max_text_length 400 \\\n",
    "    --max_audio_length 330750 \\\n",
    "    --weight_decay 1e-2 \\\n",
    "    --lr 5e-6 \\\n",
    "    --save_step 50000\n",
    "\n",
    "print(\"\\nâœ… GPT training completed!\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"â„¹ï¸ This cell is commented out. Uncomment to use if needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-06T07:20:09.233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 12: Verify training output and find best model\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"VERIFYING TRAINING OUTPUT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Search for trained models\n",
    "checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
    "\n",
    "# Look for GPT_XTTS_FT directories\n",
    "model_dirs = glob.glob(f\"{checkpoint_dir}/GPT_XTTS_FT*\")\n",
    "\n",
    "if model_dirs:\n",
    "    print(f\"\\nâœ… Found {len(model_dirs)} training output(s):\")\n",
    "    for model_dir in model_dirs:\n",
    "        print(f\"\\nğŸ“ {model_dir}\")\n",
    "        \n",
    "        # Look for best_model.pth\n",
    "        best_model = os.path.join(model_dir, \"best_model.pth\")\n",
    "        if os.path.exists(best_model):\n",
    "            size_mb = os.path.getsize(best_model) / (1024 * 1024)\n",
    "            print(f\"   âœ… best_model.pth ({size_mb:.1f} MB)\")\n",
    "        \n",
    "        # Look for config.json\n",
    "        config_file = os.path.join(model_dir, \"config.json\")\n",
    "        if os.path.exists(config_file):\n",
    "            print(f\"   âœ… config.json\")\n",
    "        \n",
    "        # List all files\n",
    "        files = os.listdir(model_dir)\n",
    "        print(f\"   ğŸ“„ Total files: {len(files)}\")\n",
    "        if len(files) <= 10:\n",
    "            for f in files:\n",
    "                print(f\"      - {f}\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ No training output found in {checkpoint_dir}\")\n",
    "    print(\"   Training may still be in progress or may have failed\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-06T07:20:09.233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 13: Test inference with trained model\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING INFERENCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Find the best model\n",
    "checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
    "model_dirs = glob.glob(f\"{checkpoint_dir}/GPT_XTTS_FT*\")\n",
    "\n",
    "if not model_dirs:\n",
    "    print(\"âŒ No trained model found. Please complete training first.\")\n",
    "else:\n",
    "    model_dir = model_dirs[0]  # Use the first one\n",
    "    best_model = os.path.join(model_dir, \"best_model.pth\")\n",
    "    config_file = os.path.join(model_dir, \"config.json\")\n",
    "    vocab_file = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\"\n",
    "    \n",
    "    if not os.path.exists(best_model):\n",
    "        print(f\"âŒ Best model not found: {best_model}\")\n",
    "    elif not os.path.exists(config_file):\n",
    "        print(f\"âŒ Config file not found: {config_file}\")\n",
    "    elif not os.path.exists(vocab_file):\n",
    "        print(f\"âŒ Vocab file not found: {vocab_file}\")\n",
    "    else:\n",
    "        print(f\"âœ… Found trained model: {model_dir}\")\n",
    "        print(f\"\\nğŸ“ Test Sinhala texts:\")\n",
    "        test_texts = [\n",
    "            \"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\",  # \"Always very important\"\n",
    "            \"à·à·Šâ€à¶»à·“ à¶½à¶‚à¶šà· à¶”à¶¶à·š à¶‹à¶­à·”à¶»à·”à¶¯à·™à·ƒà·’à¶±à·Š\",  # \"Sri Lanka from your north\"\n",
    "            \"à·ƒà·’à¶‚à·„à¶½ à¶·à·à·‚à·à·€ à¶…à¶´à¶œà·š à¶¢à·à¶­à·’à¶š à¶·à·à·‚à·à·€à¶ºà·’\",  # \"Sinhala is our national language\"\n",
    "        ]\n",
    "        \n",
    "        for i, text in enumerate(test_texts, 1):\n",
    "            print(f\"   {i}. {text}\")\n",
    "        \n",
    "        print(f\"\\nğŸ”¹ To test inference, use:\")\n",
    "        print(f\"   python inference_sinhala.py \\\\\")\n",
    "        print(f\"     --checkpoint_path {best_model} \\\\\")\n",
    "        print(f\"     --config_path {config_file} \\\\\")\n",
    "        print(f\"     --vocab_path {vocab_file} \\\\\")\n",
    "        print(f\"     --text \\\"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\\\" \\\\\")\n",
    "        print(f\"     --reference_audio <path_to_reference_audio.wav> \\\\\")\n",
    "        print(f\"     --output_path output.wav\")\n",
    "        \n",
    "        # Try to find a reference audio file\n",
    "        reference_audio = \"/kaggle/working/datasets/wavs\"\n",
    "        if os.path.exists(reference_audio):\n",
    "            audio_files = [f for f in os.listdir(reference_audio) if f.endswith('.wav')]\n",
    "            if audio_files:\n",
    "                ref_audio_path = os.path.join(reference_audio, audio_files[0])\n",
    "                print(f\"\\nâœ… Found reference audio: {ref_audio_path}\")\n",
    "                print(f\"\\nğŸ”¹ Running inference test...\")\n",
    "                \n",
    "                # Run inference\n",
    "                output_audio = \"/kaggle/working/test_output.wav\"\n",
    "                !python inference_sinhala.py \\\n",
    "                    --checkpoint_path {best_model} \\\n",
    "                    --config_path {config_file} \\\n",
    "                    --vocab_path {vocab_file} \\\n",
    "                    --text \"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\" \\\n",
    "                    --reference_audio {ref_audio_path} \\\n",
    "                    --output_path {output_audio}\n",
    "                \n",
    "                if os.path.exists(output_audio):\n",
    "                    size_mb = os.path.getsize(output_audio) / (1024 * 1024)\n",
    "                    print(f\"\\nâœ… Inference successful!\")\n",
    "                    print(f\"   Output: {output_audio} ({size_mb:.2f} MB)\")\n",
    "                else:\n",
    "                    print(f\"\\nâš ï¸ Inference may have failed - output file not found\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-11-06T07:20:09.233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CELL 14: Summary and next steps\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ… Training pipeline completed!\")\n",
    "print(\"\\nğŸ“ Output locations:\")\n",
    "print(\"   - Trained model: /kaggle/working/checkpoints/GPT_XTTS_FT-*/\")\n",
    "print(\"   - Base model files: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/\")\n",
    "print(\"   - Dataset: /kaggle/working/datasets/\")\n",
    "\n",
    "print(\"\\nğŸš€ To use the trained model:\")\n",
    "print(\"   1. Download the checkpoint directory from Kaggle\")\n",
    "print(\"   2. Use inference_sinhala.py to generate Sinhala speech\")\n",
    "print(\"   3. Provide any Sinhala text and a reference audio file\")\n",
    "\n",
    "print(\"\\nğŸ“ Example inference command:\")\n",
    "print(\"   python inference_sinhala.py \\\\\")\n",
    "print(\"     --checkpoint_path checkpoints/GPT_XTTS_FT-*/best_model.pth \\\\\")\n",
    "print(\"     --config_path checkpoints/GPT_XTTS_FT-*/config.json \\\\\")\n",
    "print(\"     --vocab_path checkpoints/XTTS_v2.0_original_model_files/vocab.json \\\\\")\n",
    "print(\"     --text \\\"à¶±à·’à¶»à¶±à·Šà¶­à¶»à¶ºà·’ à¶‰à¶­à· à·€à·à¶¯à¶œà¶­à·Š\\\" \\\\\")\n",
    "print(\"     --reference_audio reference.wav \\\\\")\n",
    "print(\"     --output_path output.wav\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ALL DONE!\")\n",
    "print(\"=\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8547335,
     "isSourceIdPinned": false,
     "sourceId": 13465111,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
