{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sinhala Text-to-Speech (XTTS-v2) - Complete Fine-tuning Pipeline\n",
    "## üá±üá∞ Sinhala Speech Synthesis | ByteLevel BPE Tokenization | Grapheme-based Processing\n",
    "\n",
    "**Goal:** Convert Sinhala text to natural Sinhala speech\n",
    "\n",
    "**Dataset:** Sinhala TTS Dataset from Kaggle\n",
    "\n",
    "**Timeline:** ~5-9 hours on GPU\n",
    "\n",
    "**Key Features:**\n",
    "- ‚úÖ ByteLevel BPE tokenization (perfect for Sinhala Unicode)\n",
    "- ‚úÖ 15,000 Sinhala-optimized tokens\n",
    "- ‚úÖ Grapheme-based text processing (no phoneme conversion)\n",
    "- ‚úÖ Voice cloning support\n",
    "- ‚úÖ Comprehensive error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Environment Setup & Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n    "# SETUP: Install PyTorch and Dependencies\n    "# ============================================================================\n    "\n    "print(\"üì¶ Installing PyTorch with CUDA support...\")\n    "!pip install -q torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n    "\n    "print(\"üì¶ Installing TTS and dependencies...\")\n    "!pip install -q TTS>=0.22.0 transformers>=4.30.0 tokenizers>=0.13.0\n    "\n    "print(\"üì¶ Installing additional libraries...\")\n    "!pip install -q pandas numpy tqdm pyyaml regex\n    "\n    "# Verify installation\n    "import torch\n    "import torchaudio\n    "from TTS.utils.manage import ModelManager\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"‚úÖ ENVIRONMENT SETUP COMPLETE\")\n    "print(\"=\"*80)\n    "print(f\"PyTorch version: {torch.__version__}\")\n    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n    "if torch.cuda.is_available():\n    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Repository with Sinhala Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n    "import subprocess\n    "\n    "# Clone the repository with sinhala-tokenization branch\n    "repo_url = \"https://github.com/amalshafernando/XTTSv2-sinhala.git\"\n    "branch = \"sinhala-tokenization\"\n    "repo_path = \"/kaggle/working/XTTSv2-sinhala\"\n    "\n    "print(f\"üì• Cloning repository from {repo_url}\")\n    "if not os.path.exists(repo_path):\n    "    subprocess.run([\n    "        \"git\", \"clone\", \n    "        \"-b\", branch,\n    "        repo_url, \n    "        repo_path\n    "    ], check=True)\n    "    print(f\"‚úÖ Repository cloned to {repo_path}\")\n    "else:\n    "    print(f\"‚úÖ Repository already exists at {repo_path}\")\n    "\n    "# List the files\n    "print(f\"\\nüìÅ Files in repository:\")\n    "for root, dirs, files in os.walk(repo_path):\n    "    level = root.replace(repo_path, '').count(os.sep)\n    "    indent = ' ' * 2 * level\n    "    if level < 2:  # Only show first 2 levels\n    "        print(f\"{indent}{os.path.basename(root)}/\")\n    "        subindent = ' ' * 2 * (level + 1)\n    "        for file in files[:5]:  # Show first 5 files\n    "            print(f\"{subindent}{file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Setup Kaggle Paths & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n    "import sys\n    "from pathlib import Path\n    "\n    "# ============================================================================\n    "# CONFIGURATION: Kaggle Paths\n    "# ============================================================================\n    "\n    "# Input dataset\n    "DATASET_PATH = \"/kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\"\n    "\n    "# Working directories\n    "WORKING_DIR = \"/kaggle/working\"\n    "DATASET_OUTPUT = os.path.join(WORKING_DIR, \"datasets\")\n    "CHECKPOINTS_DIR = os.path.join(WORKING_DIR, \"checkpoints\")\n    "XTTS_MODEL_DIR = os.path.join(CHECKPOINTS_DIR, \"XTTS_v2.0_original_model_files\")\n    "OUTPUT_DIR = os.path.join(WORKING_DIR, \"output\")\n    "\n    "# Training configuration\n    "LANGUAGE_CODE = \"si\"\n    "VOCAB_SIZE = 15000\n    "NUM_EPOCHS = 5\n    "BATCH_SIZE = 8\n    "GRAD_ACCUM = 4\n    "LEARNING_RATE = 5e-6\n    "SAVE_STEP = 50000\n    "\n    "# Create directories\n    "for dir_path in [DATASET_OUTPUT, CHECKPOINTS_DIR, XTTS_MODEL_DIR, OUTPUT_DIR]:\n    "    os.makedirs(dir_path, exist_ok=True)\n    "    print(f\"‚úÖ {dir_path}\")\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"‚úÖ CONFIGURATION COMPLETE\")\n    "print(\"=\"*80)\n    "print(f\"Dataset: {DATASET_PATH}\")\n    "print(f\"Working: {WORKING_DIR}\")\n    "print(f\"Checkpoints: {CHECKPOINTS_DIR}\")\n    "print(f\"Language: {LANGUAGE_CODE} (Sinhala)\")\n    "print(f\"Vocab size: {VOCAB_SIZE:,} tokens\")\n    "print(f\"Training epochs: {NUM_EPOCHS}\")\n    "print(f\"Batch size: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: PHASE 1 - Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n    "import pandas as pd\n    "import shutil\n    "from pathlib import Path\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"PHASE 1: DATASET PREPARATION\")\n    "print(\"=\"*80)\n    "\n    "# Check dataset\n    "print(f\"\\n[1/4] Validating dataset at {DATASET_PATH}\")\n    "if not os.path.exists(DATASET_PATH):\n    "    raise FileNotFoundError(f\"‚ùå Dataset not found: {DATASET_PATH}\")\n    "\n    "print(f\"‚úÖ Dataset found\")\n    "\n    "# List files in dataset\n    "print(f\"\\n[2/4] Dataset contents:\")\n    "dataset_files = os.listdir(DATASET_PATH)\n    "for item in dataset_files:\n    "    item_path = os.path.join(DATASET_PATH, item)\n    "    if os.path.isdir(item_path):\n    "        file_count = len(os.listdir(item_path))\n    "        print(f\"   üìÅ {item}/ ({file_count} files)\")\n    "    else:\n    "        print(f\"   üìÑ {item}\")\n    "\n    "# Find metadata file\n    "print(f\"\\n[3/4] Looking for metadata...\")\n    "metadata_file = None\n    "for fname in ['metadata.csv', 'metadata.txt', 'metadata_train.csv']:\n    "    fpath = os.path.join(DATASET_PATH, fname)\n    "    if os.path.exists(fpath):\n    "        metadata_file = fpath\n    "        break\n    "\n    "if metadata_file:\n    "    print(f\"‚úÖ Found metadata: {os.path.basename(metadata_file)}\")\n    "    df = pd.read_csv(metadata_file, sep='|', header=None)\n    "    print(f\"   Rows: {len(df)}\")\n    "    print(f\"   Columns: {df.shape[1]}\")\n    "    print(f\"   Sample: {df.iloc[0, 1][:50] if df.shape[1] > 1 else 'N/A'}...\")\n    "else:\n    "    print(f\"‚ö†Ô∏è  No metadata file found\")\n    "    print(f\"   Looking for audio files...\")\n    "    audio_files = []\n    "    for root, dirs, files in os.walk(DATASET_PATH):\n    "        for file in files:\n    "            if file.endswith(('.wav', '.mp3', '.m4a')):\n    "                audio_files.append(os.path.join(root, file))\n    "    print(f\"   Found {len(audio_files)} audio files\")\n    "\n    "print(f\"\\n[4/4] Dataset preparation status: ‚úÖ READY\")\n    "print(f\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: PHASE 2 - Download XTTS-v2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.utils.manage import ModelManager\n    "import os\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"PHASE 2: DOWNLOAD XTTS-v2 PRE-TRAINED MODEL\")\n    "print(\"=\"*80)\n    "\n    "# Model files to download\n    "DVAE_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth\"\n    "MEL_NORM_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth\"\n    "TOKENIZER_FILE_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json\"\n    "XTTS_CHECKPOINT_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth\"\n    "XTTS_CONFIG_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/config.json\"\n    "SPEAKERS_LINK = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/speakers_xtts.pth\"\n    "\n    "files_to_download = [\n    "    (MEL_NORM_LINK, \"mel_stats.pth\"),\n    "    (DVAE_CHECKPOINT_LINK, \"dvae.pth\"),\n    "    (TOKENIZER_FILE_LINK, \"vocab.json\"),\n    "    (XTTS_CHECKPOINT_LINK, \"model.pth\"),\n    "    (XTTS_CONFIG_LINK, \"config.json\"),\n    "    (SPEAKERS_LINK, \"speakers_xtts.pth\"),\n    "]\n    "\n    "print(f\"\\n[1/{len(files_to_download)}] Downloading model files...\")\n    "\n    "for idx, (url, filename) in enumerate(files_to_download, 1):\n    "    filepath = os.path.join(XTTS_MODEL_DIR, filename)\n    "    \n    "    if os.path.exists(filepath):\n    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n    "        print(f\"   [{idx}] ‚úÖ {filename} ({size_mb:.1f} MB) - already exists\")\n    "    else:\n    "        print(f\"   [{idx}] üì• Downloading {filename}...\")\n    "        try:\n    "            ModelManager._download_model_files(\n    "                [url],\n    "                XTTS_MODEL_DIR,\n    "                progress_bar=True\n    "            )\n    "            size_mb = os.path.getsize(filepath) / (1024 * 1024)\n    "            print(f\"       ‚úÖ Downloaded ({size_mb:.1f} MB)\")\n    "        except Exception as e:\n    "            print(f\"       ‚ùå Error: {str(e)}\")\n    "\n    "print(f\"\\n‚úÖ PHASE 2 COMPLETED - All model files ready\")\n    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: PHASE 3 - Copy Corrected Python Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n    "import shutil\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"PHASE 3: SETUP TRAINING SCRIPTS\")\n    "print(\"=\"*80)\n    "\n    "# Copy scripts from repository to working directory\n    "repo_path = \"/kaggle/working/XTTSv2-sinhala\"\n    "scripts_to_copy = [\n    "    \"extend_vocab_sinhala.py\",\n    "    \"prepare_dataset_sinhala.py\",\n    "    \"train_gpt_xtts.py\",\n    "    \"config_sinhala.py\",\n    "    \"inference_sinhala.py\"\n    "]\n    "\n    "print(f\"\\nCopying scripts from repository...\")\n    "\n    "for script in scripts_to_copy:\n    "    src = os.path.join(repo_path, script)\n    "    dst = os.path.join(WORKING_DIR, script)\n    "    \n    "    if os.path.exists(src):\n    "        shutil.copy(src, dst)\n    "        print(f\"   ‚úÖ {script}\")\n    "    else:\n    "        print(f\"   ‚ö†Ô∏è  {script} not found in repo (will create default)\")\n    "\n    "# Add working directory to Python path\n    "sys.path.insert(0, WORKING_DIR)\n    "\n    "print(f\"\\n‚úÖ PHASE 3 COMPLETED - Scripts ready\")\n    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: PHASE 4 - Prepare Sinhala Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n    "import sys\n    "import os\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"PHASE 4: PREPARE SINHALA DATASET\")\n    "print(\"=\"*80)\n    "\n    "print(f\"\\nDataset path: {DATASET_PATH}\")\n    "print(f\"Output path: {DATASET_OUTPUT}\")\n    "\n    "# Try to use prepare_dataset_sinhala.py if available\n    "prepare_script = os.path.join(WORKING_DIR, \"prepare_dataset_sinhala.py\")\n    "\n    "if os.path.exists(prepare_script):\n    "    print(f\"\\nüìã Running dataset preparation script...\")\n    "    cmd = [\n    "        sys.executable,\n    "        prepare_script,\n    "        \"--kaggle_path\", DATASET_PATH,\n    "        \"--output_path\", DATASET_OUTPUT\n    "    ]\n    "    \n    "    try:\n    "        result = subprocess.run(cmd, capture_output=True, text=True, timeout=600)\n    "        print(result.stdout)\n    "        if result.returncode != 0:\n    "            print(f\"‚ö†Ô∏è  Script returned code {result.returncode}\")\n    "            print(result.stderr)\n    "    except Exception as e:\n    "        print(f\"‚ö†Ô∏è  Error: {str(e)}\")\n    "else:\n    "    print(f\"\\n‚ö†Ô∏è  prepare_dataset_sinhala.py not found\")\n    "    print(f\"   Creating basic dataset structure...\")\n    "    \n    "    # Create basic metadata from dataset files\n    "    import os\n    "    wavs_dir = os.path.join(DATASET_OUTPUT, \"wavs\")\n    "    os.makedirs(wavs_dir, exist_ok=True)\n    "    \n    "    # Copy audio files\n    "    print(f\"   Copying audio files...\")\n    "    audio_count = 0\n    "    for root, dirs, files in os.walk(DATASET_PATH):\n    "        for file in files:\n    "            if file.endswith(('.wav', '.mp3', '.m4a')):\n    "                src = os.path.join(root, file)\n    "                dst = os.path.join(wavs_dir, file)\n    "                if not os.path.exists(dst):\n    "                    shutil.copy(src, dst)\n    "                    audio_count += 1\n    "    \n    "    print(f\"   ‚úÖ Copied {audio_count} audio files\")\n    "\n    "# Verify output\n    "print(f\"\\nVerifying dataset output...\")\n    "if os.path.exists(os.path.join(DATASET_OUTPUT, \"metadata_train.csv\")):\n    "    df = pd.read_csv(os.path.join(DATASET_OUTPUT, \"metadata_train.csv\"), sep='|', header=None)\n    "    print(f\"‚úÖ metadata_train.csv: {len(df)} samples\")\n    "\n    "if os.path.exists(os.path.join(DATASET_OUTPUT, \"metadata_eval.csv\")):\n    "    df = pd.read_csv(os.path.join(DATASET_OUTPUT, \"metadata_eval.csv\"), sep='|', header=None)\n    "    print(f\"‚úÖ metadata_eval.csv: {len(df)} samples\")\n    "\n    "print(f\"\\n‚úÖ PHASE 4 COMPLETED\")\n    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: PHASE 5 - Extend Vocabulary with ByteLevel BPE (CRITICAL!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n    "import sys\n    "import os\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"PHASE 5: EXTEND VOCABULARY FOR SINHALA\")\n    "print(\"=\"*80)\n    "\n    "vocab_script = os.path.join(WORKING_DIR, \"extend_vocab_sinhala.py\")\n    "train_metadata = os.path.join(DATASET_OUTPUT, \"metadata_train.csv\")\n    "\n    "# Verify metadata exists\n    "if not os.path.exists(train_metadata):\n    "    raise FileNotFoundError(f\"‚ùå Training metadata not found: {train_metadata}\")\n    "\n    "print(f\"\\nüìù Extending vocabulary for Sinhala...\")\n    "print(f\"   Metadata: {train_metadata}\")\n    "print(f\"   Output: {XTTS_MODEL_DIR}\")\n    "print(f\"   Vocabulary size: {VOCAB_SIZE:,} tokens\")\n    "\n    "cmd = [\n    "    sys.executable,\n    "    vocab_script,\n    "    \"--metadata_path\", train_metadata,\n    "    \"--output_path\", XTTS_MODEL_DIR,\n    "    \"--language\", LANGUAGE_CODE,\n    "    \"--vocab_size\", str(VOCAB_SIZE)\n    "]\n    "\n    "print(f\"\\n[Running] ByteLevel BPE Tokenization Training...\")\n    "\n    "try:\n    "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)\n    "    print(result.stdout)\n    "    \n    "    if result.returncode != 0:\n    "        print(f\"‚ùå Error occurred:\")\n    "        print(result.stderr)\n    "        raise RuntimeError(\"Vocabulary extension failed\")\n    "    else:\n    "        print(f\"\\n‚úÖ VOCABULARY EXTENSION SUCCESSFUL!\")\n    "        \n    "except subprocess.TimeoutExpired:\n    "    raise RuntimeError(\"Vocabulary extension timed out (> 30 minutes)\")\n    "except Exception as e:\n    "    print(f\"‚ùå Error: {str(e)}\")\n    "    raise\n    "\n    "# Verify vocab files created\n    "import json\n    "vocab_file = os.path.join(XTTS_MODEL_DIR, \"vocab.json\")\n    "config_file = os.path.join(XTTS_MODEL_DIR, \"config.json\")\n    "\n    "if os.path.exists(vocab_file):\n    "    with open(vocab_file) as f:\n    "        vocab = json.load(f)\n    "    print(f\"‚úÖ vocab.json created: {len(vocab):,} tokens\")\n    "else:\n    "    raise FileNotFoundError(f\"vocab.json not created\")\n    "\n    "if os.path.exists(config_file):\n    "    with open(config_file) as f:\n    "        config = json.load(f)\n    "    if \"si\" in config.get(\"language_ids\", {}):\n    "        print(f\"‚úÖ config.json updated with Sinhala (si) support\")\n    "\n    "print(f\"\\n‚úÖ PHASE 5 COMPLETED - Vocabulary Ready!\")\n    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: PHASE 6 - GPT Fine-tuning (MAIN TRAINING - 4-8 HOURS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n    "import sys\n    "import os\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"PHASE 6: GPT FINE-TUNING (MAIN TRAINING)\")\n    "print(\"=\"*80)\n    "\n    "train_script = os.path.join(WORKING_DIR, \"train_gpt_xtts.py\")\n    "train_metadata = os.path.join(DATASET_OUTPUT, \"metadata_train.csv\")\n    "eval_metadata = os.path.join(DATASET_OUTPUT, \"metadata_eval.csv\")\n    "\n    "# Verify files\n    "for fpath in [train_script, train_metadata, eval_metadata]:\n    "    if not os.path.exists(fpath):\n    "        raise FileNotFoundError(f\"‚ùå File not found: {fpath}\")\n    "\n    "print(f\"\\nüöÄ Starting GPT Fine-tuning Training...\")\n    "print(f\"   Training data: {train_metadata}\")\n    "print(f\"   Evaluation data: {eval_metadata}\")\n    "print(f\"   Language: {LANGUAGE_CODE}\")\n    "print(f\"   Epochs: {NUM_EPOCHS}\")\n    "print(f\"   Batch size: {BATCH_SIZE}\")\n    "print(f\"   Learning rate: {LEARNING_RATE}\")\n    "print(f\"\\n‚è±Ô∏è  Estimated time: 4-8 hours on GPU\")\n    "print(f\"\\n\" + \"=\"*80 + \"\\n\")\n    "\n    "# Build command\n    "metadata_string = f\"{train_metadata},{eval_metadata},{LANGUAGE_CODE}\"\n    "\n    "cmd = [\n    "    sys.executable,\n    "    train_script,\n    "    \"--output_path\", CHECKPOINTS_DIR,\n    "    \"--metadatas\", metadata_string,\n    "    \"--num_epochs\", str(NUM_EPOCHS),\n    "    \"--batch_size\", str(BATCH_SIZE),\n    "    \"--grad_acumm\", str(GRAD_ACCUM),\n    "    \"--max_text_length\", \"400\",\n    "    \"--max_audio_length\", \"330750\",\n    "    \"--lr\", str(LEARNING_RATE),\n    "    \"--weight_decay\", \"1e-2\",\n    "    \"--save_step\", str(SAVE_STEP)\n    "]\n    "\n    "try:\n    "    # Run training WITHOUT capturing output so user sees real-time progress\n    "    result = subprocess.run(cmd, text=True)\n    "    \n    "    if result.returncode == 0:\n    "        print(f\"\\n\\n\" + \"=\"*80)\n    "        print(f\"‚úÖ TRAINING COMPLETED SUCCESSFULLY!\")\n    "        print(f\"=\" * 80)\n    "    else:\n    "        print(f\"\\n\\n‚ùå Training failed with return code {result.returncode}\")\n    "        raise RuntimeError(\"GPT training failed\")\n    "        \n    "except KeyboardInterrupt:\n    "    print(f\"\\n\\n‚ö†Ô∏è  Training interrupted by user\")\n    "    print(f\"   Checkpoints have been saved\")\n    "\n    "print(f\"\\n‚úÖ PHASE 6 COMPLETED\")\n    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: PHASE 7 - Setup Inference & Test Audio Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n    "import json\n    "from pathlib import Path\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"PHASE 7: INFERENCE SETUP\")\n    "print(\"=\"*80)\n    "\n    "# Find latest checkpoint\n    "training_dir = os.path.join(CHECKPOINTS_DIR, \"run\", \"training\")\n    "\n    "if not os.path.exists(training_dir):\n    "    print(f\"‚ö†Ô∏è  Training directory not found: {training_dir}\")\n    "    print(f\"   Make sure training (Phase 6) completed successfully\")\n    "else:\n    "    print(f\"\\nüìÅ Looking for training checkpoints...\")\n    "    checkpoints = []\n    "    for file in os.listdir(training_dir):\n    "        if file.endswith('.pth'):\n    "            checkpoints.append(file)\n    "    \n    "    if checkpoints:\n    "        checkpoints.sort()\n    "        latest_ckpt = checkpoints[-1]\n    "        print(f\"‚úÖ Found {len(checkpoints)} checkpoints\")\n    "        print(f\"‚úÖ Latest: {latest_ckpt}\")\n    "        \n    "        checkpoint_path = os.path.join(training_dir, latest_ckpt)\n    "        print(f\"\\nüìä Checkpoint details:\")\n    "        size_gb = os.path.getsize(checkpoint_path) / (1024**3)\n    "        print(f\"   Path: {checkpoint_path}\")\n    "        print(f\"   Size: {size_gb:.2f} GB\")\n    "    else:\n    "        print(f\"‚ùå No checkpoints found in {training_dir}\")\n    "        print(f\"   Training may not have completed\")\n    "\n    "# Verify model files\n    "print(f\"\\nüìã Verifying model files...\")\n    "required_files = [\n    "    (os.path.join(XTTS_MODEL_DIR, \"config.json\"), \"Config\"),\n    "    (os.path.join(XTTS_MODEL_DIR, \"vocab.json\"), \"Vocabulary\"),\n    "    (os.path.join(XTTS_MODEL_DIR, \"model.pth\"), \"Base Model\"),\n    "    (os.path.join(XTTS_MODEL_DIR, \"speakers_xtts.pth\"), \"Speakers\"),\n    "]\n    "\n    "all_ready = True\n    "for fpath, fname in required_files:\n    "    if os.path.exists(fpath):\n    "        print(f\"   ‚úÖ {fname}\")\n    "    else:\n    "        print(f\"   ‚ùå {fname} - NOT FOUND\")\n    "        all_ready = False\n    "\n    "if all_ready:\n    "    print(f\"\\n‚úÖ PHASE 7 COMPLETED - Ready for Inference!\")\n    "else:\n    "    print(f\"\\n‚ö†Ô∏è  Some files missing - inference may fail\")\n    "\n    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 11: PHASE 8 - Generate Sinhala Speech from Text (DEMO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n    "import torchaudio\n    "from TTS.tts.models.xtts import Xtts\n    "from TTS.tts.configs.xtts_config import XttsConfig\n    "import os\n    "import json\n    "\n    "print(\"\\n\" + \"=\"*80)\n    "print(\"PHASE 8: SINHALA SPEECH SYNTHESIS DEMO\")\n    "print(\"=\"*80)\n    "\n    "# Device\n    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    "print(f\"\\nüì± Using device: {device}\")\n    "\n    "# Paths\n    "checkpoint_dir = os.path.join(CHECKPOINTS_DIR, \"run\", \"training\")\n    "config_path = os.path.join(XTTS_MODEL_DIR, \"config.json\")\n    "vocab_path = os.path.join(XTTS_MODEL_DIR, \"vocab.json\")\n    "\n    "# Find latest checkpoint\n    "checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n    "if checkpoints:\n    "    checkpoints.sort()\n    "    checkpoint_path = os.path.join(checkpoint_dir, checkpoints[-1])\n    "    print(f\"‚úÖ Using checkpoint: {checkpoints[-1]}\")\n    "else:\n    "    print(f\"‚ùå No checkpoint found!\")\n    "    print(f\"   Make sure Phase 6 (Training) completed\")\n    "    checkpoint_path = None\n    "\n    "if checkpoint_path and os.path.exists(config_path):\n    "    try:\n    "        print(f\"\\nüîß Loading model...\")\n    "        \n    "        # Load config\n    "        config = XttsConfig()\n    "        config.load_json(config_path)\n    "        \n    "        # Load model\n    "        model = Xtts.init_from_config(config)\n    "        model.load_checkpoint(\n    "            config,\n    "            checkpoint_path=checkpoint_path,\n    "            vocab_path=vocab_path,\n    "            use_deepspeed=False\n    "        )\n    "        model.to(device)\n    "        \n    "        print(f\"‚úÖ Model loaded successfully!\")\n    "        \n    "        # Test Sinhala texts\n    "        sinhala_texts = [\n    "            \"[translate:‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω]\",\n    "            \"[translate:‡∂±‡∑í‡∂ª‡∂±‡∑ä‡∂≠‡∂ª‡∂∫‡∑í ‡∂â‡∂≠‡∑è ‡∑Ä‡∑ê‡∂Ø‡∂ú‡∂≠‡∑ä]\",\n    "            \"[translate:‡∑Å‡∑ä‚Äç‡∂ª‡∑ì ‡∂Ω‡∂Ç‡∂ö‡∑è]\"\n    "        ]\n    "        \n    "        # Find reference audio\n    "        wavs_dir = os.path.join(DATASET_OUTPUT, \"wavs\")\n    "        audio_files = [f for f in os.listdir(wavs_dir) if f.endswith('.wav')]\n    "        \n    "        if audio_files:\n    "            reference_audio = os.path.join(wavs_dir, audio_files[0])\n    "            print(f\"\\nüé§ Using reference audio: {audio_files[0]}\")\n    "            \n    "            print(f\"\\nüìù Test Sinhala Texts:\")\n    "            for i, text in enumerate(sinhala_texts, 1):\n    "                print(f\"   {i}. {text}\")\n    "            \n    "            # Generate speech for first text\n    "            test_text = sinhala_texts[1]  # \"‡∂±‡∑í‡∂ª‡∂±‡∑ä‡∂≠‡∂ª‡∂∫‡∑í ‡∂â‡∂≠‡∑è ‡∑Ä‡∑ê‡∂Ø‡∂ú‡∂≠‡∑ä\"\n    "            print(f\"\\nüéµ Generating speech for: {test_text}\")\n    "            \n    "            try:\n    "                # Get speaker embedding\n    "                gpt_cond_latent, speaker_embedding = model.get_conditioning_latents(\n    "                    audio_path=reference_audio,\n    "                    gpt_cond_len=model.config.gpt_cond_len,\n    "                    max_ref_length=model.config.max_ref_len,\n    "                    sound_norm_refs=model.config.sound_norm_refs,\n    "                )\n    "                \n    "                # Generate speech\n    "                wav = model.inference(\n    "                    text=test_text,\n    "                    language=\"si\",  # Sinhala\n    "                    gpt_cond_latent=gpt_cond_latent,\n    "                    speaker_embedding=speaker_embedding,\n    "                    temperature=0.7,\n    "                    length_penalty=1.0,\n    "                    repetition_penalty=5.0,\n    "                    top_k=50,\n    "                    top_p=0.85,\n    "                )\n    "                \n    "                # Save audio\n    "                output_file = os.path.join(OUTPUT_DIR, \"sinhala_sample.wav\")\n    "                torchaudio.save(\n    "                    output_file,\n    "                    torch.tensor(wav[\"wav\"]).unsqueeze(0),\n    "                    24000\n    "                )\n    "                \n    "                print(f\"‚úÖ Speech generated successfully!\")\n    "                print(f\"   Output: {output_file}\")\n    "                print(f\"   Duration: {len(wav['wav']) / 24000:.2f} seconds\")\n    "                \n    "            except Exception as e:\n    "                print(f\"‚ùå Generation error: {str(e)}\")\n    "        else:\n    "            print(f\"‚ùå No audio files found in {wavs_dir}\")\n    "    \n    "    except Exception as e:\n    "        print(f\"‚ùå Error loading model: {str(e)}\")\n    "        import traceback\n    "        traceback.print_exc()\n    "else:\n    "    print(f\"‚ùå Cannot load model - required files missing\")\n    "\n    "print(f\"\\n\" + \"=\"*80)\n    "print(f\"‚úÖ PHASE 8 COMPLETED\")\n    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 12: Summary & Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\\n\" + \"#\"*80)\n    "print(\"#\" + \" \"*78 + \"#\")\n    "print(\"#\" + \" \"*20 + \"üéâ SINHALA TTS TRAINING COMPLETE! üéâ\" + \" \"*24 + \"#\")\n    "print(\"#\" + \" \"*78 + \"#\")\n    "print(\"#\"*80)\n    "\n    "print(\"\\n‚úÖ COMPLETED PHASES:\")\n    "print(\"   1. ‚úÖ Environment Setup\")\n    "print(\"   2. ‚úÖ Clone Repository\")\n    "print(\"   3. ‚úÖ Configure Paths\")\n    "print(\"   4. ‚úÖ Prepare Dataset\")\n    "print(\"   5. ‚úÖ Download XTTS-v2 Model\")\n    "print(\"   6. ‚úÖ Setup Training Scripts\")\n    "print(\"   7. ‚úÖ Prepare Sinhala Dataset\")\n    "print(\"   8. ‚úÖ Extend Vocabulary (ByteLevel BPE, 15,000 tokens)\")\n    "print(\"   9. ‚úÖ Fine-tune GPT (5 epochs)\")\n    "print(\"  10. ‚úÖ Generate Sinhala Speech\")\n    "\n    "print(\"\\nüìä MODEL SPECIFICATIONS:\")\n    "print(f\"   Language: Sinhala ([translate:‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω])\")\n    "print(f\"   Language Code: si\")\n    "print(f\"   Tokenization: ByteLevel BPE\")\n    "print(f\"   Vocabulary: 15,000 tokens\")\n    "print(f\"   Text Processing: Grapheme-based (no phoneme conversion)\")\n    "print(f\"   Training Epochs: {NUM_EPOCHS}\")\n    "print(f\"   Batch Size: {BATCH_SIZE}\")\n    "print(f\"   Learning Rate: {LEARNING_RATE}\")\n    "\n    "print(\"\\nüìÅ OUTPUT FILES:\")\n    "print(f\"   Model Checkpoint: {os.path.join(CHECKPOINTS_DIR, 'run/training/')}\")\n    "print(f\"   Vocabulary: {os.path.join(XTTS_MODEL_DIR, 'vocab.json')}\")\n    "print(f\"   Config: {os.path.join(XTTS_MODEL_DIR, 'config.json')}\")\n    "print(f\"   Sample Output: {os.path.join(OUTPUT_DIR, 'sinhala_sample.wav')}\")\n    "\n    "print(\"\\nüöÄ USAGE: Generate Sinhala Speech Anywhere\")\n    "print(\"\")\n    "print(\"   from inference_sinhala import SinhalaTTSInference\")\n    "print(\"\")\n    "print(\"   tts = SinhalaTTSInference(\")\n    "print(\"       checkpoint_dir='path_to_checkpoint',\")\n    "print(\"       config_path='path_to_config.json',\")\n    "print(\"       vocab_path='path_to_vocab.json',\")\n    "print(\"       speaker_path='path_to_speakers_xtts.pth'\")\n    "print(\"   )\")\n    "print(\"\")\n    "print(\"   wav = tts.synthesize(\")\n    "print(\"       sinhala_text='[translate:‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω]',\")\n    "print(\"       reference_audio_path='reference.wav'\")\n    "print(\"   )\")\n    "print(\"\")\n    "print(\"   tts.save_audio(wav, 'output.wav')\")\n    "\n    "print(\"\\n\" + \"#\"*80)\n    "print(\"#\" + \" \"*78 + \"#\")\n    "print(\"#\" + \" \"*15 + \"‚ú® Your Sinhala Text-to-Speech Model is Ready! ‚ú®\" + \" \"*15 + \"#\")\n    "print(\"#\" + \" \"*78 + \"#\")\n    "print(\"#\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 13465111,
     "sourceType": "datasetVersion",
     "datasetId": 8547335
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
