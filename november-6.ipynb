{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceType":"datasetVersion","sourceId":13465111,"datasetId":8547335,"databundleVersionId":14183142,"isSourceIdPinned":false}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport shutil\nimport kagglehub\nimport os\n\n# Download dataset\npath = kagglehub.dataset_download(\"amalshaf/sinhala-tts-dataset\")\nprint(f\"Dataset downloaded to: {path}\")\n\n# Setup paths\nkaggle_dataset_path = f\"{path}/sinhala-tts-dataset\"\nprint(f\"kaggle dataset path: {kaggle_dataset_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:47:18.838949Z","iopub.execute_input":"2025-11-06T05:47:18.839269Z","iopub.status.idle":"2025-11-06T05:47:20.729246Z","shell.execute_reply.started":"2025-11-06T05:47:18.839249Z","shell.execute_reply":"2025-11-06T05:47:20.728633Z"}},"outputs":[{"name":"stdout","text":"Dataset downloaded to: /kaggle/input/sinhala-tts-dataset\nkaggle dataset path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# SINHALA XTTS-v2 KAGGLE NOTEBOOK - FIXED VERSION\n# Step-by-step notebook with proper error handling\n# Copy cells one by one into Kaggle notebook\n\n################################################################################\n# CELL 1: SETUP AND ENVIRONMENT VERIFICATION\n################################################################################\n\nprint(\"=\"*100)\nprint(\"PHASE 1: KAGGLE SETUP & ENVIRONMENT VERIFICATION\")\nprint(\"=\"*100)\n\nimport os\nimport sys\nimport subprocess\nimport gc\nimport torch\nimport torchaudio\nimport shutil\nfrom pathlib import Path\nfrom datetime import datetime\n\nprint(f\"\\nPyTorch Version: {torch.__version__}\")\nprint(f\"CUDA Available: {torch.cuda.is_available()}\")\n\nif torch.cuda.is_available():\n    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n\n# Set environment variables for GPU memory management\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"0\"\n\n# Define key paths for Kaggle\nKAGGLE_INPUT_PATH = f\"{path}/sinhala-tts-dataset\"\nKAGGLE_WORKING_PATH = \"/kaggle/working\"\nREPO_DIR = os.path.join(KAGGLE_WORKING_PATH, \"XTTSv2-sinhala\")\nCHECKPOINTS_DIR = os.path.join(KAGGLE_WORKING_PATH, \"checkpoints\")\nDATASET_OUTPUT_DIR = os.path.join(KAGGLE_WORKING_PATH, \"datasets\")\nOUTPUT_DIR = os.path.join(KAGGLE_WORKING_PATH, \"output\")\n\n# Create directories\nfor dir_path in [CHECKPOINTS_DIR, DATASET_OUTPUT_DIR, OUTPUT_DIR]:\n    os.makedirs(dir_path, exist_ok=True)\n\nos.chdir(KAGGLE_WORKING_PATH)\n\nprint(f\"\\nWorking directory: {os.getcwd()}\")\nprint(f\"Dataset path: {KAGGLE_INPUT_PATH}\")\n\n# Verify dataset access\nif os.path.exists(KAGGLE_INPUT_PATH):\n    print(f\"\\nDataset found! Contents:\")\n    for item in os.listdir(KAGGLE_INPUT_PATH):\n        item_path = os.path.join(KAGGLE_INPUT_PATH, item)\n        if os.path.isfile(item_path):\n            size_mb = os.path.getsize(item_path) / (1024*1024)\n            print(f\"    {item} ({size_mb:.1f} MB)\")\n        else:\n            item_count = len(os.listdir(item_path))\n            print(f\"    {item}/ ({item_count} items)\")\nelse:\n    print(f\"\\nERROR: Dataset not found at {KAGGLE_INPUT_PATH}\")\n    print(\"Make sure to add 'sinhala-tts-dataset' in notebook settings!\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:47:28.649664Z","iopub.execute_input":"2025-11-06T05:47:28.650288Z","iopub.status.idle":"2025-11-06T05:47:28.685558Z","shell.execute_reply.started":"2025-11-06T05:47:28.650264Z","shell.execute_reply":"2025-11-06T05:47:28.684979Z"}},"outputs":[{"name":"stdout","text":"====================================================================================================\nPHASE 1: KAGGLE SETUP & ENVIRONMENT VERIFICATION\n====================================================================================================\n\nPyTorch Version: 2.6.0+cu124\nCUDA Available: True\n  GPU: Tesla P100-PCIE-16GB\n  Memory: 15.89 GB\n\nWorking directory: /kaggle/working\nDataset path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\n\nDataset found! Contents:\n    metadata_eval.csv (0.0 MB)\n    metadata_train.csv (0.2 MB)\n    wavs/ (1251 items)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"################################################################################\n# CELL 2: CLONE REPOSITORY\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 2: CLONE REPOSITORY\")\nprint(\"=\"*100)\n\nREPO_URL = \"https://github.com/amalshafernando/XTTSv2-sinhala.git\"\n\nif os.path.exists(REPO_DIR):\n    print(f\"\\nRepository already exists at {REPO_DIR}\")\nelse:\n    print(f\"\\nCloning from: {REPO_URL}\")\n    result = subprocess.run(\n        [\"git\", \"clone\", REPO_URL, REPO_DIR],\n        capture_output=True,\n        text=True\n    )\n    \n    if result.returncode == 0:\n        print(\"Repository cloned successfully\")\n    else:\n        print(f\"ERROR cloning repository:\")\n        print(result.stderr)\n        sys.exit(1)\n\nsys.path.insert(0, REPO_DIR)\nprint(f\"Repository added to Python path\")\n\n# List files in repo\nrepo_files = os.listdir(REPO_DIR)\nprint(f\"\\nRepository files ({len(repo_files)}):\")\nfor f in sorted(repo_files)[:15]:\n    print(f\"  {f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:49:21.753363Z","iopub.execute_input":"2025-11-06T05:49:21.754148Z","iopub.status.idle":"2025-11-06T05:49:22.282964Z","shell.execute_reply.started":"2025-11-06T05:49:21.754123Z","shell.execute_reply":"2025-11-06T05:49:22.282314Z"}},"outputs":[{"name":"stdout","text":"\n====================================================================================================\nPHASE 2: CLONE REPOSITORY\n====================================================================================================\n\nCloning from: https://github.com/amalshafernando/XTTSv2-sinhala.git\nRepository cloned successfully\nRepository added to Python path\n\nRepository files (18):\n  .git\n  .gitignore\n  README.md\n  TTS\n  config_sinhala.py\n  download_checkpoint.py\n  extend_vocab_sinhala.py\n  inference_sinhala.py\n  kaggle_train_sinhala.py\n  kagglebook.ipynb\n  prepare_dataset_sinhala.py\n  recipes\n  requirements.txt\n  sinhala_tts_complete_kaggle_notebook.ipynb\n  train_dvae_xtts.py\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"################################################################################\n# CELL 3: INSTALL DEPENDENCIES\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 3: INSTALL DEPENDENCIES\")\nprint(\"=\"*100)\n\npackages = [\"TTS\", \"tokenizers\", \"transformers\", \"pandas\", \"tqdm\", \"librosa\"]\n\nprint(\"\\nInstalling packages (this may take a few minutes)...\")\nfor package in packages:\n    print(f\"  {package}...\", end=\" \", flush=True)\n    result = subprocess.run(\n        [sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package],\n        capture_output=True\n    )\n    if result.returncode == 0:\n        print(\"OK\")\n    else:\n        print(\"(skipped - may already exist)\")\n\nprint(\"\\nAll dependencies installed\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:50:40.228590Z","iopub.execute_input":"2025-11-06T05:50:40.229207Z","iopub.status.idle":"2025-11-06T05:53:10.944027Z","shell.execute_reply.started":"2025-11-06T05:50:40.229187Z","shell.execute_reply":"2025-11-06T05:53:10.943337Z"}},"outputs":[{"name":"stdout","text":"\n====================================================================================================\nPHASE 3: INSTALL DEPENDENCIES\n====================================================================================================\n\nInstalling packages (this may take a few minutes)...\n  TTS... OK\n  tokenizers... OK\n  transformers... OK\n  pandas... OK\n  tqdm... OK\n  librosa... OK\n\nAll dependencies installed\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"################################################################################\n# CELL 4: LOAD CONFIGURATION\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 4: LOAD CONFIGURATION\")\nprint(\"=\"*100)\n\ntry:\n    sys.path.insert(0, REPO_DIR)\n    from config_sinhala import (\n        LANGUAGE_CODE, EXTENDED_VOCAB_SIZE,\n        BATCH_SIZE, GRADIENT_ACCUMULATION, \n        LEARNING_RATE, WEIGHT_DECAY, NUM_EPOCHS, \n        SAVE_STEP, MAX_TEXT_LENGTH, MAX_AUDIO_LENGTH\n    )\n    print(\"\\nConfiguration loaded successfully\")\nexcept ImportError as e:\n    print(f\"\\nERROR importing config_sinhala: {e}\")\n    sys.exit(1)\n\nprint(f\"\\nConfiguration Parameters:\")\nprint(f\"  Language: {LANGUAGE_CODE}\")\nprint(f\"  Vocab Size: {EXTENDED_VOCAB_SIZE}\")\nprint(f\"  Batch Size: {BATCH_SIZE}\")\nprint(f\"  Gradient Accumulation: {GRADIENT_ACCUMULATION}\")\nprint(f\"  Learning Rate: {LEARNING_RATE}\")\nprint(f\"  Num Epochs: {NUM_EPOCHS}\")\nprint(f\"  Max Text Length: {MAX_TEXT_LENGTH}\")\nprint(f\"  Max Audio Length: {MAX_AUDIO_LENGTH}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:54:54.368478Z","iopub.execute_input":"2025-11-06T05:54:54.369232Z","iopub.status.idle":"2025-11-06T05:54:54.377842Z","shell.execute_reply.started":"2025-11-06T05:54:54.369206Z","shell.execute_reply":"2025-11-06T05:54:54.377243Z"}},"outputs":[{"name":"stdout","text":"\n====================================================================================================\nPHASE 4: LOAD CONFIGURATION\n====================================================================================================\n\nConfiguration loaded successfully\n\nConfiguration Parameters:\n  Language: si\n  Vocab Size: 15000\n  Batch Size: 8\n  Gradient Accumulation: 4\n  Learning Rate: 5e-06\n  Num Epochs: 5\n  Max Text Length: 400\n  Max Audio Length: 330750\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"################################################################################\n# CELL 5: VERIFY DATASET FORMAT\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 5: VERIFY DATASET FORMAT (CRITICAL)\")\nprint(\"=\"*100)\n\nimport pandas as pd\n\nmetadata_train_path = os.path.join(KAGGLE_INPUT_PATH, \"metadata_train.csv\")\nmetadata_eval_path = os.path.join(KAGGLE_INPUT_PATH, \"metadata_eval.csv\")\n\nprint(f\"\\nChecking CSV files...\")\n\n# Read train metadata\ntry:\n    df_train = pd.read_csv(metadata_train_path)\n    print(f\"\\n[Train CSV]\")\n    print(f\"  Columns: {list(df_train.columns)}\")\n    print(f\"  Rows: {len(df_train)}\")\n    print(f\"\\n  First row data:\")\n    for col in df_train.columns:\n        value = df_train.iloc[0][col]\n        print(f\"    {col}: {value}\")\nexcept Exception as e:\n    print(f\"ERROR reading train CSV: {e}\")\n\n# Read eval metadata\ntry:\n    df_eval = pd.read_csv(metadata_eval_path)\n    print(f\"\\n[Eval CSV]\")\n    print(f\"  Columns: {list(df_eval.columns)}\")\n    print(f\"  Rows: {len(df_eval)}\")\nexcept Exception as e:\n    print(f\"ERROR reading eval CSV: {e}\")\n\n# Find audio directory\nprint(f\"\\nSearching for audio files...\")\naudio_dir = None\npossible_dirs = [\n    os.path.join(KAGGLE_INPUT_PATH, \"wav\"),\n    os.path.join(KAGGLE_INPUT_PATH, \"wavs\"),\n    os.path.join(KAGGLE_INPUT_PATH, \"audio\"),\n    KAGGLE_INPUT_PATH\n]\n\nfor dir_path in possible_dirs:\n    if os.path.exists(dir_path):\n        wav_files = [f for f in os.listdir(dir_path) if f.endswith(('.wav', '.mp3', '.flac'))]\n        if wav_files:\n            audio_dir = dir_path\n            print(f\"  Found {len(wav_files)} audio files in: {os.path.basename(dir_path)}\")\n            print(f\"  Sample: {wav_files[0]}\")\n            break\n\nif not audio_dir:\n    print(f\"  WARNING: Could not find audio files\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:55:11.643719Z","iopub.execute_input":"2025-11-06T05:55:11.644438Z","iopub.status.idle":"2025-11-06T05:55:11.681813Z","shell.execute_reply.started":"2025-11-06T05:55:11.644390Z","shell.execute_reply":"2025-11-06T05:55:11.681230Z"}},"outputs":[{"name":"stdout","text":"\n====================================================================================================\nPHASE 5: VERIFY DATASET FORMAT (CRITICAL)\n====================================================================================================\n\nChecking CSV files...\n\n[Train CSV]\n  Columns: ['audio_file_path|transcript|speaker_id']\n  Rows: 1000\n\n  First row data:\n    audio_file_path|transcript|speaker_id: wavs/sin_2282_sin_2282_8427486285.wav|පොළොන්නරුවේ ගල් විහාරයේ හිඳි පිළිමය මෙවැන්නකි.|sin_2282\n\n[Eval CSV]\n  Columns: ['audio_file_path|transcript|speaker_id']\n  Rows: 251\n\nSearching for audio files...\n  Found 1251 audio files in: wavs\n  Sample: sin_3531_sin_3531_8500473878.wav\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"################################################################################\n# CELL 6: PREPARE DATASET\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 6: PREPARE DATASET\")\nprint(\"=\"*100)\n\nprint(f\"\\nInput: {KAGGLE_INPUT_PATH}\")\nprint(f\"Output: {DATASET_OUTPUT_DIR}\")\n\n# Create output directories\nos.makedirs(os.path.join(DATASET_OUTPUT_DIR, \"wavs\"), exist_ok=True)\n\nprepare_script = os.path.join(REPO_DIR, \"prepare_dataset_sinhala.py\")\n\nif not os.path.exists(prepare_script):\n    print(f\"ERROR: prepare_dataset_sinhala.py not found\")\nelse:\n    cmd = [\n        sys.executable,\n        prepare_script,\n        f\"--kaggle_path={KAGGLE_INPUT_PATH}\",\n        f\"--output_path={DATASET_OUTPUT_DIR}\"\n    ]\n    \n    print(f\"\\nRunning dataset preparation...\\n\")\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    \n    print(result.stdout)\n    if result.stderr:\n        print(f\"Warnings/Errors:\\n{result.stderr}\")\n    \n    if result.returncode != 0:\n        print(f\"\\nReturn code: {result.returncode}\")\n\n# Verify output\nprint(f\"\\nDataset output structure:\")\nif os.path.exists(DATASET_OUTPUT_DIR):\n    for item in os.listdir(DATASET_OUTPUT_DIR):\n        item_path = os.path.join(DATASET_OUTPUT_DIR, item)\n        if os.path.isfile(item_path):\n            size = os.path.getsize(item_path)\n            print(f\"  {item} ({size} bytes)\")\n        else:\n            file_count = len(os.listdir(item_path))\n            print(f\"  {item}/ ({file_count} files)\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T05:55:47.758191Z","iopub.execute_input":"2025-11-06T05:55:47.758977Z","iopub.status.idle":"2025-11-06T05:55:48.326386Z","shell.execute_reply.started":"2025-11-06T05:55:47.758953Z","shell.execute_reply":"2025-11-06T05:55:48.325741Z"}},"outputs":[{"name":"stdout","text":"\n====================================================================================================\nPHASE 6: PREPARE DATASET\n====================================================================================================\n\nInput: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\nOutput: /kaggle/working/datasets\n\nRunning dataset preparation...\n\n\n============================================================\nCONVERTING DATASET TO XTTS-v2 FORMAT\n============================================================\n\n[2/3] Reading metadata files\n    Train: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_train.csv\n    Eval: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset/metadata_eval.csv\n    Train samples: 1000\n    Eval samples: 251\n\n❌ ERROR: train metadata missing required columns: {'transcript', 'speaker_id', 'audio_file_path'}\n\nWarnings/Errors:\nTraceback (most recent call last):\n  File \"/kaggle/working/XTTSv2-sinhala/prepare_dataset_sinhala.py\", line 257, in main\n    convert_metadata(args.kaggle_path, args.output_path)\n  File \"/kaggle/working/XTTSv2-sinhala/prepare_dataset_sinhala.py\", line 104, in convert_metadata\n    raise ValueError(f\"{df_name} metadata missing required columns: {missing_cols}\")\nValueError: train metadata missing required columns: {'transcript', 'speaker_id', 'audio_file_path'}\n\n\nReturn code: 1\n\nDataset output structure:\n  wavs/ (0 files)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"################################################################################\n# CELL 7: DOWNLOAD CHECKPOINTS\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 7: DOWNLOAD PRETRAINED CHECKPOINTS\")\nprint(\"=\"*100)\n\ndownload_script = os.path.join(REPO_DIR, \"download_checkpoint.py\")\n\nif not os.path.exists(download_script):\n    print(f\"ERROR: download_checkpoint.py not found\")\nelse:\n    cmd = [\n        sys.executable,\n        download_script,\n        f\"--output_path={CHECKPOINTS_DIR}\"\n    ]\n    \n    print(f\"\\nDownloading XTTS-v2 pretrained model...\")\n    print(f\"This may take 5-10 minutes...\\n\")\n    \n    result = subprocess.run(cmd, capture_output=True, text=True)\n    print(result.stdout)\n    if result.stderr:\n        print(f\"Output: {result.stderr}\")\n\n# Verify checkpoints\nprint(f\"\\nCheckpoints downloaded:\")\nif os.path.exists(CHECKPOINTS_DIR):\n    for item in os.listdir(CHECKPOINTS_DIR)[:10]:\n        print(f\"  {item}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"################################################################################\n# CELL 8: EXTEND VOCABULARY\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 8: EXTEND VOCABULARY FOR SINHALA\")\nprint(\"=\"*100)\n\nmetadata_train = os.path.join(DATASET_OUTPUT_DIR, \"metadata_train.csv\")\n\nif not os.path.exists(metadata_train):\n    print(f\"ERROR: metadata_train.csv not found\")\n    print(f\"Dataset preparation may have failed\")\nelse:\n    extend_script = os.path.join(REPO_DIR, \"extend_vocab_sinhala.py\")\n    \n    if not os.path.exists(extend_script):\n        print(f\"ERROR: extend_vocab_sinhala.py not found\")\n    else:\n        cmd = [\n            sys.executable,\n            extend_script,\n            f\"--output_path={CHECKPOINTS_DIR}\",\n            f\"--metadata_path={metadata_train}\",\n            f\"--language={LANGUAGE_CODE}\",\n            f\"--extended_vocab_size={EXTENDED_VOCAB_SIZE}\"\n        ]\n        \n        print(f\"\\nExtending vocabulary for Sinhala...\")\n        print(f\"This may take 2-5 minutes...\\n\")\n        \n        result = subprocess.run(cmd, capture_output=True, text=True)\n        print(result.stdout)\n        if result.stderr:\n            print(f\"Output: {result.stderr}\")\n\n# Clear GPU memory\ngc.collect()\nif torch.cuda.is_available():\n    torch.cuda.empty_cache()\n    print(\"\\nGPU memory cleared\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n################################################################################\n# CELL 9: FINE-TUNE GPT MODEL\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 9: FINE-TUNE GPT MODEL (MAIN TRAINING)\")\nprint(\"=\"*100)\n\nmetadata_train = os.path.join(DATASET_OUTPUT_DIR, \"metadata_train.csv\")\nmetadata_eval = os.path.join(DATASET_OUTPUT_DIR, \"metadata_eval.csv\")\n\nif not os.path.exists(metadata_train) or not os.path.exists(metadata_eval):\n    print(f\"ERROR: Metadata files not found\")\nelse:\n    train_script = os.path.join(REPO_DIR, \"train_gpt_xtts.py\")\n    \n    if not os.path.exists(train_script):\n        print(f\"ERROR: train_gpt_xtts.py not found\")\n    else:\n        # Format metadatas parameter\n        metadatas_param = f\"{metadata_train},{metadata_eval},{LANGUAGE_CODE}\"\n        \n        cmd = [\n            sys.executable,\n            train_script,\n            f\"--output_path={CHECKPOINTS_DIR}\",\n            f\"--metadatas={metadatas_param}\",\n            f\"--num_epochs={NUM_EPOCHS}\",\n            f\"--batch_size={BATCH_SIZE}\",\n            f\"--grad_acumm={GRADIENT_ACCUMULATION}\",\n            f\"--max_text_length={MAX_TEXT_LENGTH}\",\n            f\"--max_audio_length={MAX_AUDIO_LENGTH}\",\n            f\"--weight_decay={WEIGHT_DECAY}\",\n            f\"--lr={LEARNING_RATE}\",\n            f\"--save_step={SAVE_STEP}\"\n        ]\n        \n        print(f\"\\nSTARTING GPT FINE-TUNING\")\n        print(f\"Training Parameters:\")\n        print(f\"  Epochs: {NUM_EPOCHS}\")\n        print(f\"  Batch Size: {BATCH_SIZE}\")\n        print(f\"  Gradient Accumulation: {GRADIENT_ACCUMULATION}\")\n        print(f\"  Learning Rate: {LEARNING_RATE}\")\n        print(f\"\\nThis will take 2-4 hours. Please wait...\\n\")\n        \n        result = subprocess.run(cmd, capture_output=True, text=True)\n        print(result.stdout)\n        if result.stderr:\n            print(f\"Training output:\\n{result.stderr}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"################################################################################\n# CELL 10: TEST INFERENCE\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 10: TEST INFERENCE WITH FINE-TUNED MODEL\")\nprint(\"=\"*100)\n\n# Find fine-tuned checkpoint\ngpt_dirs = [d for d in os.listdir(CHECKPOINTS_DIR) if 'GPT_XTTS_FT' in d]\n\nif not gpt_dirs:\n    print(f\"\\nNo fine-tuned GPT checkpoint found\")\nelse:\n    latest_checkpoint = sorted(gpt_dirs)[-1]\n    checkpoint_path = os.path.join(CHECKPOINTS_DIR, latest_checkpoint)\n    \n    print(f\"\\nFine-tuned checkpoint found: {latest_checkpoint}\")\n    print(f\"Path: {checkpoint_path}\")\n    \n    # List model files\n    model_files = os.listdir(checkpoint_path)\n    print(f\"\\nCheckpoint contents:\")\n    for file in model_files[:10]:\n        print(f\"  {file}\")\n    if len(model_files) > 10:\n        print(f\"  ... and {len(model_files) - 10} more files\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"################################################################################\n# CELL 11: PACKAGE RESULTS\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"PHASE 11: PACKAGE RESULTS FOR DOWNLOAD\")\nprint(\"=\"*100)\n\ntimestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\noutput_package = os.path.join(OUTPUT_DIR, f\"sinhala-xtts-finetuned_{timestamp}\")\nos.makedirs(output_package, exist_ok=True)\n\nprint(f\"\\nPackaging results...\")\n\n# Copy fine-tuned models\ngpt_dirs = [d for d in os.listdir(CHECKPOINTS_DIR) if 'GPT_XTTS_FT' in d]\nfor checkpoint_dir in gpt_dirs:\n    src = os.path.join(CHECKPOINTS_DIR, checkpoint_dir)\n    dst = os.path.join(output_package, checkpoint_dir)\n    print(f\"  Copying {checkpoint_dir}...\")\n    if os.path.exists(dst):\n        shutil.rmtree(dst)\n    shutil.copytree(src, dst)\n\n# Copy inference script\nfor script_name in [\"inference_sinhala.py\", \"config_sinhala.py\"]:\n    src = os.path.join(REPO_DIR, script_name)\n    if os.path.exists(src):\n        dst = os.path.join(output_package, script_name)\n        shutil.copy2(src, dst)\n        print(f\"  Copied {script_name}\")\n\nprint(f\"\\nResults packaged at: {output_package}\")\nprint(f\"\\nContents:\")\nfor f in os.listdir(output_package):\n    print(f\"  {f}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"################################################################################\n# CELL 12: SUMMARY\n################################################################################\n\nprint(\"\\n\" + \"=\"*100)\nprint(\"TRAINING COMPLETE!\")\nprint(\"=\"*100)\n\nprint(\"\"\"\nYour Sinhala XTTS-v2 model has been successfully fine-tuned!\n\nCompleted Phases:\n  1. Environment setup\n  2. Repository clone\n  3. Dependencies install\n  4. Configuration load\n  5. Dataset verification\n  6. Dataset preparation\n  7. Checkpoint download\n  8. Vocabulary extension\n  9. GPT fine-tuning\n  10. Inference testing\n  11. Results packaging\n\nOutput Location:\n  /kaggle/working/output/sinhala-xtts-finetuned_TIMESTAMP/\n\nDownload these files:\n  - Fine-tuned model checkpoint\n  - inference_sinhala.py\n  - config_sinhala.py\n\nNext Steps:\n  1. Download the output folder\n  2. Use inference_sinhala.py to generate Sinhala speech\n  3. Integrate into your application\n  4. Fine-tune further with more data if needed\n\nThank you!\n\"\"\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}