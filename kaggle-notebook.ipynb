{
    "cells": [
     {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
       "# XTTS-v2 Sinhala Fine-tuning on Kaggle\n",
       "\n",
       "This notebook performs complete fine-tuning of XTTS-v2 for Sinhala language:\n",
       "1. Setup environment\n",
       "2. Clone repository\n",
       "3. Download dataset\n",
       "4. Download XTTS-v2 model\n",
       "5. Prepare dataset\n",
       "6. Extend vocabulary for Sinhala\n",
       "7. (Optional) DVAE fine-tuning\n",
       "8. GPT fine-tuning\n",
       "\n",
       "**Repository**: https://github.com/amalshafernando/XTTSv2-sinhala  \n",
       "**Dataset**: https://www.kaggle.com/datasets/amalshaf/sinhala-tts-dataset"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 1: Install PyTorch\n",
       "!pip install torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
       "\n",
       "# Verify\n",
       "import torch\n",
       "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
       "print(f\"PyTorch version: {torch.__version__}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 2: Set environment variables and verify GPU\n",
       "import os\n",
       "import sys\n",
       "\n",
       "# CRITICAL: Set these BEFORE any TTS imports\n",
       "os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT'] = '1'\n",
       "os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION'] = '1'\n",
       "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
       "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
       "\n",
       "print(\"‚úÖ Environment variables set\")\n",
       "print(f\"Working directory: {os.getcwd()}\")\n",
       "print(f\"Python version: {sys.version}\")\n",
       "\n",
       "# Check GPU\n",
       "import torch\n",
       "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
       "if torch.cuda.is_available():\n",
       "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
       "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 3: Install TTS and dependencies\n",
       "!pip install -q TTS==0.22.0\n",
       "!pip install -q transformers==4.36.0 tokenizers==0.15.0\n",
       "!pip install -q librosa==0.10.2 soundfile==0.12.1 scipy==1.11.2 pysbd==0.3.4\n",
       "!pip install -q pandas==1.5.3 scikit-learn==1.3.2 tqdm==4.66.3\n",
       "!pip install -q einops==0.7.0 unidecode==1.3.8 inflect==7.0.0\n",
       "!pip install -q coqpit==0.0.16 trainer==0.0.36 mutagen\n",
       "!pip install -q pypinyin hangul_romanize num2words kagglehub\n",
       "\n",
       "print(\"‚úÖ All dependencies installed successfully!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 4: Verify critical packages\n",
       "import trainer\n",
       "import TTS\n",
       "import transformers\n",
       "import librosa\n",
       "\n",
       "print(f\"trainer version: {trainer.__version__}\")\n",
       "print(f\"TTS installed: {TTS.__version__}\")\n",
       "print(f\"transformers version: {transformers.__version__}\")\n",
       "print(f\"librosa version: {librosa.__version__}\")\n",
       "print(\"‚úÖ All packages verified!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 5: Clone repository\n",
       "import os\n",
       "\n",
       "repo_url = \"https://github.com/amalshafernando/XTTSv2-sinhala.git\"\n",
       "repo_name = \"XTTSv2-sinhala\"\n",
       "\n",
       "# Clone only if it doesn't exist\n",
       "if not os.path.exists(repo_name):\n",
       "    print(f\"üîπ Cloning {repo_name}...\")\n",
       "    !git clone {repo_url}\n",
       "    print(\"‚úÖ Repository cloned\")\n",
       "else:\n",
       "    print(f\"‚úÖ Repository already exists: {repo_name}\")\n",
       "\n",
       "# Change to repo directory\n",
       "os.chdir(repo_name)\n",
       "print(f\"‚úÖ Current directory: {os.getcwd()}\")\n",
       "\n",
       "# List contents\n",
       "print(\"\\nüîπ Repository contents:\")\n",
       "!ls -la"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 6: Download dataset\n",
       "import pandas as pd\n",
       "import shutil\n",
       "import kagglehub\n",
       "import os\n",
       "\n",
       "# Download dataset\n",
       "path = kagglehub.dataset_download(\"amalshaf/sinhala-tts-dataset\")\n",
       "print(f\"Dataset downloaded to: {path}\")\n",
       "\n",
       "# Setup paths\n",
       "kaggle_dataset_path = f\"{path}/sinhala-tts-dataset\"\n",
       "print(f\"Kaggle dataset path: {kaggle_dataset_path}\")\n",
       "target_dataset_path = \"/kaggle/working/datasets/\"\n",
       "print(f\"Target dataset path: {target_dataset_path}\")\n",
       "\n",
       "# Create target directory\n",
       "os.makedirs(f\"{target_dataset_path}/wavs\", exist_ok=True)\n",
       "\n",
       "# Copy audio files if they exist\n",
       "possible_audio_dirs = [\n",
       "    f\"{kaggle_dataset_path}/wavs\",\n",
       "    f\"{kaggle_dataset_path}/wav\",\n",
       "    f\"{kaggle_dataset_path}/audio\",\n",
       "    kaggle_dataset_path\n",
       "]\n",
       "\n",
       "audio_copied = False\n",
       "for audio_dir in possible_audio_dirs:\n",
       "    if os.path.exists(audio_dir):\n",
       "        # Check if it contains audio files\n",
       "        audio_files = [f for f in os.listdir(audio_dir) if f.endswith(('.wav', '.mp3', '.flac'))]\n",
       "        if audio_files:\n",
       "            print(f\"‚úÖ Found audio directory: {audio_dir} ({len(audio_files)} files)\")\n",
       "            if os.path.isdir(audio_dir):\n",
       "                shutil.copytree(audio_dir, f\"{target_dataset_path}/wavs\", dirs_exist_ok=True)\n",
       "            else:\n",
       "                shutil.copy2(audio_dir, f\"{target_dataset_path}/wavs/\")\n",
       "            audio_copied = True\n",
       "            break\n",
       "\n",
       "if not audio_copied:\n",
       "    print(\"‚ö†Ô∏è Warning: Could not find audio files. Will proceed with metadata only.\")\n",
       "\n",
       "# Convert CSV to XTTS format using prepare_dataset_sinhala.py\n",
       "print(\"\\nüîπ Converting dataset to XTTS format...\")\n",
       "!python prepare_dataset_sinhala.py --kaggle_path \"{kaggle_dataset_path}\" --output_path \"{target_dataset_path}\"\n",
       "\n",
       "# Verify output\n",
       "train_meta = f\"{target_dataset_path}/metadata_train.csv\"\n",
       "eval_meta = f\"{target_dataset_path}/metadata_eval.csv\"\n",
       "\n",
       "if os.path.exists(train_meta) and os.path.exists(eval_meta):\n",
       "    df_train = pd.read_csv(train_meta, sep='|', header=None, names=['audio_file', 'text', 'speaker_name'])\n",
       "    df_eval = pd.read_csv(eval_meta, sep='|', header=None, names=['audio_file', 'text', 'speaker_name'])\n",
       "    print(f\"\\n‚úÖ Training samples: {len(df_train)}\")\n",
       "    print(f\"‚úÖ Validation samples: {len(df_eval)}\")\n",
       "else:\n",
       "    print(\"‚ùå Error: Metadata files not created!\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 7: Download XTTS-v2 model files\n",
       "import os\n",
       "import requests\n",
       "from tqdm import tqdm\n",
       "\n",
       "# Create output directory\n",
       "output_dir = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
       "os.makedirs(output_dir, exist_ok=True)\n",
       "\n",
       "print(\"=\" * 80)\n",
       "print(\"DOWNLOADING XTTS-v2 MODEL FILES\")\n",
       "print(\"=\" * 80)\n",
       "\n",
       "# Use Coqui gateway for downloads\n",
       "base_url = \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/\"\n",
       "\n",
       "files_to_download = {\n",
       "    \"config.json\": f\"{base_url}config.json\",\n",
       "    \"vocab.json\": f\"{base_url}vocab.json\",\n",
       "    \"model.pth\": f\"{base_url}model.pth\",\n",
       "    \"dvae.pth\": f\"{base_url}dvae.pth\",\n",
       "    \"mel_stats.pth\": f\"{base_url}mel_stats.pth\",\n",
       "}\n",
       "\n",
       "def download_file(url, output_path):\n",
       "    '''Download file with progress bar'''\n",
       "    response = requests.get(url, stream=True)\n",
       "    total_size = int(response.headers.get('content-length', 0))\n",
       "    \n",
       "    with open(output_path, 'wb') as f:\n",
       "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=os.path.basename(output_path)) as pbar:\n",
       "            for chunk in response.iter_content(chunk_size=8192):\n",
       "                if chunk:\n",
       "                    f.write(chunk)\n",
       "                    pbar.update(len(chunk))\n",
       "\n",
       "# Download each file\n",
       "for filename, url in files_to_download.items():\n",
       "    output_path = os.path.join(output_dir, filename)\n",
       "    \n",
       "    if os.path.exists(output_path):\n",
       "        print(f\"‚úÖ {filename} already exists, skipping...\")\n",
       "    else:\n",
       "        print(f\"\\nüîπ Downloading {filename}...\")\n",
       "        try:\n",
       "            download_file(url, output_path)\n",
       "            print(f\"‚úÖ {filename} downloaded successfully\")\n",
       "        except Exception as e:\n",
       "            print(f\"‚ùå Failed to download {filename}: {e}\")\n",
       "\n",
       "# Verify all files downloaded\n",
       "print(f\"\\n{'=' * 80}\")\n",
       "print(\"VERIFICATION\")\n",
       "print(f\"{'=' * 80}\")\n",
       "\n",
       "all_downloaded = True\n",
       "for filename in files_to_download.keys():\n",
       "    filepath = os.path.join(output_dir, filename)\n",
       "    if os.path.exists(filepath):\n",
       "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
       "        print(f\"‚úÖ {filename}: {size_mb:.1f} MB\")\n",
       "    else:\n",
       "        print(f\"‚ùå {filename}: MISSING!\")\n",
       "        all_downloaded = False\n",
       "\n",
       "if all_downloaded:\n",
       "    print(f\"\\n{'=' * 80}\")\n",
       "    print(\"‚úÖ ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\")\n",
       "    print(f\"{'=' * 80}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 8: Extend vocabulary for Sinhala (CORRECTED - uses extend_vocab_sinhala.py)\n",
       "import os\n",
       "import json\n",
       "import subprocess\n",
       "import sys\n",
       "\n",
       "print(\"=\" * 80)\n",
       "print(\"EXTENDING VOCABULARY FOR SINHALA\")\n",
       "print(\"=\" * 80)\n",
       "\n",
       "# Correct paths\n",
       "vocab_script = \"extend_vocab_sinhala.py\"\n",
       "train_metadata_path = \"/kaggle/working/datasets/metadata_train.csv\"\n",
       "output_path = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
       "language_code = \"si\"\n",
       "vocab_size = 15000\n",
       "\n",
       "# Verify script exists\n",
       "if not os.path.exists(vocab_script):\n",
       "    print(f\"‚ùå Error: {vocab_script} not found!\")\n",
       "    print(\"Current directory:\", os.getcwd())\n",
       "    print(\"Files in current directory:\")\n",
       "    !ls -la\n",
       "else:\n",
       "    print(f\"‚úÖ Found {vocab_script}\")\n",
       "\n",
       "# Verify metadata exists\n",
       "if not os.path.exists(train_metadata_path):\n",
       "    print(f\"‚ùå Error: Training metadata not found at {train_metadata_path}\")\n",
       "else:\n",
       "    print(f\"‚úÖ Found training metadata: {train_metadata_path}\")\n",
       "\n",
       "# Run vocabulary extension\n",
       "print(f\"\\nüîπ Running vocabulary extension...\")\n",
       "print(f\"   Script: {vocab_script}\")\n",
       "print(f\"   Metadata: {train_metadata_path}\")\n",
       "print(f\"   Output: {output_path}\")\n",
       "print(f\"   Language: {language_code}\")\n",
       "print(f\"   Vocab size: {vocab_size}\")\n",
       "\n",
       "cmd = [\n",
       "    sys.executable,\n",
       "    vocab_script,\n",
       "    \"--metadata_path\", train_metadata_path,\n",
       "    \"--output_path\", output_path,  # CORRECT: Full path to XTTS_v2.0_original_model_files\n",
       "    \"--language\", language_code,\n",
       "    \"--vocab_size\", str(vocab_size)\n",
       "]\n",
       "\n",
       "print(f\"\\n[Running] Command: {' '.join(cmd)}\")\n",
       "\n",
       "try:\n",
       "    result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)\n",
       "    print(result.stdout)\n",
       "    \n",
       "    if result.returncode != 0:\n",
       "        print(f\"‚ùå Error:\")\n",
       "        print(result.stderr)\n",
       "        raise RuntimeError(\"Vocabulary extension failed\")\n",
       "    else:\n",
       "        print(f\"‚úÖ Vocabulary extension SUCCESSFUL\")\n",
       "        \n",
       "except subprocess.TimeoutExpired:\n",
       "    print(f\"‚ùå Timeout - vocabulary extension took too long\")\n",
       "    raise\n",
       "except Exception as e:\n",
       "    print(f\"‚ùå Exception: {e}\")\n",
       "    raise\n",
       "\n",
       "# Verify vocab.json was created/updated\n",
       "vocab_path = os.path.join(output_path, \"vocab.json\")\n",
       "if os.path.exists(vocab_path):\n",
       "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
       "        vocab = json.load(f)\n",
       "    print(f\"\\n‚úÖ Extended vocabulary size: {len(vocab)} tokens\")\n",
       "    \n",
       "    # Check for Sinhala characters in vocab\n",
       "    sinhala_tokens = [token for token in vocab.keys() if any('\\u0D80' <= char <= '\\u0DFF' for char in token)]\n",
       "    print(f\"‚úÖ Sinhala-specific tokens: {len(sinhala_tokens)}\")\n",
       "else:\n",
       "    print(f\"‚ùå Vocabulary file not found at {vocab_path}!\")\n",
       "\n",
       "# Verify config.json was updated\n",
       "config_path = os.path.join(output_path, \"config.json\")\n",
       "if os.path.exists(config_path):\n",
       "    with open(config_path, 'r', encoding='utf-8') as f:\n",
       "        config = json.load(f)\n",
       "    \n",
       "    if 'languages' in config and language_code in config['languages']:\n",
       "        print(f\"‚úÖ Language '{language_code}' added to config.json\")\n",
       "    else:\n",
       "        print(f\"‚ö†Ô∏è Warning: Language '{language_code}' not found in config.json\")\n",
       "\n",
       "print(f\"\\n{'=' * 80}\")\n",
       "print(\"‚úÖ VOCABULARY EXTENSION COMPLETE\")\n",
       "print(f\"{'=' * 80}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 9: (OPTIONAL) DVAE Fine-tuning\n",
       "import os\n",
       "import subprocess\n",
       "import sys\n",
       "\n",
       "print(\"=\" * 80)\n",
       "print(\"DVAE FINE-TUNING (OPTIONAL)\")\n",
       "print(\"=\" * 80)\n",
       "print(\"‚ö†Ô∏è Only run this if you have 20+ hours of training data\")\n",
       "print(\"‚ö†Ô∏è This step is optional and usually not needed\")\n",
       "print(\"=\" * 80)\n",
       "\n",
       "# Set to True to enable DVAE training, False to skip\n",
       "ENABLE_DVAE_TRAINING = False\n",
       "\n",
       "if ENABLE_DVAE_TRAINING:\n",
       "    print(\"\\nüîπ Starting DVAE fine-tuning...\")\n",
       "    \n",
       "    dvae_script = \"train_dvae_xtts.py\"\n",
       "    output_path = \"/kaggle/working/checkpoints\"\n",
       "    train_csv_path = \"/kaggle/working/datasets/metadata_train.csv\"\n",
       "    eval_csv_path = \"/kaggle/working/datasets/metadata_eval.csv\"\n",
       "    language = \"si\"\n",
       "    num_epochs = 3\n",
       "    batch_size = 256\n",
       "    lr = 5e-6\n",
       "    \n",
       "    # Verify script exists\n",
       "    if not os.path.exists(dvae_script):\n",
       "        print(f\"‚ùå Error: {dvae_script} not found!\")\n",
       "    else:\n",
       "        cmd = [\n",
       "            sys.executable,\n",
       "            dvae_script,\n",
       "            \"--output_path\", output_path,\n",
       "            \"--train_csv_path\", train_csv_path,\n",
       "            \"--eval_csv_path\", eval_csv_path,\n",
       "            \"--language\", language,\n",
       "            \"--num_epochs\", str(num_epochs),\n",
       "            \"--batch_size\", str(batch_size),\n",
       "            \"--lr\", str(lr)\n",
       "        ]\n",
       "        \n",
       "        print(f\"Command: {' '.join(cmd)}\")\n",
       "        print(\"\\n‚ö†Ô∏è This may take several hours...\")\n",
       "        \n",
       "        try:\n",
       "            result = subprocess.run(cmd, text=True)\n",
       "            if result.returncode == 0:\n",
       "                print(\"\\n‚úÖ DVAE fine-tuning completed successfully!\")\n",
       "            else:\n",
       "                print(f\"\\n‚ö†Ô∏è DVAE fine-tuning had errors (return code: {result.returncode})\")\n",
       "        except Exception as e:\n",
       "            print(f\"‚ùå Error during DVAE training: {e}\")\n",
       "else:\n",
       "    print(\"\\n‚ÑπÔ∏è DVAE fine-tuning skipped (recommended for most cases)\")\n",
       "    print(\"   Set ENABLE_DVAE_TRAINING = True to enable\")\n",
       "\n",
       "print(f\"\\n{'=' * 80}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 10: GPT Fine-tuning\n",
       "import os\n",
       "import subprocess\n",
       "import sys\n",
       "\n",
       "print(\"=\" * 80)\n",
       "print(\"STARTING GPT FINE-TUNING\")\n",
       "print(\"=\" * 80)\n",
       "\n",
       "# Configuration\n",
       "gpt_script = \"train_gpt_xtts.py\"\n",
       "output_path = \"/kaggle/working/checkpoints\"\n",
       "train_metadata = \"/kaggle/working/datasets/metadata_train.csv\"\n",
       "eval_metadata = \"/kaggle/working/datasets/metadata_eval.csv\"\n",
       "language = \"si\"\n",
       "\n",
       "# Training parameters\n",
       "num_epochs = 5\n",
       "batch_size = 8\n",
       "grad_acumm = 4\n",
       "max_text_length = 400\n",
       "max_audio_length = 330750\n",
       "weight_decay = 1e-2\n",
       "lr = 5e-6\n",
       "save_step = 50000\n",
       "\n",
       "# Verify script exists\n",
       "if not os.path.exists(gpt_script):\n",
       "    print(f\"‚ùå Error: {gpt_script} not found!\")\n",
       "    print(\"Current directory:\", os.getcwd())\n",
       "    !ls -la\n",
       "else:\n",
       "    print(f\"‚úÖ Found {gpt_script}\")\n",
       "\n",
       "# Verify metadata files exist\n",
       "if not os.path.exists(train_metadata):\n",
       "    print(f\"‚ùå Error: Training metadata not found: {train_metadata}\")\n",
       "elif not os.path.exists(eval_metadata):\n",
       "    print(f\"‚ùå Error: Evaluation metadata not found: {eval_metadata}\")\n",
       "else:\n",
       "    print(f\"‚úÖ Training metadata: {train_metadata}\")\n",
       "    print(f\"‚úÖ Evaluation metadata: {eval_metadata}\")\n",
       "\n",
       "# Construct metadata string\n",
       "metadata_string = f\"{train_metadata},{eval_metadata},{language}\"\n",
       "\n",
       "print(f\"\\nüìã Training Configuration:\")\n",
       "print(f\"   - Epochs: {num_epochs}\")\n",
       "print(f\"   - Batch size: {batch_size}\")\n",
       "print(f\"   - Gradient accumulation: {grad_acumm}\")\n",
       "print(f\"   - Effective batch size: {batch_size * grad_acumm}\")\n",
       "print(f\"   - Learning rate: {lr}\")\n",
       "print(f\"   - Max text length: {max_text_length}\")\n",
       "print(f\"   - Max audio length: {max_audio_length}\")\n",
       "print(f\"   - Save step: {save_step}\")\n",
       "\n",
       "# Run GPT training\n",
       "cmd = [\n",
       "    sys.executable,\n",
       "    gpt_script,\n",
       "    \"--output_path\", output_path,\n",
       "    \"--metadatas\", metadata_string,\n",
       "    \"--num_epochs\", str(num_epochs),\n",
       "    \"--batch_size\", str(batch_size),\n",
       "    \"--grad_acumm\", str(grad_acumm),\n",
       "    \"--max_text_length\", str(max_text_length),\n",
       "    \"--max_audio_length\", str(max_audio_length),\n",
       "    \"--weight_decay\", str(weight_decay),\n",
       "    \"--lr\", str(lr),\n",
       "    \"--save_step\", str(save_step)\n",
       "]\n",
       "\n",
       "print(f\"\\n[Running] Command: {' '.join(cmd)}\")\n",
       "print(f\"\\n‚ö†Ô∏è This may take several hours (4-8 hours depending on dataset size)...\")\n",
       "print(f\"‚ö†Ô∏è Make sure Kaggle notebook is set to GPU and has enough time...\")\n",
       "\n",
       "try:\n",
       "    result = subprocess.run(cmd, text=True)\n",
       "    \n",
       "    if result.returncode == 0:\n",
       "        print(f\"\\n{'=' * 80}\")\n",
       "        print(\"‚úÖ GPT TRAINING COMPLETED SUCCESSFULLY!\")\n",
       "        print(f\"{'=' * 80}\")\n",
       "        \n",
       "        # Check for checkpoint\n",
       "        checkpoint_dir = os.path.join(output_path, \"run\", \"training\")\n",
       "        if os.path.exists(checkpoint_dir):\n",
       "            checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth')]\n",
       "            if checkpoints:\n",
       "                print(f\"\\n‚úÖ Found {len(checkpoints)} checkpoint(s):\")\n",
       "                for cp in checkpoints:\n",
       "                    cp_path = os.path.join(checkpoint_dir, cp)\n",
       "                    size_mb = os.path.getsize(cp_path) / (1024 * 1024)\n",
       "                    print(f\"   - {cp} ({size_mb:.1f} MB)\")\n",
       "    else:\n",
       "        print(f\"\\n‚ùå Training failed with return code: {result.returncode}\")\n",
       "except KeyboardInterrupt:\n",
       "    print(f\"\\n‚ö†Ô∏è Training interrupted by user\")\n",
       "except Exception as e:\n",
       "    print(f\"\\n‚ùå Error during training: {e}\")\n",
       "    import traceback\n",
       "    traceback.print_exc()\n",
       "\n",
       "print(f\"\\n{'=' * 80}\")"
      ]
     },
     {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
       "# Cell 11: Summary and Next Steps\n",
       "import os\n",
       "\n",
       "print(\"\\n\" + \"#\" * 80)\n",
       "print(\"#\" + \" \" * 78 + \"#\")\n",
       "print(\"#\" + \" \" * 15 + \"‚úÖ SINHALA XTTS-v2 FINE-TUNING COMPLETE!\" + \" \" * 26 + \"#\")\n",
       "print(\"#\" + \" \" * 78 + \"#\")\n",
       "print(\"#\" * 80)\n",
       "\n",
       "print(\"\\n‚úÖ COMPLETED PHASES:\")\n",
       "phases = [\n",
       "    \"Environment Setup\",\n",
       "    \"Clone Repository\",\n",
       "    \"Download Dataset\",\n",
       "    \"Download XTTS-v2 Model\",\n",
       "    \"Prepare Dataset\",\n",
       "    \"Extend Vocabulary (15,000 Sinhala tokens)\",\n",
       "    \"GPT Fine-tuning\"\n",
       "]\n",
       "\n",
       "for i, phase in enumerate(phases, 1):\n",
       "    print(f\"   {i}. ‚úÖ {phase}\")\n",
       "\n",
       "print(\"\\nüìä MODEL SPECIFICATIONS:\")\n",
       "print(f\"   Language: Sinhala (‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω)\")\n",
       "print(f\"   Language Code: si\")\n",
       "print(f\"   Tokenization: ByteLevel BPE\")\n",
       "print(f\"   Vocabulary: 15,000 tokens\")\n",
       "\n",
       "print(\"\\nüìÅ OUTPUT FILES:\")\n",
       "checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
       "model_dir = os.path.join(checkpoint_dir, \"XTTS_v2.0_original_model_files\")\n",
       "training_dir = os.path.join(checkpoint_dir, \"run\", \"training\")\n",
       "\n",
       "print(f\"   Model Files: {model_dir}\")\n",
       "print(f\"   - vocab.json: Extended vocabulary\")\n",
       "print(f\"   - config.json: Updated with Sinhala language\")\n",
       "print(f\"   Training Checkpoints: {training_dir}\")\n",
       "\n",
       "if os.path.exists(training_dir):\n",
       "    checkpoints = [f for f in os.listdir(training_dir) if f.endswith('.pth')]\n",
       "    if checkpoints:\n",
       "        print(f\"   - Found {len(checkpoints)} checkpoint(s)\")\n",
       "\n",
       "print(\"\\nüéâ NEXT STEPS:\")\n",
       "print(f\"   1. Download checkpoints from Kaggle\")\n",
       "print(f\"   2. Use trained model for Sinhala text-to-speech inference\")\n",
       "print(f\"   3. Test with Sinhala test texts\")\n",
       "\n",
       "print(\"\\n\" + \"#\" * 80)\n",
       "print(\"#\" * 80)"
      ]
     }
    ],
    "metadata": {
     "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     },
     "language_info": {
      "name": "python",
      "version": "3.11.0"
     }
    },
    "nbformat": 4,
    "nbformat_minor": 4
   }