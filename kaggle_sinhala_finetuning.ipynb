{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# XTTS-v2 Sinhala Fine-tuning on Kaggle\n",
        "\n",
        "This notebook fine-tunes XTTS-v2 for Sinhala language using the complete pipeline.\n",
        "\n",
        "**Steps:**\n",
        "1. Environment setup (PyTorch, TTS, dependencies)\n",
        "2. Clone repository\n",
        "3. Download dataset\n",
        "4. Download XTTS-v2 base model\n",
        "5. Prepare dataset\n",
        "6. Extend vocabulary for Sinhala\n",
        "7. Fine-tune GPT model\n",
        "8. Test inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 1: Install PyTorch with CUDA support\n",
        "# ============================================================================\n",
        "\n",
        "!pip install torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Verify\n",
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 2: Set environment variables and verify setup\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# ‚ö†Ô∏è CRITICAL: Set these BEFORE any TTS imports\n",
        "os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT'] = '1'\n",
        "os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION'] = '1'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "print(\"‚úÖ Environment variables set\")\n",
        "print(f\"TRANSFORMERS_NO_TORCHAO_IMPORT = {os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT']}\")\n",
        "print(f\"TORCH_ALLOW_UNSAFE_DESERIALIZATION = {os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION']}\")\n",
        "print(f\"PYTORCH_CUDA_ALLOC_CONF = {os.environ['PYTORCH_CUDA_ALLOC_CONF']}\")\n",
        "\n",
        "# Check Python version\n",
        "print(f\"\\nPython version: {sys.version}\")\n",
        "print(f\"Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Check GPU\n",
        "import torch\n",
        "print(f\"\\nCUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 3: Install TTS and all dependencies\n",
        "# ============================================================================\n",
        "\n",
        "# Install TTS and related packages\n",
        "!pip install -q TTS==0.22.0\n",
        "\n",
        "# ‚ö†Ô∏è CRITICAL FIX: Use transformers 4.36.0 instead of 4.45.2\n",
        "!pip install -q transformers==4.36.0 tokenizers==0.15.0\n",
        "\n",
        "!pip install -q librosa==0.10.2 soundfile==0.12.1 scipy==1.11.2 pysbd==0.3.4\n",
        "!pip install -q pandas==1.5.3 scikit-learn==1.3.2 tqdm==4.66.3\n",
        "!pip install -q einops==0.7.0 unidecode==1.3.8 inflect==7.0.0\n",
        "!pip install -q coqpit==0.0.16 trainer==0.0.36 mutagen\n",
        "!pip install -q pypinyin hangul_romanize num2words kagglehub\n",
        "!pip install -q requests\n",
        "\n",
        "print(\"‚úÖ All dependencies installed successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 4: Verify critical packages\n",
        "# ============================================================================\n",
        "\n",
        "import trainer\n",
        "import TTS\n",
        "import transformers\n",
        "import librosa\n",
        "import tokenizers\n",
        "\n",
        "print(f\"trainer version: {trainer.__version__}\")\n",
        "print(f\"TTS installed: {TTS.__version__}\")\n",
        "print(f\"transformers version: {transformers.__version__}\")\n",
        "print(f\"tokenizers version: {tokenizers.__version__}\")\n",
        "print(f\"librosa version: {librosa.__version__}\")\n",
        "print(\"‚úÖ All packages verified!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 5: Clone repository\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "\n",
        "repo_url = \"https://github.com/amalshafernando/XTTSv2-sinhala.git\"\n",
        "repo_name = \"XTTSv2-sinhala\"\n",
        "\n",
        "# Clone only if it doesn't exist\n",
        "if not os.path.exists(repo_name):\n",
        "    print(f\"üîπ Cloning {repo_name}...\")\n",
        "    !git clone {repo_url}\n",
        "    print(\"‚úÖ Repository cloned\")\n",
        "else:\n",
        "    print(f\"‚úÖ Repository already exists: {repo_name}\")\n",
        "\n",
        "# Change to repo directory\n",
        "os.chdir(repo_name)\n",
        "print(f\"‚úÖ Current directory: {os.getcwd()}\")\n",
        "\n",
        "# List contents\n",
        "print(\"\\nüîπ Repository contents:\")\n",
        "!ls -la | head -20\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 6: Download Sinhala TTS dataset\n",
        "# ============================================================================\n",
        "\n",
        "import kagglehub\n",
        "import os\n",
        "\n",
        "# Download dataset\n",
        "path = kagglehub.dataset_download(\"amalshaf/sinhala-tts-dataset\")\n",
        "print(f\"‚úÖ Dataset downloaded to: {path}\")\n",
        "\n",
        "# Setup paths\n",
        "kaggle_dataset_path = f\"{path}/sinhala-tts-dataset\"\n",
        "print(f\"üìÅ Kaggle dataset path: {kaggle_dataset_path}\")\n",
        "\n",
        "# Verify dataset structure\n",
        "if os.path.exists(kaggle_dataset_path):\n",
        "    print(f\"\\nüìÇ Dataset contents:\")\n",
        "    !ls -lh {kaggle_dataset_path}\n",
        "    \n",
        "    # Check for metadata files\n",
        "    metadata_train = f\"{kaggle_dataset_path}/metadata_train.csv\"\n",
        "    metadata_eval = f\"{kaggle_dataset_path}/metadata_eval.csv\"\n",
        "    \n",
        "    if os.path.exists(metadata_train):\n",
        "        print(f\"\\n‚úÖ Found: metadata_train.csv\")\n",
        "    if os.path.exists(metadata_eval):\n",
        "        print(f\"‚úÖ Found: metadata_eval.csv\")\n",
        "    \n",
        "    # Check for audio directory\n",
        "    audio_dirs = [\"wav\", \"wavs\", \"audio\", \"audio_files\"]\n",
        "    for audio_dir in audio_dirs:\n",
        "        audio_path = os.path.join(kaggle_dataset_path, audio_dir)\n",
        "        if os.path.exists(audio_path):\n",
        "            print(f\"‚úÖ Found audio directory: {audio_dir}\")\n",
        "            break\n",
        "else:\n",
        "    print(f\"‚ùå Dataset path not found: {kaggle_dataset_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 7: Prepare dataset using prepare_dataset_sinhala.py\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PREPARING DATASET FOR XTTS-v2\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get dataset path from previous cell\n",
        "kaggle_dataset_path = \"/kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\"\n",
        "output_dataset_path = \"/kaggle/working/datasets\"\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(output_dataset_path, exist_ok=True)\n",
        "\n",
        "# Run dataset preparation script\n",
        "!python prepare_dataset_sinhala.py \\\n",
        "    --kaggle_path {kaggle_dataset_path} \\\n",
        "    --output_path {output_dataset_path}\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ DATASET PREPARATION COMPLETED\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verify output files\n",
        "train_metadata = f\"{output_dataset_path}/metadata_train.csv\"\n",
        "eval_metadata = f\"{output_dataset_path}/metadata_eval.csv\"\n",
        "\n",
        "if os.path.exists(train_metadata):\n",
        "    import pandas as pd\n",
        "    df_train = pd.read_csv(train_metadata, sep='|', header=None)\n",
        "    print(f\"\\n‚úÖ Training samples: {len(df_train)}\")\n",
        "    \n",
        "if os.path.exists(eval_metadata):\n",
        "    df_eval = pd.read_csv(eval_metadata, sep='|', header=None)\n",
        "    print(f\"‚úÖ Evaluation samples: {len(df_eval)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 8: Download XTTS-v2 base model files\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Create output directory\n",
        "output_dir = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"DOWNLOADING XTTS-v2 MODEL FILES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define all required files from Hugging Face\n",
        "base_url = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/\"\n",
        "\n",
        "files_to_download = {\n",
        "    \"config.json\": f\"{base_url}config.json\",\n",
        "    \"vocab.json\": f\"{base_url}vocab.json\",\n",
        "    \"model.pth\": f\"{base_url}model.pth\",\n",
        "    \"dvae.pth\": f\"{base_url}dvae.pth\",\n",
        "    \"mel_stats.pth\": f\"{base_url}mel_stats.pth\",\n",
        "}\n",
        "\n",
        "def download_file(url, output_path):\n",
        "    '''Download file with progress bar'''\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    \n",
        "    with open(output_path, 'wb') as f:\n",
        "        with tqdm(total=total_size, unit='B', unit_scale=True, desc=os.path.basename(output_path)) as pbar:\n",
        "            for chunk in response.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "\n",
        "# Download each file\n",
        "for filename, url in files_to_download.items():\n",
        "    output_path = os.path.join(output_dir, filename)\n",
        "    \n",
        "    if os.path.exists(output_path):\n",
        "        size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
        "        print(f\"‚úÖ {filename} already exists ({size_mb:.1f} MB), skipping...\")\n",
        "    else:\n",
        "        print(f\"\\nüîπ Downloading {filename}...\")\n",
        "        try:\n",
        "            download_file(url, output_path)\n",
        "            print(f\"‚úÖ {filename} downloaded successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to download {filename}: {e}\")\n",
        "\n",
        "# Verify all files downloaded\n",
        "print(f\"\\n{'=' * 80}\")\n",
        "print(\"VERIFICATION\")\n",
        "print(f\"{'=' * 80}\")\n",
        "\n",
        "all_downloaded = True\n",
        "for filename in files_to_download.keys():\n",
        "    filepath = os.path.join(output_dir, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
        "        print(f\"‚úÖ {filename}: {size_mb:.1f} MB\")\n",
        "    else:\n",
        "        print(f\"‚ùå {filename}: MISSING!\")\n",
        "        all_downloaded = False\n",
        "\n",
        "if all_downloaded:\n",
        "    print(f\"\\n{'=' * 80}\")\n",
        "    print(\"‚úÖ ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\")\n",
        "    print(f\"{'=' * 80}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 9: Extend vocabulary for Sinhala language\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"EXTENDING VOCABULARY FOR SINHALA\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Paths\n",
        "metadata_path = \"/kaggle/working/datasets/metadata_train.csv\"\n",
        "output_path = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\n",
        "\n",
        "# Verify paths exist\n",
        "if not os.path.exists(metadata_path):\n",
        "    print(f\"‚ùå Error: Metadata file not found: {metadata_path}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Metadata file found: {metadata_path}\")\n",
        "\n",
        "if not os.path.exists(output_path):\n",
        "    print(f\"‚ùå Error: Output path not found: {output_path}\")\n",
        "else:\n",
        "    print(f\"‚úÖ Output path exists: {output_path}\")\n",
        "\n",
        "# Run extend_vocab_sinhala.py\n",
        "print(f\"\\nüîπ Running extend_vocab_sinhala.py...\")\n",
        "!python extend_vocab_sinhala.py \\\n",
        "    --metadata_path {metadata_path} \\\n",
        "    --output_path {output_path} \\\n",
        "    --language si \\\n",
        "    --vocab_size 15000\n",
        "\n",
        "print(\"\\n‚úÖ Vocabulary extension completed!\")\n",
        "\n",
        "# Verify the extended vocab\n",
        "vocab_path = os.path.join(output_path, \"vocab.json\")\n",
        "if os.path.exists(vocab_path):\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "        vocab = json.load(f)\n",
        "    print(f\"\\n‚úÖ Extended vocabulary size: {len(vocab):,} tokens\")\n",
        "    \n",
        "    # Check for Sinhala characters in vocab\n",
        "    sinhala_tokens = [token for token in vocab.keys() if any('\\u0D80' <= char <= '\\u0DFF' for char in token)]\n",
        "    print(f\"‚úÖ Sinhala-specific tokens: {len(sinhala_tokens)}\")\n",
        "    \n",
        "    # Verify config.json was updated\n",
        "    config_path = os.path.join(output_path, \"config.json\")\n",
        "    if os.path.exists(config_path):\n",
        "        with open(config_path, 'r', encoding='utf-8') as f:\n",
        "            config = json.load(f)\n",
        "        if 'language_ids' in config and 'si' in config['language_ids']:\n",
        "            print(f\"‚úÖ Sinhala language (si) added to config.json\")\n",
        "            print(f\"   Language ID: {config['language_ids']['si']}\")\n",
        "else:\n",
        "    print(f\"\\n‚ùå Vocabulary file not found at: {vocab_path}\")\n",
        "    \n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 10: Run complete training pipeline using kaggle_train_sinhala.py\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import sys\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STARTING COMPLETE TRAINING PIPELINE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Verify we're in the repo directory\n",
        "if not os.path.exists(\"kaggle_train_sinhala.py\"):\n",
        "    print(\"‚ùå Error: kaggle_train_sinhala.py not found in current directory\")\n",
        "    print(f\"Current directory: {os.getcwd()}\")\n",
        "    print(\"\\nTrying to find it...\")\n",
        "    !find . -name \"kaggle_train_sinhala.py\" -type f\n",
        "else:\n",
        "    print(\"‚úÖ Found kaggle_train_sinhala.py\")\n",
        "    \n",
        "    # Run the complete training pipeline\n",
        "    print(\"\\nüöÄ Starting training pipeline...\")\n",
        "    print(\"This will run all phases:\")\n",
        "    print(\"  1. Setup verification\")\n",
        "    print(\"  2. Dataset preparation\")\n",
        "    print(\"  3. Model download\")\n",
        "    print(\"  4. Vocabulary extension\")\n",
        "    print(\"  5. GPT fine-tuning\")\n",
        "    print(\"\\n‚ö†Ô∏è This may take several hours...\")\n",
        "    \n",
        "    !python kaggle_train_sinhala.py\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚úÖ TRAINING PIPELINE COMPLETED!\")\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 11: Alternative - Run GPT training directly (if pipeline fails)\n",
        "# ============================================================================\n",
        "\n",
        "# Uncomment this cell only if kaggle_train_sinhala.py fails\n",
        "# This runs GPT training directly with all required parameters\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ALTERNATIVE: DIRECT GPT TRAINING\")\n",
        "print(\"=\" * 80)\n",
        "print(\"‚ö†Ô∏è Only use this if the pipeline in Cell 10 failed\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Uncomment below to run directly:\n",
        "\"\"\"\n",
        "!CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n",
        "    --output_path /kaggle/working/checkpoints/ \\\n",
        "    --metadatas /kaggle/working/datasets/metadata_train.csv,/kaggle/working/datasets/metadata_eval.csv,si \\\n",
        "    --num_epochs 5 \\\n",
        "    --batch_size 8 \\\n",
        "    --grad_acumm 4 \\\n",
        "    --max_text_length 400 \\\n",
        "    --max_audio_length 330750 \\\n",
        "    --weight_decay 1e-2 \\\n",
        "    --lr 5e-6 \\\n",
        "    --save_step 50000\n",
        "\n",
        "print(\"\\n‚úÖ GPT training completed!\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"‚ÑπÔ∏è This cell is commented out. Uncomment to use if needed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 12: Verify training output and find best model\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"VERIFYING TRAINING OUTPUT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Search for trained models\n",
        "checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
        "\n",
        "# Look for GPT_XTTS_FT directories\n",
        "model_dirs = glob.glob(f\"{checkpoint_dir}/GPT_XTTS_FT*\")\n",
        "\n",
        "if model_dirs:\n",
        "    print(f\"\\n‚úÖ Found {len(model_dirs)} training output(s):\")\n",
        "    for model_dir in model_dirs:\n",
        "        print(f\"\\nüìÅ {model_dir}\")\n",
        "        \n",
        "        # Look for best_model.pth\n",
        "        best_model = os.path.join(model_dir, \"best_model.pth\")\n",
        "        if os.path.exists(best_model):\n",
        "            size_mb = os.path.getsize(best_model) / (1024 * 1024)\n",
        "            print(f\"   ‚úÖ best_model.pth ({size_mb:.1f} MB)\")\n",
        "        \n",
        "        # Look for config.json\n",
        "        config_file = os.path.join(model_dir, \"config.json\")\n",
        "        if os.path.exists(config_file):\n",
        "            print(f\"   ‚úÖ config.json\")\n",
        "        \n",
        "        # List all files\n",
        "        files = os.listdir(model_dir)\n",
        "        print(f\"   üìÑ Total files: {len(files)}\")\n",
        "        if len(files) <= 10:\n",
        "            for f in files:\n",
        "                print(f\"      - {f}\")\n",
        "else:\n",
        "    print(f\"\\n‚ö†Ô∏è No training output found in {checkpoint_dir}\")\n",
        "    print(\"   Training may still be in progress or may have failed\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 13: Test inference with trained model\n",
        "# ============================================================================\n",
        "\n",
        "import os\n",
        "import glob\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTING INFERENCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Find the best model\n",
        "checkpoint_dir = \"/kaggle/working/checkpoints\"\n",
        "model_dirs = glob.glob(f\"{checkpoint_dir}/GPT_XTTS_FT*\")\n",
        "\n",
        "if not model_dirs:\n",
        "    print(\"‚ùå No trained model found. Please complete training first.\")\n",
        "else:\n",
        "    model_dir = model_dirs[0]  # Use the first one\n",
        "    best_model = os.path.join(model_dir, \"best_model.pth\")\n",
        "    config_file = os.path.join(model_dir, \"config.json\")\n",
        "    vocab_file = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\"\n",
        "    \n",
        "    if not os.path.exists(best_model):\n",
        "        print(f\"‚ùå Best model not found: {best_model}\")\n",
        "    elif not os.path.exists(config_file):\n",
        "        print(f\"‚ùå Config file not found: {config_file}\")\n",
        "    elif not os.path.exists(vocab_file):\n",
        "        print(f\"‚ùå Vocab file not found: {vocab_file}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Found trained model: {model_dir}\")\n",
        "        print(f\"\\nüìù Test Sinhala texts:\")\n",
        "        test_texts = [\n",
        "            \"‡∂±‡∑í‡∂ª‡∂±‡∑ä‡∂≠‡∂ª‡∂∫‡∑í ‡∂â‡∂≠‡∑è ‡∑Ä‡∑ê‡∂Ø‡∂ú‡∂≠‡∑ä\",  # \"Always very important\"\n",
        "            \"‡∑Å‡∑ä‚Äç‡∂ª‡∑ì ‡∂Ω‡∂Ç‡∂ö‡∑è ‡∂î‡∂∂‡∑ö ‡∂ã‡∂≠‡∑î‡∂ª‡∑î‡∂Ø‡∑ô‡∑É‡∑í‡∂±‡∑ä\",  # \"Sri Lanka from your north\"\n",
        "            \"‡∑É‡∑í‡∂Ç‡∑Ñ‡∂Ω ‡∂∑‡∑è‡∑Ç‡∑è‡∑Ä ‡∂Ö‡∂¥‡∂ú‡∑ö ‡∂¢‡∑è‡∂≠‡∑í‡∂ö ‡∂∑‡∑è‡∑Ç‡∑è‡∑Ä‡∂∫‡∑í\",  # \"Sinhala is our national language\"\n",
        "        ]\n",
        "        \n",
        "        for i, text in enumerate(test_texts, 1):\n",
        "            print(f\"   {i}. {text}\")\n",
        "        \n",
        "        print(f\"\\nüîπ To test inference, use:\")\n",
        "        print(f\"   python inference_sinhala.py \\\\\")\n",
        "        print(f\"     --checkpoint_path {best_model} \\\\\")\n",
        "        print(f\"     --config_path {config_file} \\\\\")\n",
        "        print(f\"     --vocab_path {vocab_file} \\\\\")\n",
        "        print(f\"     --text \\\"‡∂±‡∑í‡∂ª‡∂±‡∑ä‡∂≠‡∂ª‡∂∫‡∑í ‡∂â‡∂≠‡∑è ‡∑Ä‡∑ê‡∂Ø‡∂ú‡∂≠‡∑ä\\\" \\\\\")\n",
        "        print(f\"     --reference_audio <path_to_reference_audio.wav> \\\\\")\n",
        "        print(f\"     --output_path output.wav\")\n",
        "        \n",
        "        # Try to find a reference audio file\n",
        "        reference_audio = \"/kaggle/working/datasets/wavs\"\n",
        "        if os.path.exists(reference_audio):\n",
        "            audio_files = [f for f in os.listdir(reference_audio) if f.endswith('.wav')]\n",
        "            if audio_files:\n",
        "                ref_audio_path = os.path.join(reference_audio, audio_files[0])\n",
        "                print(f\"\\n‚úÖ Found reference audio: {ref_audio_path}\")\n",
        "                print(f\"\\nüîπ Running inference test...\")\n",
        "                \n",
        "                # Run inference\n",
        "                output_audio = \"/kaggle/working/test_output.wav\"\n",
        "                !python inference_sinhala.py \\\n",
        "                    --checkpoint_path {best_model} \\\n",
        "                    --config_path {config_file} \\\n",
        "                    --vocab_path {vocab_file} \\\n",
        "                    --text \"‡∂±‡∑í‡∂ª‡∂±‡∑ä‡∂≠‡∂ª‡∂∫‡∑í ‡∂â‡∂≠‡∑è ‡∑Ä‡∑ê‡∂Ø‡∂ú‡∂≠‡∑ä\" \\\n",
        "                    --reference_audio {ref_audio_path} \\\n",
        "                    --output_path {output_audio}\n",
        "                \n",
        "                if os.path.exists(output_audio):\n",
        "                    size_mb = os.path.getsize(output_audio) / (1024 * 1024)\n",
        "                    print(f\"\\n‚úÖ Inference successful!\")\n",
        "                    print(f\"   Output: {output_audio} ({size_mb:.2f} MB)\")\n",
        "                else:\n",
        "                    print(f\"\\n‚ö†Ô∏è Inference may have failed - output file not found\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# CELL 14: Summary and next steps\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TRAINING SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n‚úÖ Training pipeline completed!\")\n",
        "print(\"\\nüìÅ Output locations:\")\n",
        "print(\"   - Trained model: /kaggle/working/checkpoints/GPT_XTTS_FT-*/\")\n",
        "print(\"   - Base model files: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/\")\n",
        "print(\"   - Dataset: /kaggle/working/datasets/\")\n",
        "\n",
        "print(\"\\nüöÄ To use the trained model:\")\n",
        "print(\"   1. Download the checkpoint directory from Kaggle\")\n",
        "print(\"   2. Use inference_sinhala.py to generate Sinhala speech\")\n",
        "print(\"   3. Provide any Sinhala text and a reference audio file\")\n",
        "\n",
        "print(\"\\nüìù Example inference command:\")\n",
        "print(\"   python inference_sinhala.py \\\\\")\n",
        "print(\"     --checkpoint_path checkpoints/GPT_XTTS_FT-*/best_model.pth \\\\\")\n",
        "print(\"     --config_path checkpoints/GPT_XTTS_FT-*/config.json \\\\\")\n",
        "print(\"     --vocab_path checkpoints/XTTS_v2.0_original_model_files/vocab.json \\\\\")\n",
        "print(\"     --text \\\"‡∂±‡∑í‡∂ª‡∂±‡∑ä‡∂≠‡∂ª‡∂∫‡∑í ‡∂â‡∂≠‡∑è ‡∑Ä‡∑ê‡∂Ø‡∂ú‡∂≠‡∑ä\\\" \\\\\")\n",
        "print(\"     --reference_audio reference.wav \\\\\")\n",
        "print(\"     --output_path output.wav\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ ALL DONE!\")\n",
        "print(\"=\" * 80)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
