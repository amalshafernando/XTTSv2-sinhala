{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":13465111,"sourceType":"datasetVersion","datasetId":8547335}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch==2.1.0 torchaudio==2.1.0 --index-url https://download.pytorch.org/whl/cu118\n\n# Verify\nimport torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"PyTorch version: {torch.__version__}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:20:24.563986Z","iopub.execute_input":"2025-10-29T06:20:24.564219Z","iopub.status.idle":"2025-10-29T06:22:19.496870Z","shell.execute_reply.started":"2025-10-29T06:20:24.564196Z","shell.execute_reply":"2025-10-29T06:22:19.495985Z"}},"outputs":[{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nCollecting torch==2.1.0\n  Downloading https://download.pytorch.org/whl/cu118/torch-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (2325.9 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m406.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.1.0\n  Downloading https://download.pytorch.org/whl/cu118/torchaudio-2.1.0%2Bcu118-cp311-cp311-linux_x86_64.whl (3.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.19.1)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (4.15.0)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.1.0) (2025.9.0)\nCollecting triton==2.1.0 (from torch==2.1.0)\n  Downloading https://download.pytorch.org/whl/triton-2.1.0-0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.1.0) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->torch==2.1.0) (1.3.0)\nInstalling collected packages: triton, torch, torchaudio\n  Attempting uninstall: triton\n    Found existing installation: triton 3.2.0\n    Uninstalling triton-3.2.0:\n      Successfully uninstalled triton-3.2.0\n  Attempting uninstall: torch\n    Found existing installation: torch 2.6.0+cu124\n    Uninstalling torch-2.6.0+cu124:\n      Successfully uninstalled torch-2.6.0+cu124\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.6.0+cu124\n    Uninstalling torchaudio-2.6.0+cu124:\n      Successfully uninstalled torchaudio-2.6.0+cu124\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed torch-2.1.0+cu118 torchaudio-2.1.0+cu118 triton-2.1.0\nCUDA available: True\nPyTorch version: 2.1.0+cu118\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport sys\n\n# âš ï¸ CRITICAL: Set these BEFORE any TTS imports\nos.environ['TRANSFORMERS_NO_TORCHAO_IMPORT'] = '1'\nos.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION'] = '1'\n\nprint(\"âœ… Environment variables set\")\nprint(f\"TRANSFORMERS_NO_TORCHAO_IMPORT = {os.environ['TRANSFORMERS_NO_TORCHAO_IMPORT']}\")\nprint(f\"TORCH_ALLOW_UNSAFE_DESERIALIZATION = {os.environ['TORCH_ALLOW_UNSAFE_DESERIALIZATION']}\")\n\n# Check Python version\nprint(f\"\\nPython version: {sys.version}\")\nprint(f\"Working directory: {os.getcwd()}\")\n\n# Enable GPU\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Check GPU\nimport torch\nprint(f\"\\nCUDA available: {torch.cuda.is_available()}\")\nif torch.cuda.is_available():\n    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:22:19.497769Z","iopub.execute_input":"2025-10-29T06:22:19.498291Z","iopub.status.idle":"2025-10-29T06:22:19.525665Z","shell.execute_reply.started":"2025-10-29T06:22:19.498261Z","shell.execute_reply":"2025-10-29T06:22:19.524925Z"}},"outputs":[{"name":"stdout","text":"âœ… Environment variables set\nTRANSFORMERS_NO_TORCHAO_IMPORT = 1\nTORCH_ALLOW_UNSAFE_DESERIALIZATION = 1\n\nPython version: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\nWorking directory: /kaggle/working\n\nCUDA available: True\nGPU: Tesla P100-PCIE-16GB\nGPU Memory: 17.1 GB\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install TTS and related packages\n!pip install -q TTS==0.22.0\n\n# âš ï¸ CRITICAL FIX: Use transformers 4.36.0 instead of 4.45.2\n!pip install -q transformers==4.36.0 tokenizers==0.15.0\n\n!pip install -q librosa==0.10.2 soundfile==0.12.1 scipy==1.11.2 pysbd==0.3.4\n!pip install -q pandas==1.5.3 scikit-learn==1.3.2 tqdm==4.66.3\n!pip install -q einops==0.7.0 unidecode==1.3.8 inflect==7.0.0\n!pip install -q coqpit==0.0.16 trainer==0.0.36 mutagen\n!pip install -q pypinyin hangul_romanize num2words kagglehub\n\nprint(\"âœ… All dependencies installed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:22:19.527546Z","iopub.execute_input":"2025-10-29T06:22:19.528044Z","iopub.status.idle":"2025-10-29T06:24:10.370012Z","shell.execute_reply.started":"2025-10-29T06:22:19.528025Z","shell.execute_reply":"2025-10-29T06:24:10.368973Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m73.5/73.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m60.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m938.0/938.0 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m345.1/345.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.1/71.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.2/51.2 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m840.2/840.2 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m293.8/293.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.2/72.2 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m94.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Building wheel for gruut (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for encodec (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for bnnumerizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut-ipa (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_de (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_en (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_es (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for gruut_lang_fr (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nvisions 0.8.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.3 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nscikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0+cu118 which is incompatible.\nxarray 2025.7.1 requires pandas>=2.2, but you have pandas 1.5.3 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\nnx-cugraph-cu12 25.6.0 requires networkx>=3.2, but you have networkx 2.8.8 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m59.1/59.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m260.0/260.0 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\nscikit-image 0.25.2 requires networkx>=3.0, but you have networkx 2.8.8 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.11.2 which is incompatible.\nmizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nplotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.6/57.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nwoodwork 0.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\nfeaturetools 1.31.0 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npyldavis 3.4.1 requires pandas>=2.0.0, but you have pandas 1.5.3 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.11.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\numap-learn 0.5.9.post2 requires scikit-learn>=1.6, but you have scikit-learn 1.3.2 which is incompatible.\ndataproc-spark-connect 0.8.3 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.8.3 requires tqdm>=4.67, but you have tqdm 4.66.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.4/194.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hâœ… All dependencies installed successfully!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Verify critical packages\nimport trainer\nimport TTS\nimport transformers\nimport librosa\n\nprint(f\"trainer version: {trainer.__version__}\")\nprint(f\"TTS installed: {TTS.__version__}\")\nprint(f\"transformers version: {transformers.__version__}\")\nprint(f\"librosa version: {librosa.__version__}\")\nprint(\"âœ… All packages verified!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:10.373505Z","iopub.execute_input":"2025-10-29T06:24:10.373786Z","iopub.status.idle":"2025-10-29T06:24:12.481163Z","shell.execute_reply.started":"2025-10-29T06:24:10.373756Z","shell.execute_reply":"2025-10-29T06:24:12.480374Z"}},"outputs":[{"name":"stdout","text":"trainer version: v0.0.36\nTTS installed: 0.22.0\ntransformers version: 4.36.0\nlibrosa version: 0.10.2\nâœ… All packages verified!\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import os\n\nrepo_url = \"https://github.com/amalshafernando/XTTSv2-Finetuning-for-New-Languages.git\"\nrepo_name = \"XTTSv2-Finetuning-for-New-Languages\"\n\n# Clone only if it doesn't exist\nif not os.path.exists(repo_name):\n    print(f\"ğŸ”¹ Cloning {repo_name}...\")\n    !git clone {repo_url}\n    print(\"âœ… Repository cloned\")\nelse:\n    print(f\"âœ… Repository already exists: {repo_name}\")\n\n# Change to repo directory\nos.chdir(repo_name)\nprint(f\"âœ… Current directory: {os.getcwd()}\")\n\n# List contents\nprint(\"\\nğŸ”¹ Repository contents:\")\n!ls -la\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:12.481989Z","iopub.execute_input":"2025-10-29T06:24:12.482648Z","iopub.status.idle":"2025-10-29T06:24:13.657442Z","shell.execute_reply.started":"2025-10-29T06:24:12.482626Z","shell.execute_reply":"2025-10-29T06:24:13.656664Z"}},"outputs":[{"name":"stdout","text":"ğŸ”¹ Cloning XTTSv2-Finetuning-for-New-Languages...\nCloning into 'XTTSv2-Finetuning-for-New-Languages'...\nremote: Enumerating objects: 631, done.\u001b[K\nremote: Counting objects: 100% (320/320), done.\u001b[K\nremote: Compressing objects: 100% (224/224), done.\u001b[K\nremote: Total 631 (delta 131), reused 110 (delta 96), pack-reused 311 (from 2)\u001b[K\nReceiving objects: 100% (631/631), 2.08 MiB | 21.92 MiB/s, done.\nResolving deltas: 100% (155/155), done.\nâœ… Repository cloned\nâœ… Current directory: /kaggle/working/XTTSv2-Finetuning-for-New-Languages\n\nğŸ”¹ Repository contents:\ntotal 108\ndrwxr-xr-x  6 root root 4096 Oct 29 06:24  .\ndrwxr-xr-x  4 root root 4096 Oct 29 06:24  ..\n-rw-r--r--  1 root root    0 Oct 29 06:24 '=0.0.16'\n-rw-r--r--  1 root root 2461 Oct 29 06:24  download_checkpoint.py\n-rw-r--r--  1 root root 1905 Oct 29 06:24  download_model_manual.py\n-rw-r--r--  1 root root 3831 Oct 29 06:24  extend_vocab_config.py\ndrwxr-xr-x  8 root root 4096 Oct 29 06:24  .git\n-rw-r--r--  1 root root  141 Oct 29 06:24  .gitignore\n-rw-r--r--  1 root root 2821 Oct 29 06:24  kaggle_dataset_setup.py\n-rw-r--r--  1 root root 5320 Oct 29 06:24  kaggle_training.py\n-rw-r--r--  1 root root 1125 Oct 29 06:24  Readme2.md\n-rw-r--r--  1 root root 3199 Oct 29 06:24  README-3.md\n-rw-r--r--  1 root root 5711 Oct 29 06:24  Readme.md\ndrwxr-xr-x  9 root root 4096 Oct 29 06:24  recipes\n-rw-r--r--  1 root root  662 Oct 29 06:24  requirements.txt\n-rw-r--r--  1 root root 1038 Oct 29 06:24  requirements.txt.example\n-rw-r--r--  1 root root 1178 Oct 29 06:24  setup_kaggle.py\n-rw-r--r--  1 root root 7456 Oct 29 06:24  train_dvae_xtts.py\n-rwxr-xr-x  1 root root  235 Oct 29 06:24  train_dvae_xtts.sh\n-rw-r--r--  1 root root 9483 Oct 29 06:24  train_gpt_xtts.py\n-rwxr-xr-x  1 root root  381 Oct 29 06:24  train_gpt_xtts.sh\ndrwxr-xr-x 11 root root 4096 Oct 29 06:24  TTS\ndrwxr-xr-x  2 root root 4096 Oct 29 06:24  XTTS-v2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\nimport shutil\nimport kagglehub\nimport os\n\n# Download dataset\npath = kagglehub.dataset_download(\"amalshaf/sinhala-tts-dataset\")\nprint(f\"Dataset downloaded to: {path}\")\n\n# Setup paths\nkaggle_dataset_path = f\"{path}/sinhala-tts-dataset\"\nprint(f\"kaggle dataset path: {kaggle_dataset_path}\")\ntarget_dataset_path = \"/kaggle/working/datasets/\"\nprint(f\"target dataset path: {target_dataset_path}\")\n\n\n# Create target directory\nos.makedirs(f\"{target_dataset_path}/wavs\", exist_ok=True)\n\n# Copy audio files\nif os.path.exists(f\"{kaggle_dataset_path}/wavs\"):\n    shutil.copytree(f\"{kaggle_dataset_path}/wavs\", f\"{target_dataset_path}/wavs\", dirs_exist_ok=True)\n    print(f\"âœ… Copied audio files\")\n\n# Convert CSV to XTTS format\ndf_train = pd.read_csv(f\"{kaggle_dataset_path}/metadata_train.csv\", sep='|')\ndf_train_xtts = pd.DataFrame()\ndf_train_xtts['audio_file'] = df_train['audio_file_path'].apply(lambda x: x.replace('wav/', 'wavs/'))\ndf_train_xtts['text'] = df_train['transcript']\ndf_train_xtts['speaker_name'] = df_train['speaker_id']\ndf_train_xtts.to_csv(f\"{target_dataset_path}/metadata_train.csv\", sep='|', index=False)\n\ndf_eval = pd.read_csv(f\"{kaggle_dataset_path}/metadata_eval.csv\", sep='|')\ndf_eval_xtts = pd.DataFrame()\ndf_eval_xtts['audio_file'] = df_eval['audio_file_path'].apply(lambda x: x.replace('wav/', 'wavs/'))\ndf_eval_xtts['text'] = df_eval['transcript']\ndf_eval_xtts['speaker_name'] = df_eval['speaker_id']\ndf_eval_xtts.to_csv(f\"{target_dataset_path}/metadata_eval.csv\", sep='|', index=False)\n\nprint(f\"âœ… Training samples: {len(df_train_xtts)}\")\nprint(f\"âœ… Validation samples: {len(df_eval_xtts)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:13.658651Z","iopub.execute_input":"2025-10-29T06:24:13.659278Z","iopub.status.idle":"2025-10-29T06:24:33.757170Z","shell.execute_reply.started":"2025-10-29T06:24:13.659253Z","shell.execute_reply":"2025-10-29T06:24:33.756385Z"}},"outputs":[{"name":"stdout","text":"Dataset downloaded to: /kaggle/input/sinhala-tts-dataset\nkaggle dataset path: /kaggle/input/sinhala-tts-dataset/sinhala-tts-dataset\ntarget dataset path: /kaggle/working/datasets/\nâœ… Copied audio files\nâœ… Training samples: 1000\nâœ… Validation samples: 251\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Run download script\n#!python download_checkpoint.py --output_path checkpoints/\n# Download XTTS-v2 base model\n#!python download_checkpoint.py --output_path /kaggle/working/checkpoints/\n\n#print(\"âœ… XTTS-v2 model downloaded\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:33.757803Z","iopub.execute_input":"2025-10-29T06:24:33.758032Z","iopub.status.idle":"2025-10-29T06:24:33.761523Z","shell.execute_reply.started":"2025-10-29T06:24:33.758015Z","shell.execute_reply":"2025-10-29T06:24:33.760772Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# ============================================================================\n# CELL 8 (CORRECTED): Download XTTS-v2 Model Files\n# ============================================================================\n\nimport os\nimport requests\nfrom tqdm import tqdm\n\n# Create output directory\noutput_dir = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files\"\nos.makedirs(output_dir, exist_ok=True)\n\nprint(\"=\" * 80)\nprint(\"DOWNLOADING XTTS-v2 MODEL FILES\")\nprint(\"=\" * 80)\n\n# Define all required files from Hugging Face\nbase_url = \"https://huggingface.co/coqui/XTTS-v2/resolve/main/\"\n\nfiles_to_download = {\n    \"config.json\": f\"{base_url}config.json\",\n    \"vocab.json\": f\"{base_url}vocab.json\",\n    \"model.pth\": f\"{base_url}model.pth\",\n    \"dvae.pth\": f\"{base_url}dvae.pth\",\n    \"mel_stats.pth\": f\"{base_url}mel_stats.pth\",\n    \"speakers_xtts.pth\": f\"{base_url}speakers_xtts.pth\",\n}\n\ndef download_file(url, output_path):\n    '''Download file with progress bar'''\n    response = requests.get(url, stream=True)\n    total_size = int(response.headers.get('content-length', 0))\n    \n    with open(output_path, 'wb') as f:\n        with tqdm(total=total_size, unit='B', unit_scale=True, desc=os.path.basename(output_path)) as pbar:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n                    pbar.update(len(chunk))\n\n# Download each file\nfor filename, url in files_to_download.items():\n    output_path = os.path.join(output_dir, filename)\n    \n    if os.path.exists(output_path):\n        print(f\"âœ… {filename} already exists, skipping...\")\n    else:\n        print(f\"\\nğŸ”¹ Downloading {filename}...\")\n        try:\n            download_file(url, output_path)\n            print(f\"âœ… {filename} downloaded successfully\")\n        except Exception as e:\n            print(f\"âŒ Failed to download {filename}: {e}\")\n\n# Verify all files downloaded\nprint(f\"\\n{'=' * 80}\")\nprint(\"VERIFICATION\")\nprint(f\"{'=' * 80}\")\n\nall_downloaded = True\nfor filename in files_to_download.keys():\n    filepath = os.path.join(output_dir, filename)\n    if os.path.exists(filepath):\n        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n        print(f\"âœ… {filename}: {size_mb:.1f} MB\")\n    else:\n        print(f\"âŒ {filename}: MISSING!\")\n        all_downloaded = False\n\nif all_downloaded:\n    print(f\"\\n{'=' * 80}\")\n    print(\"âœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\")\n    print(f\"{'=' * 80}\")\n\n# List all downloaded files\nprint(f\"\\nğŸ”¹ Contents of {output_dir}:\")\n!ls -lh {output_dir}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:33.763589Z","iopub.execute_input":"2025-10-29T06:24:33.763802Z","iopub.status.idle":"2025-10-29T06:24:42.651622Z","shell.execute_reply.started":"2025-10-29T06:24:33.763783Z","shell.execute_reply":"2025-10-29T06:24:42.650697Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nDOWNLOADING XTTS-v2 MODEL FILES\n================================================================================\n\nğŸ”¹ Downloading config.json...\n","output_type":"stream"},{"name":"stderr","text":"config.json: 4.37kB [00:00, 6.40MB/s]","output_type":"stream"},{"name":"stdout","text":"âœ… config.json downloaded successfully\n\nğŸ”¹ Downloading vocab.json...\n","output_type":"stream"},{"name":"stderr","text":"\nvocab.json: 361kB [00:00, 67.3MB/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… vocab.json downloaded successfully\n\nğŸ”¹ Downloading model.pth...\n","output_type":"stream"},{"name":"stderr","text":"model.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.87G/1.87G [00:06<00:00, 288MB/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… model.pth downloaded successfully\n\nğŸ”¹ Downloading dvae.pth...\n","output_type":"stream"},{"name":"stderr","text":"dvae.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 211M/211M [00:00<00:00, 280MB/s] \n","output_type":"stream"},{"name":"stdout","text":"âœ… dvae.pth downloaded successfully\n\nğŸ”¹ Downloading mel_stats.pth...\n","output_type":"stream"},{"name":"stderr","text":"mel_stats.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.07k/1.07k [00:00<00:00, 8.48MB/s]\n","output_type":"stream"},{"name":"stdout","text":"âœ… mel_stats.pth downloaded successfully\n\nğŸ”¹ Downloading speakers_xtts.pth...\n","output_type":"stream"},{"name":"stderr","text":"speakers_xtts.pth: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.75M/7.75M [00:00<00:00, 135MB/s]","output_type":"stream"},{"name":"stdout","text":"âœ… speakers_xtts.pth downloaded successfully\n\n================================================================================\nVERIFICATION\n================================================================================\nâœ… config.json: 0.0 MB\nâœ… vocab.json: 0.3 MB\nâœ… model.pth: 1781.4 MB\nâœ… dvae.pth: 200.8 MB\nâœ… mel_stats.pth: 0.0 MB\nâœ… speakers_xtts.pth: 7.4 MB\n\n================================================================================\nâœ… ALL XTTS-v2 MODEL FILES DOWNLOADED SUCCESSFULLY!\n================================================================================\n\nğŸ”¹ Contents of /kaggle/working/checkpoints/XTTS_v2.0_original_model_files:\ntotal 2.0G\n-rw-r--r-- 1 root root 4.3K Oct 29 06:24 config.json\n-rw-r--r-- 1 root root 201M Oct 29 06:24 dvae.pth\n-rw-r--r-- 1 root root 1.1K Oct 29 06:24 mel_stats.pth\n-rw-r--r-- 1 root root 1.8G Oct 29 06:24 model.pth\n-rw-r--r-- 1 root root 7.4M Oct 29 06:24 speakers_xtts.pth\n-rw-r--r-- 1 root root 353K Oct 29 06:24 vocab.json\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"#cell 9\nimport os\nimport glob\n\n# Verify vocab.json location\nvocab_path = \"/kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\"\nprint(f\"Checking for vocab.json at: {vocab_path}\")\n\nif os.path.exists(vocab_path):\n    print(f\"âœ… Found vocab.json at: {vocab_path}\")\nelse:\n    print(f\"âŒ vocab.json not found at: {vocab_path}\")\n    print(\"\\nSearching for vocab.json in checkpoints directory...\")\n    vocab_files = glob.glob(\"checkpoints/**/vocab.json\", recursive=True)\n    if vocab_files:\n        print(f\"Found vocab files at: {vocab_files}\")\n        vocab_path = vocab_files[0]\n    else:\n        print(\"âŒ No vocab.json found! Download may have failed.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:42.652699Z","iopub.execute_input":"2025-10-29T06:24:42.652999Z","iopub.status.idle":"2025-10-29T06:24:42.659118Z","shell.execute_reply.started":"2025-10-29T06:24:42.652975Z","shell.execute_reply":"2025-10-29T06:24:42.658334Z"}},"outputs":[{"name":"stdout","text":"Checking for vocab.json at: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\nâœ… Found vocab.json at: /kaggle/working/checkpoints/XTTS_v2.0_original_model_files/vocab.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ============================================================================\n# CELL 10: Extend Vocabulary for Sinhala (CORRECTED)\n# ============================================================================\n\nimport os\nimport json\n\nprint(\"=\" * 80)\nprint(\"EXTENDING VOCABULARY FOR SINHALA\")\nprint(\"=\" * 80)\n\n# Run extend_vocab with correct path and larger vocab size\n!python extend_vocab_config.py \\\n    --output_path=/kaggle/working/checkpoints/ \\\n    --metadata_path=/kaggle/working/datasets/metadata_train.csv \\\n    --language=si \\\n    --extended_vocab_size=15000\n\nprint(\"\\nâœ… Vocabulary extension completed!\")\n\n# Verify the extended vocab\nvocab_path = \"/kaggle/working/checkpoints/XTTS-v2/vocab.json\"\nif os.path.exists(vocab_path):\n    with open(vocab_path, 'r', encoding='utf-8') as f:\n        vocab = json.load(f)\n    print(f\"âœ… Extended vocabulary size: {len(vocab)} tokens\")\n    \n    # Check for Sinhala characters in vocab\n    sinhala_tokens = [token for token in vocab.keys() if any('\\u0D80' <= char <= '\\u0DFF' for char in token)]\n    print(f\"âœ… Sinhala-specific tokens: {len(sinhala_tokens)}\")\nelse:\n    print(\"âŒ Vocabulary file not found!\")\n    \nprint(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:42.659968Z","iopub.execute_input":"2025-10-29T06:24:42.660340Z","iopub.status.idle":"2025-10-29T06:24:43.394364Z","shell.execute_reply.started":"2025-10-29T06:24:42.660315Z","shell.execute_reply":"2025-10-29T06:24:43.393546Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nEXTENDING VOCABULARY FOR SINHALA\n================================================================================\nTraceback (most recent call last):\n  File \"/kaggle/working/XTTSv2-Finetuning-for-New-Languages/extend_vocab_config.py\", line 100, in <module>\n    extend_tokenizer(args)\n  File \"/kaggle/working/XTTSv2-Finetuning-for-New-Languages/extend_vocab_config.py\", line 47, in extend_tokenizer\n    existing_tokenizer = Tokenizer.from_file(os.path.join(root, \"vocab.json\"))\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nException: No such file or directory (os error 2)\n\nâœ… Vocabulary extension completed!\nâŒ Vocabulary file not found!\n================================================================================\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ============================================================================\n# CELL 11: Verify Tokenization Coverage (CRITICAL STEP)\n# ============================================================================\n\nimport pandas as pd\nfrom tokenizers import Tokenizer\nimport torch\n\nprint(\"=\" * 80)\nprint(\"VERIFYING TOKENIZATION COVERAGE\")\nprint(\"=\" * 80)\n\n# Load the extended tokenizer\ntokenizer_path = \"/kaggle/working/checkpoints/XTTS-v2/vocab.json\"\ntokenizer = Tokenizer.from_file(tokenizer_path)\n\n# Load training data\ntraindf = pd.read_csv(\n    \"/kaggle/working/datasets/metadata_train.csv\", \n    sep=\"|\"\n)\n\n# Check for UNK tokens in samples\nprint(f\"\\nChecking {len(traindf)} training samples for UNK tokens...\")\nunk_count = 0\nsamples_checked = 0\n\nfor idx, row in traindf.iterrows():\n    text = row['text']\n    encoding = tokenizer.encode(text)\n    \n    # Token ID 1 is UNK\n    if 1 in encoding.ids:\n        unk_count += 1\n        if unk_count <= 3:  # Show first 3 examples\n            print(f\"\\nâŒ Sample {idx} has UNK tokens:\")\n            print(f\"   Text: {text[:80]}...\")\n            \n    samples_checked += 1\n    \n    # Check first 100 samples\n    if samples_checked >= 100:\n        break\n\nif unk_count == 0:\n    print(f\"\\nâœ… SUCCESS: No UNK tokens found in first {samples_checked} samples!\")\n    print(\"âœ… Vocabulary covers your Sinhala dataset properly\")\n    print(\"\\n\" + \"=\" * 80)\n    print(\"READY TO START GPT TRAINING\")\n    print(\"=\" * 80)\nelse:\n    print(f\"\\nâŒ PROBLEM: Found UNK tokens in {unk_count}/{samples_checked} samples\")\n    print(\"âŒ Need to increase --extended_vocab_size further (try 20000)\")\n    print(\"\\nâš ï¸ DO NOT PROCEED TO TRAINING - Fix vocabulary first!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:43.395413Z","iopub.execute_input":"2025-10-29T06:24:43.395677Z","iopub.status.idle":"2025-10-29T06:24:43.491183Z","shell.execute_reply.started":"2025-10-29T06:24:43.395654Z","shell.execute_reply":"2025-10-29T06:24:43.489898Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nVERIFYING TOKENIZATION COVERAGE\n================================================================================\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_37/1369581266.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Load the extended tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtokenizer_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/working/checkpoints/XTTS-v2/vocab.json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Load training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mException\u001b[0m: No such file or directory (os error 2)"],"ename":"Exception","evalue":"No such file or directory (os error 2)","output_type":"error"}],"execution_count":11},{"cell_type":"code","source":"# ============================================================================\n# CELL 13 (OPTIONAL): DVAE Finetuning\n# ============================================================================\n# NOTE: Skip this unless you have issues with audio quality\n# Most cases don't need DVAE finetuning with 20+ hours of data\n\nimport os\n\nprint(\"=\" * 80)\nprint(\"DVAE FINETUNING (OPTIONAL - Usually not needed)\")\nprint(\"=\" * 80)\nprint(\"âš ï¸ Only run this if audio quality is poor after GPT training\")\nprint(\"=\" * 80)\n\n# Uncomment below to run DVAE training\n\"\"\"\n!CUDA_VISIBLE_DEVICES=0 python train_dvae_xtts.py \\\n    --output_path=/kaggle/working/checkpoints/ \\\n    --train_csv_path=/kaggle/working/datasets/metadata_train.csv \\\n    --eval_csv_path=/kaggle/working/datasets/metadata_eval.csv \\\n    --language=\"si\" \\\n    --num_epochs=3 \\\n    --batch_size=256 \\\n    --lr=5e-6\n\nprint(\"âœ… DVAE finetuning completed!\")\n\"\"\"\n\nprint(\"â„¹ï¸ DVAE finetuning skipped (recommended for most cases)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:43.491857Z","iopub.status.idle":"2025-10-29T06:24:43.492221Z","shell.execute_reply.started":"2025-10-29T06:24:43.492041Z","shell.execute_reply":"2025-10-29T06:24:43.492060Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ============================================================================\n# CELL 13: GPT Finetuning (After DVAE)\n# ============================================================================\n\nimport os\n\nprint(\"=\" * 80)\nprint(\"STARTING GPT FINETUNING\")\nprint(\"=\" * 80)\n\n# Run GPT training AFTER DVAE\n!CUDA_VISIBLE_DEVICES=0 python train_gpt_xtts.py \\\n    --output_path /kaggle/working/checkpoints/ \\\n    --metadatas /kaggle/working/datasets/metadata_train.csv,/kaggle/working/datasets/metadata_eval.csv,si \\\n    --num_epochs 15 \\\n    --batch_size 4 \\\n    --grad_acumm 63 \\\n    --max_text_length 250 \\\n    --max_audio_length 255995 \\\n    --weight_decay 1e-2 \\\n    --lr 5e-6 \\\n    --save_step 1000\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"âœ… GPT TRAINING COMPLETED!\")\nprint(\"=\" * 80)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T06:24:43.494239Z","iopub.status.idle":"2025-10-29T06:24:43.495073Z","shell.execute_reply.started":"2025-10-29T06:24:43.494834Z","shell.execute_reply":"2025-10-29T06:24:43.494855Z"}},"outputs":[],"execution_count":null}]}